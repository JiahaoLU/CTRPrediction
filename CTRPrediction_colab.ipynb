{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTRPrediction_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "14BAV0L--W8SLx2nrYIB4fN8nUmp-ChQS",
      "authorship_tag": "ABX9TyPgVo6ZWphAcqP0mEMFTvUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiahaoLU/CTRPrediction/blob/Jiahao/CTRPrediction_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-4a8uQXyo-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cebe125b-a360-4d32-b424-4f25861eb543"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "octJXhQRlVgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "\n",
        "    def __init__(self, path=None):\n",
        "        \"\"\"\n",
        "        Initialise the preprocessor as a Data set in torch\n",
        "        or initialise it as a data provider (cut smaller data, etc)\n",
        "        :param path: file path\n",
        "        \"\"\"\n",
        "        if path is not None:\n",
        "            self.path = path\n",
        "            self.data = self.read_data_by_chunk()\n",
        "            self.labels = np.asarray(self.data.iloc[:, 1])\n",
        "            self.one_hot_data = self.get_one_hot_dataset()\n",
        "            print('Data set initiated from {0}.'.format(self.path))\n",
        "        else:\n",
        "            self.path = './Data/train.csv'\n",
        "            print('Data provider is ready.')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        return number of data\n",
        "        \"\"\"\n",
        "        return np.size(self.one_hot_data, 0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        called when iterate on Data Loader\n",
        "        :param index: index of one sample\n",
        "        :return: one hot encoding of a sample, which consists of field-relative index of value 1,\n",
        "                 and corresponding label\n",
        "        \"\"\"\n",
        "        return self.one_hot_data[index], self.labels[index]\n",
        "\n",
        "    def open_csv(self, iterator=False):\n",
        "        \"\"\"\n",
        "        Open a csv file\n",
        "        :param iterator: Open a csv file as iterator/or not\n",
        "        :return: pandas data frame\n",
        "        \"\"\"\n",
        "        try:\n",
        "            csv_file = pd.read_csv(self.path, sep=',', engine='python', iterator=iterator)\n",
        "            if csv_file is None:\n",
        "              print(\"file empty\")\n",
        "            return csv_file\n",
        "        except FileNotFoundError:\n",
        "            print(\"file not found\")\n",
        "\n",
        "    def cut_smaller_data_for_exam(self, new_path, size=2000, skip_size=40000):\n",
        "        \"\"\"\n",
        "        Cut a smaller data set for debugging code of size (smaller_data_size, num_fields + 2)\n",
        "        :param new_path: name and path for new csv file\n",
        "        :param size: the size of the smaller data set\n",
        "        :param skip_size: interval to skip to retrieve data uniformly from original set\n",
        "        :return: none\n",
        "        \"\"\"\n",
        "        print(\"Producing smaller data\")\n",
        "        print(\"Read file\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"cut by chunk\")\n",
        "        chunk_size = 100\n",
        "        loops = math.floor(size/chunk_size)\n",
        "        chunks = []\n",
        "        for loop in range(loops * 2):\n",
        "            try:\n",
        "                print(\"chunk: \", loop)\n",
        "                if loop % 2 == 0:  # avoid load data of same timestamp\n",
        "                    chunk = data_file.get_chunk(chunk_size)\n",
        "                    chunks.append(chunk)\n",
        "                else:\n",
        "                    chunk = data_file.get_chunk(skip_size)\n",
        "                    del chunk\n",
        "            except StopIteration:\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        data_frame = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        if os.path.exists(new_path):\n",
        "            os.remove(new_path)\n",
        "        data_frame.to_csv(new_path, index=False)\n",
        "        print(\"New smaller file created\")\n",
        "\n",
        "    def read_data_by_chunk(self, chunk_size=1000000):\n",
        "        \"\"\"\n",
        "        Read csv by chunk to avoid drive memory overflow\n",
        "        :param chunk_size: the size of data to read in every chunk\n",
        "        :return: csv registered in pandas data frame of size (data_size, num_fields + 2)\n",
        "        \"\"\"\n",
        "        print(\"Reading large data\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"Cut by chunk\")\n",
        "        loop = True\n",
        "        chunks = []\n",
        "        index = 0\n",
        "        while loop:\n",
        "            try:\n",
        "                print(\"chunk: \", index)\n",
        "                chunk = data_file.get_chunk(chunk_size)\n",
        "                chunks.append(chunk)\n",
        "                index += 1\n",
        "\n",
        "            except StopIteration:\n",
        "                loop = False\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        whole_data = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        print(\"Data imported\")\n",
        "        return whole_data\n",
        "\n",
        "    def get_feature_set(self, feature_array):\n",
        "        \"\"\"\n",
        "        Get array of feature set of one field.\n",
        "        :param feature_array: one column (one field) of the data frame\n",
        "        :return: an array of size (num_features)\n",
        "        \"\"\"\n",
        "        return np.array(np.unique(feature_array))\n",
        "\n",
        "    def one_hot_encoding(self, feature_instances, feature_set):\n",
        "        \"\"\"\n",
        "        One hot encoding for one column (one field) for every sample.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :param feature_instances: one column of data frame\n",
        "        :param feature_set: array of nonrecurring feature set\n",
        "        :return: one hot encoding of the whole column of size (data_size, 1)\n",
        "        \"\"\"\n",
        "        one_hot_vector = np.zeros((len(feature_instances), 1), dtype=int)\n",
        "        for i in range(len(feature_instances)):\n",
        "            if feature_instances[i] not in feature_set:\n",
        "                raise Exception(\"instance is not in the set\")\n",
        "\n",
        "            index = np.argwhere(feature_set == feature_instances[i])\n",
        "            one_hot_vector[i, 0] = int(index)\n",
        "\n",
        "        return one_hot_vector\n",
        "\n",
        "    def get_field_dims(self):\n",
        "        \"\"\"\n",
        "        Get an array which contains the dimension of each field.\n",
        "        If sum up the array, the sum is the total dimension of all features.\n",
        "        (the total length of the one hot encoding vector)\n",
        "        The length of array is number of fields.\n",
        "        :return: an array of size (num_fields)\n",
        "        \"\"\"\n",
        "        dims = []\n",
        "        for i in range(2, self.data.shape[1]):\n",
        "            dims.append(len(self.get_feature_set(self.one_hot_data[:, i-2])))\n",
        "        return np.array(dims)\n",
        "\n",
        "    def get_one_hot_dataset(self):\n",
        "        \"\"\"\n",
        "        Get one hot encoding for the whole data frame.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :return: an array of size (data_size, num_fields)\n",
        "        \"\"\"\n",
        "        print('one hot encoding: feature {0}'.format(self.data.columns[2]))\n",
        "        features = np.asarray(self.data.iloc[:, 2])\n",
        "        one_hot_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "        for i in range(3, self.data.shape[1]):\n",
        "            print('one hot encoding: feature {0}'.format(self.data.columns[i]))\n",
        "            features = np.asarray(self.data.iloc[:, i])\n",
        "            next_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "            one_hot_array = np.hstack((one_hot_array, next_array))\n",
        "        return one_hot_array\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # print('Start generating smaller data')\n",
        "#     # provider = DataPreprocessor()\n",
        "#     # provider.cut_smaller_data_for_exam('./Data/train20k.csv', size=20000, skip_size=40000)\n",
        "#     print(\"Start data cleaning\")\n",
        "#     f = \"/content/drive/My Drive/train20k.csv\"\n",
        "\n",
        "#     processor = DataPreprocessor(f)\n",
        "#     print(processor.data.head())\n",
        "#     loader = DataLoader(processor, batch_size=10, shuffle=True)\n",
        "#     print(len(processor))\n",
        "#     # print(processor.get_field_dims())\n",
        "#     # for data, label in loader:\n",
        "#     #     print(data.size(), label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aVyScX_nWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachineModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Field-aware Factorization Machine.\n",
        "\n",
        "    Reference:\n",
        "        Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialisation of the FFM model\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
        "        x = self.linear(x) + ffm_term\n",
        "        return torch.sigmoid(x.squeeze(1))\n",
        "\n",
        "\n",
        "class FeaturesLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        \"\"\"\n",
        "        Initialisation of linear first-degree terms. Use nn.Embedding to realize\n",
        "        inner product of input one-hot encoding vector and weight vector\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param output_dim: the sum of linear terms\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)  # field-relative one hot encoding\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)  # absolute-position one hot encoding\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias  # element wise linear poly sum\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachine(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Use latent vector product to replace simple scalar weight factor.\n",
        "        More adaptive to situations where dependency exists among fields.\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_fields = len(field_dims)\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
        "        ])\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        for embedding in self.embeddings:\n",
        "            nn.init.xavier_uniform_(embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]  # generation of latent mat\n",
        "        ix = list()\n",
        "        for i in range(self.num_fields - 1):\n",
        "            for j in range(i + 1, self.num_fields):\n",
        "                ix.append(xs[j][:, i] * xs[i][:, j])  # product of latent vector\n",
        "        ix = torch.stack(ix, dim=1)\n",
        "        return ix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mzjg8OnhNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "# from Data_Preprocessor import *\n",
        "# from Deep_Model import FieldAwareFactorizationMachineModel\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import threading\n",
        "\n",
        "\n",
        "def fun_timer():\n",
        "    global boom\n",
        "    boom = True\n",
        "\n",
        "\n",
        "def run_train(model, optimizer, data_loader, criterion, device, log_interval=10):\n",
        "    \"\"\"\n",
        "    train the model using backward propagation\n",
        "    :param model: the model to be trained. instance of subclass of nn.Module\n",
        "    :param optimizer: torch optimiser with learning rate\n",
        "    :param data_loader: torch DataLoader of training data set\n",
        "    :param criterion: nn.BCELoss\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :param log_interval: interval for showing training loss\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (fields, target) in enumerate(data_loader):\n",
        "        fields, target = fields.to(device), target.to(device)\n",
        "        y = model(fields)\n",
        "        loss = criterion(y, target.float())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % log_interval == 0:\n",
        "            print('    - loss:', total_loss / log_interval)\n",
        "            total_loss = 0\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def run_test(model, data_loader, device, criterion):\n",
        "    \"\"\"\n",
        "    evaluate / test the model\n",
        "    :param model: the model to be evaluated/tested. instance of subclass of nn.Module\n",
        "    :param data_loader: torch DataLoader of eval/test data set\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :return: auc score, accuracy of prediction\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    targets, predicts = list(), list()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for fields, target in data_loader:\n",
        "            fields, target = fields.to(device), target.to(device)\n",
        "            y = model(fields)\n",
        "            loss = criterion(y, target.float())\n",
        "            targets.extend(target.tolist())\n",
        "            predicts.extend(y.tolist())\n",
        "            predict_click = torch.round(y.data)\n",
        "            correct += (predict_click == target).sum().item()\n",
        "    return roc_auc_score(targets, predicts), correct / len(targets) * 100, loss.item()\n",
        "\n",
        "\n",
        "def main_process(dataset_path, epoch, learning_rate, batch_size, weight_decay, embeddim, boomtime = 60):\n",
        "    \"\"\"\n",
        "    Main process for train/evaluate/test the model, determine the hyper parameters here.\n",
        "    :param boomtime: time boom for adjusting hyperparameters to control the same training time cost\n",
        "    :param dataset_path: path of the original csv file\n",
        "    :param epoch: number of epochs\n",
        "    :param learning_rate: learning rate of gradient descent\n",
        "    :param batch_size: size of batches\n",
        "    :param weight_decay: L2 regularisation\n",
        "    :param embeddim: dimension of latent vector\n",
        "    :return: the trained model\n",
        "    \"\"\"\n",
        "    global boom\n",
        "    boom = False\n",
        "    timer = threading.Timer(boomtime, fun_timer)\n",
        "    timer.start()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "\n",
        "    # Prepare the data\n",
        "    dataset = DataPreprocessor(dataset_path)\n",
        "    train_length = int(len(dataset) * 0.8)\n",
        "    valid_length = int(len(dataset) * 0.1)\n",
        "    test_length = len(dataset) - train_length - valid_length\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, (train_length, valid_length, test_length))\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    field_dims = dataset.get_field_dims()\n",
        "\n",
        "    # Prepare the model and loss function\n",
        "    model = FieldAwareFactorizationMachineModel(field_dims, embed_dim=embeddim).to(device)\n",
        "    criterion = torch.nn.BCELoss().to(device)  # binary cross entropy loss\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    val_auc_list = []\n",
        "    val_acc_list = []\n",
        "    val_loss_list = []\n",
        "    best_model = None\n",
        "    best_val_auc = 0\n",
        "    best_val_acc = 0\n",
        "    best_val_loss = 999999\n",
        "\n",
        "    # train\n",
        "    for epoch_i in range(epoch):\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
        "        print('Cached:   ', round(torch.cuda.memory_cached(0) / 1024 ** 3, 1), 'GB')\n",
        "        train_loss = run_train(model, optimizer, train_data_loader, criterion, device)\n",
        "        val_auc, val_acc,val_loss = run_test(model, valid_data_loader, device, criterion)\n",
        "        epoch_list.append(epoch_i)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_auc_list.append(val_auc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print('epoch:', epoch_i, 'train loss:', train_loss, 'validation: auc:',\\\n",
        "              val_auc, '--- acc:', val_acc, '--- loss:', val_loss)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model\n",
        "            best_val_auc = val_auc\n",
        "            best_val_acc = val_acc\n",
        "        if boom:\n",
        "            print('time up, break')\n",
        "            break\n",
        "        else:\n",
        "            print(timer)\n",
        "    test_auc, test_acc, test_loss = run_test(model, test_data_loader, device, criterion)\n",
        "    print('test auc:', test_auc, 'test acc:', test_acc, 'test loss', test_loss)\n",
        "    return model, epoch_list, train_loss_list, val_auc_list, val_acc_list, val_loss_list, \\\n",
        "           test_auc, test_acc, test_loss, best_model, best_val_auc, best_val_acc, best_val_loss\n",
        "\n",
        "\n",
        "def main_train_test(save_model=True):\n",
        "    DATASET_PATH = \"/content/drive/My Drive/train20k.csv\"\n",
        "    EPOCH = 1000\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 3200\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    EMBED_DIM = 10\n",
        "    trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,auc,acc,loss,\\\n",
        "    best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "        main_process(DATASET_PATH, EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM)\n",
        "    print(trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,auc,acc,loss,\\\n",
        "    best_model,best_val_auc,best_val_acc,best_val_loss)\n",
        "    # if save_model:\n",
        "    #     model_name = \"FFM\"\n",
        "    #     torch.save(trained_model, f'{model_name}.pt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBS-Fwnh20q6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85044a31-40e0-4d2f-fe76-6f1d426f105f"
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class HyperFinder():\n",
        "    def __init__(self):\n",
        "        self.DATASET_PATH = \"/content/drive/My Drive/train20k.csv\"\n",
        "        self.EPOCH = 1000\n",
        "        self.LEARNING_RATE = [0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5]\n",
        "        self.BATCH_SIZE = [40*i for i in range(1,11)]\n",
        "        self.WEIGHT_DECAY = [pow(10,-6+i) for i in range(5)]\n",
        "        self.EMBED_DIM = [pow(2,i) for i in range(5)]\n",
        "        self.save_best = True\n",
        "\n",
        "        self.best_learning_rate = None\n",
        "        self.model_best = None\n",
        "        self.epoch_list = []\n",
        "        self.train_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "        self.val_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.auc_best = 0\n",
        "        self.acc_best = 1\n",
        "        self.loss_best = 999999\n",
        "        self.Total_Epoch = 1\n",
        "        self.data_save = {}\n",
        "        self.data_saveE = {}\n",
        "        self.data_saveW = {}\n",
        "\n",
        "    def save_model(self,trained_model,*args):\n",
        "        model_name = ''\n",
        "        for i in args:\n",
        "            print(i)\n",
        "            model_name += (str(i)[:7] + ' ')\n",
        "        torch.save(trained_model, f'/content/drive/My Drive/Colab Notebooks/{model_name}.pt')\n",
        "\n",
        "    def obj_2_json(self, hpname,load_dict):\n",
        "        with open(\"/content/drive/My Drive/Colab Notebooks/\" + hpname + \".json\",\"w\") as dump_f:\n",
        "            json.dump(load_dict,dump_f)\n",
        "\n",
        "    def find_best_Learning_Rate(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        WEIGHT_DECAY = 1e-6\n",
        "        EMBED_DIM = 8\n",
        "        for LEARNING_RATE in self.LEARNING_RATE:\n",
        "            self.data_save[str(LEARNING_RATE)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,5*60)\n",
        "            self.data_save[str(LEARNING_RATE)]['EPOCH'] = epoch_list\n",
        "            self.data_save[str(LEARNING_RATE)]['train_loss_list'] = train_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_auc_list'] = val_auc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_acc_list'] = val_acc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_loss_list'] = val_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['test_auc'] = test_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_acc'] = test_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_loss'] = test_loss\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_auc'] = best_val_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_acc'] = best_val_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "    def find_best_Weight_decay(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        LEARNING_RATE = 0.01\n",
        "        EMBED_DIM = 8\n",
        "        for WEIGHT_DECAY in self.WEIGHT_DECAY:\n",
        "            self.data_saveW[str(WEIGHT_DECAY)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,5*60)\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['EPOCH'] = epoch_list\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['train_loss_list'] = train_loss_list\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['val_auc_list'] = val_auc_list\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['val_acc_list'] = val_acc_list\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['val_loss_list'] = val_loss_list\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['test_auc'] = test_auc\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['test_acc'] = test_acc\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['test_loss'] = test_loss\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['best_val_auc'] = best_val_auc\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['best_val_acc'] = best_val_acc\n",
        "            self.data_saveW[str(WEIGHT_DECAY)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "    def find_best_k(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        WEIGHT_DECAY = 1e-6\n",
        "        LEARNING_RATE = 0.001\n",
        "        for EMBED_DIM in self.EMBED_DIM:\n",
        "            self.data_saveE[str(EMBED_DIM)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,5*60)\n",
        "            self.data_saveE[str(EMBED_DIM)]['EPOCH'] = epoch_list\n",
        "            self.data_saveE[str(EMBED_DIM)]['train_loss_list'] = train_loss_list\n",
        "            self.data_saveE[str(EMBED_DIM)]['val_auc_list'] = val_auc_list\n",
        "            self.data_saveE[str(EMBED_DIM)]['val_acc_list'] = val_acc_list\n",
        "            self.data_saveE[str(EMBED_DIM)]['val_loss_list'] = val_loss_list\n",
        "            self.data_saveE[str(EMBED_DIM)]['test_auc'] = test_auc\n",
        "            self.data_saveE[str(EMBED_DIM)]['test_acc'] = test_acc\n",
        "            self.data_saveE[str(EMBED_DIM)]['test_loss'] = test_loss\n",
        "            self.data_saveE[str(EMBED_DIM)]['best_val_auc'] = best_val_auc\n",
        "            self.data_saveE[str(EMBED_DIM)]['best_val_acc'] = best_val_acc\n",
        "            self.data_saveE[str(EMBED_DIM)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "\n",
        "def main():\n",
        "    hyperFinder = HyperFinder()\n",
        "    hyperFinder.find_best_Learning_Rate()\n",
        "    hyperFinder.obj_2_json('learning_rate',hyperFinder.data_save)\n",
        "    hyperFinder.find_best_Weight_decay()\n",
        "    hyperFinder.obj_2_json('Weight_decay',hyperFinder.data_saveW)\n",
        "    hyperFinder.find_best_k()\n",
        "    hyperFinder.obj_2_json('EMBED_DIM',hyperFinder.data_saveE)\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，无法输出最后 5000 行内容。\u001b[0m\n",
            "epoch: 151 train loss: 0.004457593429833651 validation: auc: 0.6821875793627539 --- acc: 82.69999999999999 --- loss: 0.8220576047897339\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035867041442543267\n",
            "epoch: 152 train loss: 0.002059865975752473 validation: auc: 0.6819895609674664 --- acc: 82.75 --- loss: 0.8272736668586731\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003205603454262018\n",
            "epoch: 153 train loss: 0.0027581024914979935 validation: auc: 0.6818670204358536 --- acc: 82.75 --- loss: 0.8270618319511414\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003427011659368873\n",
            "epoch: 154 train loss: 0.0017788432305678725 validation: auc: 0.6817782229491777 --- acc: 82.69999999999999 --- loss: 0.822987973690033\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003151450539007783\n",
            "epoch: 155 train loss: 0.003421328729018569 validation: auc: 0.681707184959837 --- acc: 82.55 --- loss: 0.8186235427856445\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000360957020893693\n",
            "epoch: 156 train loss: 0.0021464317105710506 validation: auc: 0.6816041798752928 --- acc: 82.5 --- loss: 0.8166019320487976\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034630317240953447\n",
            "epoch: 157 train loss: 0.002986603882163763 validation: auc: 0.681522486187551 --- acc: 82.5 --- loss: 0.817283570766449\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003527169115841389\n",
            "epoch: 158 train loss: 0.0022230229806154966 validation: auc: 0.6816912014122352 --- acc: 82.55 --- loss: 0.8197250962257385\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002692940179258585\n",
            "epoch: 159 train loss: 0.0044243172742426395 validation: auc: 0.6821618280916177 --- acc: 82.65 --- loss: 0.8230947256088257\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002991564804688096\n",
            "epoch: 160 train loss: 0.002604244276881218 validation: auc: 0.6822470736788266 --- acc: 82.65 --- loss: 0.8240531086921692\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033608174417167904\n",
            "epoch: 161 train loss: 0.0018403098220005631 validation: auc: 0.6819673615957975 --- acc: 82.69999999999999 --- loss: 0.8227997422218323\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00030126485507935284\n",
            "epoch: 162 train loss: 0.005474441219121218 validation: auc: 0.6817462558539742 --- acc: 82.69999999999999 --- loss: 0.8209877610206604\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037440240848809483\n",
            "epoch: 163 train loss: 0.0022173088509589434 validation: auc: 0.6815757646795565 --- acc: 82.6 --- loss: 0.818623423576355\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003331286599859595\n",
            "epoch: 164 train loss: 0.002400835044682026 validation: auc: 0.6815260380870181 --- acc: 82.55 --- loss: 0.8181983828544617\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002857492538169026\n",
            "epoch: 165 train loss: 0.003546604420989752 validation: auc: 0.6817098488844373 --- acc: 82.55 --- loss: 0.8219716548919678\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002552877645939589\n",
            "epoch: 166 train loss: 0.004349031951278448 validation: auc: 0.6819842331182657 --- acc: 82.55 --- loss: 0.8284779787063599\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00022816576529294252\n",
            "epoch: 167 train loss: 0.004878218751400709 validation: auc: 0.6821121014990792 --- acc: 82.65 --- loss: 0.8331882357597351\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00027067381888628005\n",
            "epoch: 168 train loss: 0.003503840183839202 validation: auc: 0.6818208457427821 --- acc: 82.69999999999999 --- loss: 0.8321517705917358\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032995957881212237\n",
            "epoch: 169 train loss: 0.0021282166708260775 validation: auc: 0.6811069139499075 --- acc: 82.65 --- loss: 0.8262667059898376\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003117864951491356\n",
            "epoch: 170 train loss: 0.004329731687903404 validation: auc: 0.6807872429978742 --- acc: 82.5 --- loss: 0.8219351172447205\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003475527744740248\n",
            "epoch: 171 train loss: 0.0028493115678429604 validation: auc: 0.681057187357369 --- acc: 82.5 --- loss: 0.8212544918060303\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002710669534280896\n",
            "epoch: 172 train loss: 0.00464879022911191 validation: auc: 0.6813626507115341 --- acc: 82.45 --- loss: 0.8246640563011169\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00029788578394800423\n",
            "epoch: 173 train loss: 0.001958089880645275 validation: auc: 0.6818759001845213 --- acc: 82.55 --- loss: 0.8304795622825623\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00025971385184675455\n",
            "epoch: 174 train loss: 0.0041601527482271194 validation: auc: 0.6822808167237635 --- acc: 82.6 --- loss: 0.834962785243988\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002826235955581069\n",
            "epoch: 175 train loss: 0.004299251362681389 validation: auc: 0.6825685205805936 --- acc: 82.55 --- loss: 0.8348265290260315\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002531047211959958\n",
            "epoch: 176 train loss: 0.005448546260595322 validation: auc: 0.6821955711365545 --- acc: 82.45 --- loss: 0.8326898813247681\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031196433119475844\n",
            "epoch: 177 train loss: 0.002085083397105336 validation: auc: 0.6817995343459798 --- acc: 82.35 --- loss: 0.8303987383842468\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00028908527456223966\n",
            "epoch: 178 train loss: 0.00400337902829051 validation: auc: 0.6816219393726279 --- acc: 82.45 --- loss: 0.8284271955490112\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003668874502182007\n",
            "epoch: 179 train loss: 0.0012565234210342169 validation: auc: 0.6817409280047738 --- acc: 82.55 --- loss: 0.8281900882720947\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002938622608780861\n",
            "epoch: 180 train loss: 0.004009131342172623 validation: auc: 0.681946938173862 --- acc: 82.5 --- loss: 0.8283170461654663\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031783708836883304\n",
            "epoch: 181 train loss: 0.0026838011108338833 validation: auc: 0.6819380584251943 --- acc: 82.55 --- loss: 0.8278393745422363\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00028145741671323774\n",
            "epoch: 182 train loss: 0.0026087432634085417 validation: auc: 0.681851036888252 --- acc: 82.5 --- loss: 0.8273726105690002\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002770358929410577\n",
            "epoch: 183 train loss: 0.002464129589498043 validation: auc: 0.6819043153802575 --- acc: 82.45 --- loss: 0.8272925019264221\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002738170558586717\n",
            "epoch: 184 train loss: 0.0029473903123289347 validation: auc: 0.6816077317747599 --- acc: 82.45 --- loss: 0.8280697464942932\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00029846376273781063\n",
            "epoch: 185 train loss: 0.0020512649789452553 validation: auc: 0.6811646323162469 --- acc: 82.35 --- loss: 0.8290920257568359\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003031271742656827\n",
            "epoch: 186 train loss: 0.002005096757784486 validation: auc: 0.6810927063520393 --- acc: 82.39999999999999 --- loss: 0.8289236426353455\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002479656133800745\n",
            "epoch: 187 train loss: 0.005610477179288864 validation: auc: 0.6813395633649986 --- acc: 82.39999999999999 --- loss: 0.8286654949188232\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00025077760219573977\n",
            "epoch: 188 train loss: 0.004475520458072424 validation: auc: 0.6818101900443809 --- acc: 82.39999999999999 --- loss: 0.8297555446624756\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000267978897318244\n",
            "epoch: 189 train loss: 0.0035548771265894175 validation: auc: 0.6817515837031748 --- acc: 82.39999999999999 --- loss: 0.831134021282196\n",
            "<Timer(Thread-5385, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002577350242063403\n",
            "epoch: 190 train loss: 0.004238597583025694 validation: auc: 0.6818013102957133 --- acc: 82.39999999999999 --- loss: 0.8344438672065735\n",
            "time up, break\n",
            "test auc: 0.6708284141314083 test acc: 81.5 test loss 0.8826530575752258\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.12237764596939087\n",
            "epoch: 0 train loss: 1.013910174369812 validation: auc: 0.6338068099737078 --- acc: 73.6 --- loss: 0.7355036735534668\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0631010115146637\n",
            "epoch: 1 train loss: 0.4669172167778015 validation: auc: 0.6780539423882581 --- acc: 81.85 --- loss: 0.6201246380805969\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027174210548400878\n",
            "epoch: 2 train loss: 0.1615229845046997 validation: auc: 0.6589559677559158 --- acc: 77.10000000000001 --- loss: 0.6083237528800964\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007601384818553924\n",
            "epoch: 3 train loss: 0.06678345054388046 validation: auc: 0.5992025656583168 --- acc: 75.9 --- loss: 0.7692049741744995\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004558031633496285\n",
            "epoch: 4 train loss: 0.050551753491163254 validation: auc: 0.5838308269047412 --- acc: 75.94999999999999 --- loss: 0.8714165687561035\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033072639256715775\n",
            "epoch: 5 train loss: 0.022031476721167564 validation: auc: 0.5825433027072318 --- acc: 76.75 --- loss: 0.920510470867157\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002245100028812885\n",
            "epoch: 6 train loss: 0.023166382685303688 validation: auc: 0.584078219352229 --- acc: 77.95 --- loss: 0.9676781296730042\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018169339746236801\n",
            "epoch: 7 train loss: 0.015381977893412113 validation: auc: 0.5898549232902834 --- acc: 78.8 --- loss: 1.0095818042755127\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00149520393460989\n",
            "epoch: 8 train loss: 0.018483422696590424 validation: auc: 0.5994002990378782 --- acc: 78.95 --- loss: 1.0386579036712646\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0012055597268044948\n",
            "epoch: 9 train loss: 0.010982836596667767 validation: auc: 0.6050903253300973 --- acc: 79.10000000000001 --- loss: 1.0678263902664185\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009531403891742229\n",
            "epoch: 10 train loss: 0.010948789305984974 validation: auc: 0.608371435381815 --- acc: 79.2 --- loss: 1.0889335870742798\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009621811099350452\n",
            "epoch: 11 train loss: 0.005454318132251501 validation: auc: 0.6118078428823206 --- acc: 79.25 --- loss: 1.1022487878799438\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008578152395784855\n",
            "epoch: 12 train loss: 0.005669157952070236 validation: auc: 0.6157336179249372 --- acc: 79.75 --- loss: 1.1207501888275146\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007051039952784777\n",
            "epoch: 13 train loss: 0.007119371090084314 validation: auc: 0.619599802086042 --- acc: 80.45 --- loss: 1.1372787952423096\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006418662145733833\n",
            "epoch: 14 train loss: 0.009032560512423515 validation: auc: 0.6227707592961775 --- acc: 80.95 --- loss: 1.1454275846481323\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006348071619868279\n",
            "epoch: 15 train loss: 0.005134116858243942 validation: auc: 0.6251002210279969 --- acc: 81.10000000000001 --- loss: 1.146885633468628\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006351125426590443\n",
            "epoch: 16 train loss: 0.0018426976166665554 validation: auc: 0.6275335153563896 --- acc: 81.25 --- loss: 1.152355432510376\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005710890516638755\n",
            "epoch: 17 train loss: 0.009043862111866474 validation: auc: 0.6319829678425933 --- acc: 81.5 --- loss: 1.1704084873199463\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006337822880595923\n",
            "epoch: 18 train loss: 0.005358927883207798 validation: auc: 0.6392440264655745 --- acc: 81.85 --- loss: 1.1833090782165527\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005703698843717575\n",
            "epoch: 19 train loss: 0.00703605217859149 validation: auc: 0.6465953743029672 --- acc: 82.05 --- loss: 1.1771678924560547\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005095781292766332\n",
            "epoch: 20 train loss: 0.008451603353023529 validation: auc: 0.6522619253994395 --- acc: 82.1 --- loss: 1.1484273672103882\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005287690553814172\n",
            "epoch: 21 train loss: 0.007379199378192425 validation: auc: 0.6560540724047268 --- acc: 81.85 --- loss: 1.1117610931396484\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005930214654654264\n",
            "epoch: 22 train loss: 0.0020915379282087088 validation: auc: 0.6587176042298691 --- acc: 81.85 --- loss: 1.0816469192504883\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005296441726386547\n",
            "epoch: 23 train loss: 0.0027909206692129374 validation: auc: 0.6610470659616885 --- acc: 82.05 --- loss: 1.0627446174621582\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00048507489264011384\n",
            "epoch: 24 train loss: 0.005354592576622963 validation: auc: 0.6632320649504493 --- acc: 82.1 --- loss: 1.0555042028427124\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005001596175134182\n",
            "epoch: 25 train loss: 0.0013014853466302156 validation: auc: 0.6664481667677905 --- acc: 82.25 --- loss: 1.0610941648483276\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034979176707565783\n",
            "epoch: 26 train loss: 0.006980070844292641 validation: auc: 0.6692019878073444 --- acc: 82.5 --- loss: 1.0616137981414795\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00041962023824453353\n",
            "epoch: 27 train loss: 0.004871662240475416 validation: auc: 0.6712190488572998 --- acc: 82.35 --- loss: 1.0447190999984741\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00044314470142126083\n",
            "epoch: 28 train loss: 0.0027435666415840387 validation: auc: 0.6724812920747739 --- acc: 82.39999999999999 --- loss: 1.022829532623291\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039537237025797365\n",
            "epoch: 29 train loss: 0.003049211110919714 validation: auc: 0.6738482707809657 --- acc: 82.39999999999999 --- loss: 1.0176661014556885\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004056485369801521\n",
            "epoch: 30 train loss: 0.0039197467267513275 validation: auc: 0.6755764063448036 --- acc: 82.45 --- loss: 1.0212945938110352\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004021216183900833\n",
            "epoch: 31 train loss: 0.00584398815408349 validation: auc: 0.6774056658287827 --- acc: 82.25 --- loss: 1.0176541805267334\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004768424201756716\n",
            "epoch: 32 train loss: 0.0029417872428894043 validation: auc: 0.6789207188466094 --- acc: 82.25 --- loss: 1.0085080862045288\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004608296323567629\n",
            "epoch: 33 train loss: 0.004821606446057558 validation: auc: 0.680824015486406 --- acc: 82.39999999999999 --- loss: 0.9997534155845642\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040342058055102824\n",
            "epoch: 34 train loss: 0.005632685963064432 validation: auc: 0.6820356967438097 --- acc: 82.45 --- loss: 0.9865608811378479\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00046673710457980635\n",
            "epoch: 35 train loss: 0.0008526350720785558 validation: auc: 0.6826911964404381 --- acc: 82.45 --- loss: 0.9685943126678467\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00045123444870114325\n",
            "epoch: 36 train loss: 0.003495336277410388 validation: auc: 0.6830234607494726 --- acc: 82.69999999999999 --- loss: 0.9516957998275757\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00043461862951517106\n",
            "epoch: 37 train loss: 0.003984544426202774 validation: auc: 0.6834965762329895 --- acc: 82.65 --- loss: 0.9382397532463074\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00044251708313822744\n",
            "epoch: 38 train loss: 0.002682810416445136 validation: auc: 0.6837331339747479 --- acc: 82.6 --- loss: 0.925709068775177\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003130901139229536\n",
            "epoch: 39 train loss: 0.00666101323440671 validation: auc: 0.6840942908323944 --- acc: 82.6 --- loss: 0.9279560446739197\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003451488679274917\n",
            "epoch: 40 train loss: 0.005119903478771448 validation: auc: 0.685282496894051 --- acc: 82.65 --- loss: 0.9413096308708191\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004136478062719107\n",
            "epoch: 41 train loss: 0.007562192156910896 validation: auc: 0.6880363179336049 --- acc: 82.75 --- loss: 0.9469516277313232\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004316908773034811\n",
            "epoch: 42 train loss: 0.004506505094468594 validation: auc: 0.6909472422062349 --- acc: 82.8 --- loss: 0.9320716261863708\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003694179002195597\n",
            "epoch: 43 train loss: 0.007302478421479464 validation: auc: 0.6928487330617434 --- acc: 82.6 --- loss: 0.8937418460845947\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004179417621344328\n",
            "epoch: 44 train loss: 0.005556151270866394 validation: auc: 0.6914673080812458 --- acc: 82.35 --- loss: 0.8527272343635559\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005606521386653185\n",
            "epoch: 45 train loss: 0.0034572691656649113 validation: auc: 0.6893888503654908 --- acc: 82.3 --- loss: 0.8398523926734924\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005704998504370451\n",
            "epoch: 46 train loss: 0.004690120927989483 validation: auc: 0.6897211146745253 --- acc: 82.6 --- loss: 0.8575584888458252\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00042666024528443814\n",
            "epoch: 47 train loss: 0.0061494773253798485 validation: auc: 0.6899685071220132 --- acc: 82.65 --- loss: 0.9019909501075745\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038758707232773305\n",
            "epoch: 48 train loss: 0.0068628909066319466 validation: auc: 0.6897752882031725 --- acc: 82.8 --- loss: 0.9334441423416138\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004688552115112543\n",
            "epoch: 49 train loss: 0.00554729625582695 validation: auc: 0.6916731674901042 --- acc: 82.75 --- loss: 0.9149206280708313\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004053104668855667\n",
            "epoch: 50 train loss: 0.0043148985132575035 validation: auc: 0.6936125798156656 --- acc: 82.75 --- loss: 0.8621106147766113\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004982902202755213\n",
            "epoch: 51 train loss: 0.003094462212175131 validation: auc: 0.6945588107826991 --- acc: 82.35 --- loss: 0.8154446482658386\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004298397805541754\n",
            "epoch: 52 train loss: 0.0096353180706501 validation: auc: 0.6953190459680448 --- acc: 82.45 --- loss: 0.7963487505912781\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00047043580561876295\n",
            "epoch: 53 train loss: 0.007819627411663532 validation: auc: 0.6954418392996445 --- acc: 82.5 --- loss: 0.8082871437072754\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005202466156333685\n",
            "epoch: 54 train loss: 0.0024770363233983517 validation: auc: 0.6941344514749646 --- acc: 83.0 --- loss: 0.8436776995658875\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000448140874505043\n",
            "epoch: 55 train loss: 0.005207387264817953 validation: auc: 0.6921733697379445 --- acc: 82.89999999999999 --- loss: 0.8842272162437439\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00045189796946942806\n",
            "epoch: 56 train loss: 0.006321979220956564 validation: auc: 0.6909671058334055 --- acc: 82.89999999999999 --- loss: 0.8980220556259155\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00045748250558972357\n",
            "epoch: 57 train loss: 0.00505190622061491 validation: auc: 0.6899323914362486 --- acc: 82.85 --- loss: 0.8767178058624268\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004464545287191868\n",
            "epoch: 58 train loss: 0.0068216826766729355 validation: auc: 0.6898547427118547 --- acc: 82.8 --- loss: 0.8422881960868835\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004733526613563299\n",
            "epoch: 59 train loss: 0.007828359492123127 validation: auc: 0.6924839646355206 --- acc: 82.69999999999999 --- loss: 0.8108146786689758\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005219377111643553\n",
            "epoch: 60 train loss: 0.005117356777191162 validation: auc: 0.6945371413712403 --- acc: 82.75 --- loss: 0.8023468255996704\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039936555549502373\n",
            "epoch: 61 train loss: 0.009166047908365726 validation: auc: 0.6950066452861806 --- acc: 82.8 --- loss: 0.8207631707191467\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005462965928018093\n",
            "epoch: 62 train loss: 0.0028157385531812906 validation: auc: 0.6941344514749646 --- acc: 83.05 --- loss: 0.8538058400154114\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00047639040276408194\n",
            "epoch: 63 train loss: 0.004224632866680622 validation: auc: 0.693973736673312 --- acc: 83.1 --- loss: 0.8728917837142944\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00036660295445472\n",
            "epoch: 64 train loss: 0.007929678075015545 validation: auc: 0.6942409927479704 --- acc: 83.2 --- loss: 0.854100227355957\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00042721787467598916\n",
            "epoch: 65 train loss: 0.005783883389085531 validation: auc: 0.6959402357631966 --- acc: 83.0 --- loss: 0.8107722401618958\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004116963129490614\n",
            "epoch: 66 train loss: 0.006143908016383648 validation: auc: 0.6980060529889341 --- acc: 82.8 --- loss: 0.7840425968170166\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003779493505135179\n",
            "epoch: 67 train loss: 0.007113939616829157 validation: auc: 0.6980042472046459 --- acc: 82.85 --- loss: 0.7891598343849182\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037951532285660503\n",
            "epoch: 68 train loss: 0.00679873488843441 validation: auc: 0.6959059258617202 --- acc: 83.05 --- loss: 0.8162511587142944\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00046340250410139563\n",
            "epoch: 69 train loss: 0.004643614403903484 validation: auc: 0.6922437953251857 --- acc: 83.05 --- loss: 0.8480871319770813\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004550676792860031\n",
            "epoch: 70 train loss: 0.0051094586960971355 validation: auc: 0.6901418624136836 --- acc: 83.05 --- loss: 0.8600428700447083\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004910070449113846\n",
            "epoch: 71 train loss: 0.003508820431306958 validation: auc: 0.6896055444800786 --- acc: 83.25 --- loss: 0.8393279314041138\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00046817422844469547\n",
            "epoch: 72 train loss: 0.0029888611752539873 validation: auc: 0.6895206726185317 --- acc: 83.0 --- loss: 0.8208425045013428\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004595192149281502\n",
            "epoch: 73 train loss: 0.004690550267696381 validation: auc: 0.6911531016150934 --- acc: 83.05 --- loss: 0.8195817470550537\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004440885968506336\n",
            "epoch: 74 train loss: 0.006716000381857157 validation: auc: 0.6939448441247003 --- acc: 83.05 --- loss: 0.8260155320167542\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000424388749524951\n",
            "epoch: 75 train loss: 0.006611004006117582 validation: auc: 0.6973776400566293 --- acc: 83.0 --- loss: 0.8224805593490601\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00041212025098502636\n",
            "epoch: 76 train loss: 0.004926562309265137 validation: auc: 0.6997432174742134 --- acc: 83.1 --- loss: 0.809490442276001\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004042951390147209\n",
            "epoch: 77 train loss: 0.004579657223075628 validation: auc: 0.7005233162867296 --- acc: 83.15 --- loss: 0.7992226481437683\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005346119403839111\n",
            "epoch: 78 train loss: 0.0027749785222113132 validation: auc: 0.700114306145445 --- acc: 83.05 --- loss: 0.7898094058036804\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006097888574004174\n",
            "epoch: 79 train loss: 0.0026518458034843206 validation: auc: 0.6989992343474618 --- acc: 83.05 --- loss: 0.7939857244491577\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004904879257082939\n",
            "epoch: 80 train loss: 0.006332495249807835 validation: auc: 0.6983997139637688 --- acc: 83.05 --- loss: 0.8049818277359009\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004917300771921873\n",
            "epoch: 81 train loss: 0.004175588022917509 validation: auc: 0.6982101066135045 --- acc: 83.15 --- loss: 0.8089084029197693\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004612528719007969\n",
            "epoch: 82 train loss: 0.005038956180214882 validation: auc: 0.698573069255439 --- acc: 83.1 --- loss: 0.8054010272026062\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035449562128633263\n",
            "epoch: 83 train loss: 0.008028107695281506 validation: auc: 0.698798792291468 --- acc: 83.05 --- loss: 0.8101797699928284\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00043273535557091235\n",
            "epoch: 84 train loss: 0.0039162402972579 validation: auc: 0.6985008378839099 --- acc: 83.15 --- loss: 0.820846676826477\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004522846080362797\n",
            "epoch: 85 train loss: 0.0036517903208732605 validation: auc: 0.6974625119181762 --- acc: 83.2 --- loss: 0.8266680836677551\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005828423891216516\n",
            "epoch: 86 train loss: 0.002750384621322155 validation: auc: 0.6968485452601775 --- acc: 83.15 --- loss: 0.8158519268035889\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00046177711337804796\n",
            "epoch: 87 train loss: 0.009858820587396622 validation: auc: 0.6973812516252058 --- acc: 82.89999999999999 --- loss: 0.7989446520805359\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004119012504816055\n",
            "epoch: 88 train loss: 0.009042827412486076 validation: auc: 0.6991274450319264 --- acc: 82.89999999999999 --- loss: 0.7907083630561829\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003725956194102764\n",
            "epoch: 89 train loss: 0.004788725636899471 validation: auc: 0.7002668949178008 --- acc: 82.95 --- loss: 0.7941765785217285\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003915942739695311\n",
            "epoch: 90 train loss: 0.00318325636908412 validation: auc: 0.7001693825662363 --- acc: 83.05 --- loss: 0.800201416015625\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040405597537755965\n",
            "epoch: 91 train loss: 0.005690919701009989 validation: auc: 0.6998136430614545 --- acc: 83.3 --- loss: 0.8076284527778625\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039862878620624544\n",
            "epoch: 92 train loss: 0.008717113174498081 validation: auc: 0.6995301349282019 --- acc: 83.2 --- loss: 0.8096519112586975\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0005391620565205812\n",
            "epoch: 93 train loss: 0.0012245788238942623 validation: auc: 0.7001097916847244 --- acc: 83.0 --- loss: 0.8014392256736755\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004068624693900347\n",
            "epoch: 94 train loss: 0.004293526988476515 validation: auc: 0.7008971136343937 --- acc: 82.85 --- loss: 0.7945610880851746\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00042187441140413285\n",
            "epoch: 95 train loss: 0.00292999017983675 validation: auc: 0.7018090346999509 --- acc: 82.69999999999999 --- loss: 0.7883298397064209\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004169892054051161\n",
            "epoch: 96 train loss: 0.002625267021358013 validation: auc: 0.7027155384126434 --- acc: 82.8 --- loss: 0.7896735668182373\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035084940027445554\n",
            "epoch: 97 train loss: 0.006110718939453363 validation: auc: 0.7024374476322557 --- acc: 82.95 --- loss: 0.7965425848960876\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000381116708740592\n",
            "epoch: 98 train loss: 0.0046030073426663876 validation: auc: 0.7012113201005461 --- acc: 83.05 --- loss: 0.8133066296577454\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004314652644097805\n",
            "epoch: 99 train loss: 0.001794750802218914 validation: auc: 0.7003896882494005 --- acc: 83.2 --- loss: 0.8402755260467529\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00045782257802784443\n",
            "epoch: 100 train loss: 0.0031230084132403135 validation: auc: 0.700183828840542 --- acc: 83.15 --- loss: 0.8442133665084839\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004551449790596962\n",
            "epoch: 101 train loss: 0.003845280036330223 validation: auc: 0.6996294530640547 --- acc: 82.95 --- loss: 0.8256617188453674\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00041275382973253726\n",
            "epoch: 102 train loss: 0.004546368028968573 validation: auc: 0.6990606310132617 --- acc: 82.75 --- loss: 0.8007791638374329\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004308369942009449\n",
            "epoch: 103 train loss: 0.0032724214252084494 validation: auc: 0.6993910895380082 --- acc: 82.8 --- loss: 0.7894243597984314\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035563874989748\n",
            "epoch: 104 train loss: 0.007272996474057436 validation: auc: 0.7008718326543585 --- acc: 82.89999999999999 --- loss: 0.7968729734420776\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00045260023325681685\n",
            "epoch: 105 train loss: 0.0025460892356932163 validation: auc: 0.7019336338158391 --- acc: 83.0 --- loss: 0.8123053312301636\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00030084573663771155\n",
            "epoch: 106 train loss: 0.007889394648373127 validation: auc: 0.7021936667533443 --- acc: 83.05 --- loss: 0.8269383907318115\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035872256848961114\n",
            "epoch: 107 train loss: 0.0039286185055971146 validation: auc: 0.7020636502845916 --- acc: 83.15 --- loss: 0.8376312255859375\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003828029613941908\n",
            "epoch: 108 train loss: 0.0031444502528756857 validation: auc: 0.7033132530120482 --- acc: 83.0 --- loss: 0.8262317776679993\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000378356478177011\n",
            "epoch: 109 train loss: 0.005412851460278034 validation: auc: 0.7033222819334893 --- acc: 82.75 --- loss: 0.8164427876472473\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000382265355437994\n",
            "epoch: 110 train loss: 0.006236609071493149 validation: auc: 0.7026938690011846 --- acc: 82.5 --- loss: 0.811992883682251\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00043483981862664224\n",
            "epoch: 111 train loss: 0.00318133388645947 validation: auc: 0.7024645343965792 --- acc: 82.5 --- loss: 0.8094422221183777\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004033583216369152\n",
            "epoch: 112 train loss: 0.0036778890062123537 validation: auc: 0.7024139724365087 --- acc: 82.6 --- loss: 0.8062269687652588\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000326761300675571\n",
            "epoch: 113 train loss: 0.006309313699603081 validation: auc: 0.7013991216665222 --- acc: 82.8 --- loss: 0.7999660968780518\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032231458462774756\n",
            "epoch: 114 train loss: 0.007311779074370861 validation: auc: 0.699941853745919 --- acc: 82.89999999999999 --- loss: 0.7973033785820007\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00044765067286789417\n",
            "epoch: 115 train loss: 0.003462781896814704 validation: auc: 0.6998010025714367 --- acc: 83.0 --- loss: 0.810043454170227\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003693899139761925\n",
            "epoch: 116 train loss: 0.006243413779884577 validation: auc: 0.6991960648348791 --- acc: 83.3 --- loss: 0.8298150300979614\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038846684619784353\n",
            "epoch: 117 train loss: 0.0030693053267896175 validation: auc: 0.6991978706191674 --- acc: 83.3 --- loss: 0.8442999720573425\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040128468535840513\n",
            "epoch: 118 train loss: 0.0033938277047127485 validation: auc: 0.6992574615006789 --- acc: 83.25 --- loss: 0.8403128385543823\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00041547543369233606\n",
            "epoch: 119 train loss: 0.0042884438298642635 validation: auc: 0.7002614775649361 --- acc: 82.8 --- loss: 0.8163751363754272\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004640029277652502\n",
            "epoch: 120 train loss: 0.0020941696129739285 validation: auc: 0.70099282020167 --- acc: 82.8 --- loss: 0.7978150844573975\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004241911228746176\n",
            "epoch: 121 train loss: 0.0033278611954301596 validation: auc: 0.7016609603883159 --- acc: 82.8 --- loss: 0.7969034910202026\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003375955857336521\n",
            "epoch: 122 train loss: 0.005773743614554405 validation: auc: 0.7009205888301409 --- acc: 82.89999999999999 --- loss: 0.817500650882721\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031573099549859763\n",
            "epoch: 123 train loss: 0.004680686164647341 validation: auc: 0.6997820418364105 --- acc: 83.2 --- loss: 0.8535401821136475\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003246326232329011\n",
            "epoch: 124 train loss: 0.004138358868658543 validation: auc: 0.6994506804195199 --- acc: 83.25 --- loss: 0.8709456324577332\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004202011041343212\n",
            "epoch: 125 train loss: 0.002484661526978016 validation: auc: 0.6992213458149145 --- acc: 83.35000000000001 --- loss: 0.8488596677780151\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003897499991580844\n",
            "epoch: 126 train loss: 0.006510396022349596 validation: auc: 0.6985306333246656 --- acc: 83.05 --- loss: 0.822350263595581\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004673078656196594\n",
            "epoch: 127 train loss: 0.0027788972947746515 validation: auc: 0.6978561728930108 --- acc: 83.0 --- loss: 0.816909670829773\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034272689372301103\n",
            "epoch: 128 train loss: 0.005654068663716316 validation: auc: 0.6971970616278063 --- acc: 83.0 --- loss: 0.8195385932922363\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003177937585860491\n",
            "epoch: 129 train loss: 0.004289706237614155 validation: auc: 0.6973361070180001 --- acc: 82.95 --- loss: 0.8239371180534363\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003504240419715643\n",
            "epoch: 130 train loss: 0.0032696647103875875 validation: auc: 0.6980493918118517 --- acc: 83.0 --- loss: 0.828343391418457\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031949395779520275\n",
            "epoch: 131 train loss: 0.006014521233737469 validation: auc: 0.6985550114125567 --- acc: 83.0 --- loss: 0.8359851241111755\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039089745841920377\n",
            "epoch: 132 train loss: 0.0034187298733741045 validation: auc: 0.699530134928202 --- acc: 83.05 --- loss: 0.8448951244354248\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037281590048223736\n",
            "epoch: 133 train loss: 0.004389513283967972 validation: auc: 0.6990299326803616 --- acc: 83.0 --- loss: 0.8372219204902649\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002986536826938391\n",
            "epoch: 134 train loss: 0.005453731864690781 validation: auc: 0.6973541648608823 --- acc: 82.8 --- loss: 0.8128328919410706\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034140350762754677\n",
            "epoch: 135 train loss: 0.0030864542350172997 validation: auc: 0.695172777440698 --- acc: 82.75 --- loss: 0.8036141395568848\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004275592975318432\n",
            "epoch: 136 train loss: 0.0022330177016556263 validation: auc: 0.6947375834272341 --- acc: 82.75 --- loss: 0.8155763149261475\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035734823904931545\n",
            "epoch: 137 train loss: 0.007832221686840057 validation: auc: 0.6953768310652683 --- acc: 82.95 --- loss: 0.8311588168144226\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040661301463842394\n",
            "epoch: 138 train loss: 0.002830218756571412 validation: auc: 0.6965921238912485 --- acc: 82.95 --- loss: 0.8476355075836182\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003516117110848427\n",
            "epoch: 139 train loss: 0.003437414765357971 validation: auc: 0.6987644823899917 --- acc: 82.89999999999999 --- loss: 0.8492770791053772\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033899876289069655\n",
            "epoch: 140 train loss: 0.004882108420133591 validation: auc: 0.7007057004998412 --- acc: 82.69999999999999 --- loss: 0.8316947221755981\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003254193114116788\n",
            "epoch: 141 train loss: 0.00641253124922514 validation: auc: 0.7012356981884372 --- acc: 82.75 --- loss: 0.8241333961486816\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00044591818004846573\n",
            "epoch: 142 train loss: 0.0015708815772086382 validation: auc: 0.7010533139753257 --- acc: 82.85 --- loss: 0.8295658826828003\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003346615470945835\n",
            "epoch: 143 train loss: 0.007263534236699343 validation: auc: 0.6996384819854959 --- acc: 82.8 --- loss: 0.8344502449035645\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003369057783856988\n",
            "epoch: 144 train loss: 0.004951495677232742 validation: auc: 0.6975473837797233 --- acc: 82.8 --- loss: 0.8362066745758057\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003483518259599805\n",
            "epoch: 145 train loss: 0.002900074701756239 validation: auc: 0.6957307647857618 --- acc: 82.95 --- loss: 0.8317959308624268\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037130434066057205\n",
            "epoch: 146 train loss: 0.004401479847729206 validation: auc: 0.6951998642050216 --- acc: 82.75 --- loss: 0.8262657523155212\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004092007875442505\n",
            "epoch: 147 train loss: 0.0036715022288262844 validation: auc: 0.6965505908526191 --- acc: 82.85 --- loss: 0.8227989673614502\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031912445556372406\n",
            "epoch: 148 train loss: 0.00664255628362298 validation: auc: 0.6985459824911155 --- acc: 82.85 --- loss: 0.8215212225914001\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038482036907225845\n",
            "epoch: 149 train loss: 0.0030282556544989347 validation: auc: 0.700518801826009 --- acc: 83.1 --- loss: 0.8394713997840881\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003296198323369026\n",
            "epoch: 150 train loss: 0.006635029334574938 validation: auc: 0.7018334127878421 --- acc: 83.1 --- loss: 0.8588292002677917\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037175314500927927\n",
            "epoch: 151 train loss: 0.0054985834285616875 validation: auc: 0.7003896882494005 --- acc: 82.95 --- loss: 0.8488191962242126\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003758571809157729\n",
            "epoch: 152 train loss: 0.005715701729059219 validation: auc: 0.6987382985178123 --- acc: 82.69999999999999 --- loss: 0.8207830190658569\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039403345435857774\n",
            "epoch: 153 train loss: 0.0034814276732504368 validation: auc: 0.6972033818728149 --- acc: 82.69999999999999 --- loss: 0.8163043856620789\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039036909583956003\n",
            "epoch: 154 train loss: 0.004523687530308962 validation: auc: 0.6968205556037098 --- acc: 82.95 --- loss: 0.8353139758110046\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038988024462014435\n",
            "epoch: 155 train loss: 0.0034595890901982784 validation: auc: 0.6974164644188263 --- acc: 83.1 --- loss: 0.8534117341041565\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003229079768061638\n",
            "epoch: 156 train loss: 0.006912725046277046 validation: auc: 0.698308521857213 --- acc: 83.1 --- loss: 0.8483232855796814\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00041311080567538736\n",
            "epoch: 157 train loss: 0.003629082115367055 validation: auc: 0.6989360318973736 --- acc: 82.95 --- loss: 0.8236302137374878\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003526402171701193\n",
            "epoch: 158 train loss: 0.005789576098322868 validation: auc: 0.6987238522435064 --- acc: 82.8 --- loss: 0.8125046491622925\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004145819693803787\n",
            "epoch: 159 train loss: 0.002437100512906909 validation: auc: 0.6992105111091849 --- acc: 83.0 --- loss: 0.8216699361801147\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038296461571007967\n",
            "epoch: 160 train loss: 0.0028984982054680586 validation: auc: 0.6999680376180983 --- acc: 83.05 --- loss: 0.83486407995224\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035566031001508234\n",
            "epoch: 161 train loss: 0.005678087472915649 validation: auc: 0.7003761448672388 --- acc: 82.95 --- loss: 0.8305157423019409\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003372524632140994\n",
            "epoch: 162 train loss: 0.005684224888682365 validation: auc: 0.7001431986940567 --- acc: 82.85 --- loss: 0.8185296654701233\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033724403474479916\n",
            "epoch: 163 train loss: 0.0037500702310353518 validation: auc: 0.6978047080407963 --- acc: 82.69999999999999 --- loss: 0.8156372308731079\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035799825564026834\n",
            "epoch: 164 train loss: 0.005187179893255234 validation: auc: 0.6961397749270464 --- acc: 82.85 --- loss: 0.8178564310073853\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003896364476531744\n",
            "epoch: 165 train loss: 0.004376553930342197 validation: auc: 0.6961181055155875 --- acc: 83.1 --- loss: 0.8347146511077881\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037837529089301825\n",
            "epoch: 166 train loss: 0.004837288986891508 validation: auc: 0.6962210352200167 --- acc: 83.05 --- loss: 0.8584237098693848\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004149056505411863\n",
            "epoch: 167 train loss: 0.004404493607580662 validation: auc: 0.6965280185490162 --- acc: 83.05 --- loss: 0.8583315014839172\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004609677009284496\n",
            "epoch: 168 train loss: 0.0018246789695695043 validation: auc: 0.6971401794227269 --- acc: 83.15 --- loss: 0.8488410115242004\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039682984352111815\n",
            "epoch: 169 train loss: 0.004502679221332073 validation: auc: 0.6982850466614661 --- acc: 83.15 --- loss: 0.8312785625457764\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040202122181653976\n",
            "epoch: 170 train loss: 0.005938177928328514 validation: auc: 0.6980864103897604 --- acc: 82.75 --- loss: 0.8089277148246765\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003677086206153035\n",
            "epoch: 171 train loss: 0.004303310066461563 validation: auc: 0.697880550980902 --- acc: 82.85 --- loss: 0.8029809594154358\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003523977939039469\n",
            "epoch: 172 train loss: 0.00358149828389287 validation: auc: 0.6987094059692006 --- acc: 82.8 --- loss: 0.8132207989692688\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003445442067459226\n",
            "epoch: 173 train loss: 0.005389128811657429 validation: auc: 0.6993233726271995 --- acc: 82.95 --- loss: 0.8385694622993469\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004124147351831198\n",
            "epoch: 174 train loss: 0.0031753669027239084 validation: auc: 0.6988520629279709 --- acc: 83.15 --- loss: 0.8634265661239624\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039350492879748346\n",
            "epoch: 175 train loss: 0.0037764492444694042 validation: auc: 0.6989856909653 --- acc: 83.2 --- loss: 0.8598355054855347\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004115927964448929\n",
            "epoch: 176 train loss: 0.003745388938114047 validation: auc: 0.699366711450117 --- acc: 82.85 --- loss: 0.8387694358825684\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003553234739229083\n",
            "epoch: 177 train loss: 0.00421888055279851 validation: auc: 0.6973677082430442 --- acc: 82.65 --- loss: 0.8130396604537964\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032833837904036047\n",
            "epoch: 178 train loss: 0.005237000063061714 validation: auc: 0.6958309858137586 --- acc: 82.85 --- loss: 0.803209662437439\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003326109843328595\n",
            "epoch: 179 train loss: 0.0058676209300756454 validation: auc: 0.6971239273641328 --- acc: 82.85 --- loss: 0.8244475722312927\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003582998178899288\n",
            "epoch: 180 train loss: 0.003338746028020978 validation: auc: 0.6973406214787206 --- acc: 82.89999999999999 --- loss: 0.8511462807655334\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003461065934970975\n",
            "epoch: 181 train loss: 0.0026123893912881613 validation: auc: 0.6973749313801971 --- acc: 83.05 --- loss: 0.8684661984443665\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003717097919434309\n",
            "epoch: 182 train loss: 0.002642771927639842 validation: auc: 0.6980340426454018 --- acc: 83.05 --- loss: 0.868887186050415\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003687666263431311\n",
            "epoch: 183 train loss: 0.0038744693156331778 validation: auc: 0.6969505720724625 --- acc: 82.85 --- loss: 0.851492702960968\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034364962484687565\n",
            "epoch: 184 train loss: 0.003538358025252819 validation: auc: 0.6939638048597266 --- acc: 82.75 --- loss: 0.8343426585197449\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002487532095983624\n",
            "epoch: 185 train loss: 0.005757323931902647 validation: auc: 0.6917860290081189 --- acc: 82.69999999999999 --- loss: 0.8310716152191162\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002833572681993246\n",
            "epoch: 186 train loss: 0.005382061004638672 validation: auc: 0.6923891609603883 --- acc: 82.8 --- loss: 0.8431154489517212\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003451877273619175\n",
            "epoch: 187 train loss: 0.002350365975871682 validation: auc: 0.6962083947299992 --- acc: 83.05 --- loss: 0.8610045313835144\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032757623121142386\n",
            "epoch: 188 train loss: 0.005347044672816992 validation: auc: 0.6974345222617087 --- acc: 83.1 --- loss: 0.8639760613441467\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002420837292447686\n",
            "epoch: 189 train loss: 0.007781976368278265 validation: auc: 0.697288253734362 --- acc: 82.95 --- loss: 0.8549367189407349\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000314722559414804\n",
            "epoch: 190 train loss: 0.003060195129364729 validation: auc: 0.6961343575741816 --- acc: 82.85 --- loss: 0.8428231477737427\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003810484195128083\n",
            "epoch: 191 train loss: 0.002640567719936371 validation: auc: 0.6960296220854643 --- acc: 82.89999999999999 --- loss: 0.8419889211654663\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003854989307001233\n",
            "epoch: 192 train loss: 0.005321858450770378 validation: auc: 0.695764171795094 --- acc: 83.2 --- loss: 0.8585494160652161\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003616807982325554\n",
            "epoch: 193 train loss: 0.00593260582536459 validation: auc: 0.693375119181763 --- acc: 83.1 --- loss: 0.8754246830940247\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035333645064383745\n",
            "epoch: 194 train loss: 0.005026023369282484 validation: auc: 0.6906375502008033 --- acc: 83.1 --- loss: 0.8675392270088196\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000303733954206109\n",
            "epoch: 195 train loss: 0.005769195035099983 validation: auc: 0.6884742206235012 --- acc: 82.89999999999999 --- loss: 0.854508101940155\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038629048503935337\n",
            "epoch: 196 train loss: 0.0032526517752557993 validation: auc: 0.6887920386582299 --- acc: 83.0 --- loss: 0.8541285991668701\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003446622518822551\n",
            "epoch: 197 train loss: 0.005864739418029785 validation: auc: 0.6921706610615124 --- acc: 83.0 --- loss: 0.8612911105155945\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0004117946606129408\n",
            "epoch: 198 train loss: 0.002830162411555648 validation: auc: 0.6952079902343185 --- acc: 82.95 --- loss: 0.8730174899101257\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003504724008962512\n",
            "epoch: 199 train loss: 0.004472886212170124 validation: auc: 0.6966345598220219 --- acc: 82.8 --- loss: 0.879332423210144\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00028627109713852406\n",
            "epoch: 200 train loss: 0.005324193742126226 validation: auc: 0.6971293447169975 --- acc: 82.89999999999999 --- loss: 0.8797014355659485\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031878254376351836\n",
            "epoch: 201 train loss: 0.0031156667973846197 validation: auc: 0.6963239649244459 --- acc: 82.89999999999999 --- loss: 0.8878766298294067\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00039387280121445657\n",
            "epoch: 202 train loss: 0.001346843782812357 validation: auc: 0.6960657377712289 --- acc: 82.89999999999999 --- loss: 0.8811686038970947\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00037450871895998715\n",
            "epoch: 203 train loss: 0.0028610082808882 validation: auc: 0.6944856765190257 --- acc: 82.8 --- loss: 0.8593324422836304\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035754090640693903\n",
            "epoch: 204 train loss: 0.0028604594990611076 validation: auc: 0.6920478677299124 --- acc: 82.65 --- loss: 0.8408646583557129\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00038502628449350597\n",
            "epoch: 205 train loss: 0.004117934964597225 validation: auc: 0.6926392620843085 --- acc: 82.8 --- loss: 0.8541017770767212\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00040350710041821004\n",
            "epoch: 206 train loss: 0.0015427429461851716 validation: auc: 0.6950328291583601 --- acc: 83.0 --- loss: 0.888636589050293\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032196005340665577\n",
            "epoch: 207 train loss: 0.004933861084282398 validation: auc: 0.6958093164022999 --- acc: 82.95 --- loss: 0.8987290263175964\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003116208827123046\n",
            "epoch: 208 train loss: 0.005322137847542763 validation: auc: 0.6961795021813875 --- acc: 82.8 --- loss: 0.8744298815727234\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031189208384603263\n",
            "epoch: 209 train loss: 0.0024571276735514402 validation: auc: 0.6971148984426917 --- acc: 82.75 --- loss: 0.8467605113983154\n",
            "<Timer(Thread-5769, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00036242371425032616\n",
            "epoch: 210 train loss: 0.0018770188326016068 validation: auc: 0.6971058695212504 --- acc: 82.85 --- loss: 0.8473330736160278\n",
            "time up, break\n",
            "test auc: 0.6739439303211758 test acc: 82.19999999999999 test loss 0.8742157220840454\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37310631275177003\n",
            "epoch: 0 train loss: 1.5004674196243286 validation: auc: 0.5756744183520714 --- acc: 83.45 --- loss: 3.6985950469970703\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.340663480758667\n",
            "epoch: 1 train loss: 3.6685471534729004 validation: auc: 0.6090898723659988 --- acc: 78.75 --- loss: 3.5606260299682617\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.25262372493743895\n",
            "epoch: 2 train loss: 1.5810447931289673 validation: auc: 0.6536332518160377 --- acc: 77.8 --- loss: 1.6985368728637695\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04120626151561737\n",
            "epoch: 3 train loss: 0.4259319305419922 validation: auc: 0.5567184793253191 --- acc: 58.35 --- loss: 3.126338243484497\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07928590178489685\n",
            "epoch: 4 train loss: 0.369558721780777 validation: auc: 0.5373914586044795 --- acc: 79.10000000000001 --- loss: 2.0085084438323975\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01759456843137741\n",
            "epoch: 5 train loss: 0.2041487842798233 validation: auc: 0.542163026144063 --- acc: 81.8 --- loss: 2.5941598415374756\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01832971125841141\n",
            "epoch: 6 train loss: 0.19993971288204193 validation: auc: 0.5498987218498332 --- acc: 81.2 --- loss: 2.930595874786377\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01583310067653656\n",
            "epoch: 7 train loss: 0.12785704433918 validation: auc: 0.5532285012462914 --- acc: 81.0 --- loss: 3.110938549041748\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013421057164669037\n",
            "epoch: 8 train loss: 0.11916542053222656 validation: auc: 0.5536321657232743 --- acc: 80.45 --- loss: 3.2845253944396973\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011209087073802948\n",
            "epoch: 9 train loss: 0.144079327583313 validation: auc: 0.5503901788251734 --- acc: 80.35 --- loss: 3.4891021251678467\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009097689390182495\n",
            "epoch: 10 train loss: 0.12338805943727493 validation: auc: 0.541673379323328 --- acc: 81.05 --- loss: 3.689540147781372\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008148330450057983\n",
            "epoch: 11 train loss: 0.07684242725372314 validation: auc: 0.5340164253428886 --- acc: 81.5 --- loss: 3.8593387603759766\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007856683433055877\n",
            "epoch: 12 train loss: 0.07002460956573486 validation: auc: 0.5328307740764139 --- acc: 81.6 --- loss: 3.9536936283111572\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006511842459440231\n",
            "epoch: 13 train loss: 0.05800480395555496 validation: auc: 0.5368882356242046 --- acc: 81.5 --- loss: 3.9779226779937744\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006091170385479927\n",
            "epoch: 14 train loss: 0.03839028999209404 validation: auc: 0.5416145492986556 --- acc: 81.3 --- loss: 3.977766275405884\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004602449759840965\n",
            "epoch: 15 train loss: 0.08099786937236786 validation: auc: 0.5457923861276992 --- acc: 81.45 --- loss: 3.949765205383301\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004750968888401985\n",
            "epoch: 16 train loss: 0.052519045770168304 validation: auc: 0.549974748343256 --- acc: 81.45 --- loss: 3.927915096282959\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00473085455596447\n",
            "epoch: 17 train loss: 0.04139569401741028 validation: auc: 0.5549535785851469 --- acc: 81.5 --- loss: 3.9149115085601807\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004325215518474579\n",
            "epoch: 18 train loss: 0.039643947035074234 validation: auc: 0.5610556097596295 --- acc: 81.3 --- loss: 3.8839292526245117\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003381277993321419\n",
            "epoch: 19 train loss: 0.06094186753034592 validation: auc: 0.5657375746462505 --- acc: 81.3 --- loss: 3.851661443710327\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003708859533071518\n",
            "epoch: 20 train loss: 0.04096518084406853 validation: auc: 0.5693551686249523 --- acc: 81.39999999999999 --- loss: 3.812696933746338\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0037177909165620805\n",
            "epoch: 21 train loss: 0.028148896992206573 validation: auc: 0.5730234469326024 --- acc: 81.35 --- loss: 3.7731921672821045\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003802971914410591\n",
            "epoch: 22 train loss: 0.03395582735538483 validation: auc: 0.5777651469211984 --- acc: 81.45 --- loss: 3.7320947647094727\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003405165299773216\n",
            "epoch: 23 train loss: 0.03958689048886299 validation: auc: 0.5815637563604308 --- acc: 81.55 --- loss: 3.69016432762146\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002770540677011013\n",
            "epoch: 24 train loss: 0.04778209701180458 validation: auc: 0.5837974871433769 --- acc: 81.55 --- loss: 3.634920835494995\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003108520433306694\n",
            "epoch: 25 train loss: 0.029700500890612602 validation: auc: 0.5866638669608771 --- acc: 81.6 --- loss: 3.598680257797241\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021058032289147377\n",
            "epoch: 26 train loss: 0.06656298786401749 validation: auc: 0.5908371784034074 --- acc: 81.55 --- loss: 3.53959321975708\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029309261590242386\n",
            "epoch: 27 train loss: 0.03700976073741913 validation: auc: 0.5944538673048065 --- acc: 81.6 --- loss: 3.479330539703369\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033851023763418198\n",
            "epoch: 28 train loss: 0.020570244640111923 validation: auc: 0.59668669301045 --- acc: 81.6 --- loss: 3.4055211544036865\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030309649184346197\n",
            "epoch: 29 train loss: 0.034733399748802185 validation: auc: 0.5983990992670684 --- acc: 81.65 --- loss: 3.33197021484375\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023687191307544707\n",
            "epoch: 30 train loss: 0.057333849370479584 validation: auc: 0.6004219470384966 --- acc: 81.65 --- loss: 3.2605531215667725\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032131124287843706\n",
            "epoch: 31 train loss: 0.024158800020813942 validation: auc: 0.6027199383099311 --- acc: 81.65 --- loss: 3.2039527893066406\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003108467161655426\n",
            "epoch: 32 train loss: 0.026485679671168327 validation: auc: 0.6043536028412186 --- acc: 81.6 --- loss: 3.164048671722412\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034986644983291625\n",
            "epoch: 33 train loss: 0.005109657533466816 validation: auc: 0.6066298722573895 --- acc: 81.6 --- loss: 3.1177425384521484\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003538985550403595\n",
            "epoch: 34 train loss: 0.001010625041089952 validation: auc: 0.6093405787788335 --- acc: 81.45 --- loss: 3.056523084640503\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022682422772049905\n",
            "epoch: 35 train loss: 0.05619502067565918 validation: auc: 0.6110285479482803 --- acc: 81.5 --- loss: 2.9873151779174805\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031141696497797968\n",
            "epoch: 36 train loss: 0.01756598986685276 validation: auc: 0.6120096517443555 --- acc: 81.39999999999999 --- loss: 2.9254424571990967\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025503430515527725\n",
            "epoch: 37 train loss: 0.03934576362371445 validation: auc: 0.6136740889039333 --- acc: 81.39999999999999 --- loss: 2.8734164237976074\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029504232108592985\n",
            "epoch: 38 train loss: 0.01974223181605339 validation: auc: 0.615854420126023 --- acc: 81.45 --- loss: 2.8193788528442383\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033651165664196014\n",
            "epoch: 39 train loss: 0.00988862756639719 validation: auc: 0.6173912413859268 --- acc: 81.39999999999999 --- loss: 2.7787954807281494\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028878767043352127\n",
            "epoch: 40 train loss: 0.03580576181411743 validation: auc: 0.619260226015904 --- acc: 81.35 --- loss: 2.731431245803833\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029328394681215285\n",
            "epoch: 41 train loss: 0.03133304417133331 validation: auc: 0.6212885042511481 --- acc: 81.39999999999999 --- loss: 2.6810412406921387\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025172214955091476\n",
            "epoch: 42 train loss: 0.037400439381599426 validation: auc: 0.6230515948367149 --- acc: 81.45 --- loss: 2.6212470531463623\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026072733104228973\n",
            "epoch: 43 train loss: 0.03003224916756153 validation: auc: 0.6246472461212912 --- acc: 81.5 --- loss: 2.5695834159851074\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030478667467832565\n",
            "epoch: 44 train loss: 0.01048498135060072 validation: auc: 0.6252120143581462 --- acc: 81.5 --- loss: 2.5152902603149414\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002807307057082653\n",
            "epoch: 45 train loss: 0.01820070669054985 validation: auc: 0.6254609106163757 --- acc: 81.55 --- loss: 2.4573607444763184\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002634938806295395\n",
            "epoch: 46 train loss: 0.027478497475385666 validation: auc: 0.626163250603234 --- acc: 81.5 --- loss: 2.406895160675049\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00313175804913044\n",
            "epoch: 47 train loss: 0.009003696031868458 validation: auc: 0.62875177168882 --- acc: 81.65 --- loss: 2.366657018661499\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026650652289390564\n",
            "epoch: 48 train loss: 0.02635478414595127 validation: auc: 0.6311665179322966 --- acc: 81.69999999999999 --- loss: 2.3284645080566406\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002241644635796547\n",
            "epoch: 49 train loss: 0.034824322909116745 validation: auc: 0.6335151935326796 --- acc: 81.75 --- loss: 2.2963619232177734\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023991579189896585\n",
            "epoch: 50 train loss: 0.01997677981853485 validation: auc: 0.6349642222942262 --- acc: 81.85 --- loss: 2.265082836151123\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010487590916454792\n",
            "epoch: 51 train loss: 0.04438677430152893 validation: auc: 0.636495613090314 --- acc: 81.89999999999999 --- loss: 2.2355129718780518\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0014672900550067424\n",
            "epoch: 52 train loss: 0.027232909575104713 validation: auc: 0.6381030303798247 --- acc: 81.89999999999999 --- loss: 2.2022829055786133\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019609015434980392\n",
            "epoch: 53 train loss: 0.026955824345350266 validation: auc: 0.6396353262532153 --- acc: 81.89999999999999 --- loss: 2.1682193279266357\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020718149840831756\n",
            "epoch: 54 train loss: 0.02125084400177002 validation: auc: 0.6415224124292457 --- acc: 81.95 --- loss: 2.1481716632843018\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020774204283952715\n",
            "epoch: 55 train loss: 0.016571713611483574 validation: auc: 0.6428691674555924 --- acc: 82.0 --- loss: 2.1190295219421387\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010091166943311692\n",
            "epoch: 56 train loss: 0.06456427276134491 validation: auc: 0.6439045758898267 --- acc: 82.05 --- loss: 2.099034070968628\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001559988036751747\n",
            "epoch: 57 train loss: 0.05076475441455841 validation: auc: 0.6437126995016644 --- acc: 82.0 --- loss: 2.0801098346710205\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002707364968955517\n",
            "epoch: 58 train loss: 0.01176432240754366 validation: auc: 0.644469344126682 --- acc: 81.95 --- loss: 2.0595688819885254\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026265161111950876\n",
            "epoch: 59 train loss: 0.011221442371606827 validation: auc: 0.6457672249786854 --- acc: 82.05 --- loss: 2.0462489128112793\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026875603944063187\n",
            "epoch: 60 train loss: 0.011096271686255932 validation: auc: 0.6463989689359368 --- acc: 82.05 --- loss: 2.031304121017456\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002252956107258797\n",
            "epoch: 61 train loss: 0.028074687346816063 validation: auc: 0.6455753485905231 --- acc: 82.05 --- loss: 2.009122610092163\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002375801093876362\n",
            "epoch: 62 train loss: 0.01915682666003704 validation: auc: 0.6444602933536554 --- acc: 82.0 --- loss: 1.9912960529327393\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002497313544154167\n",
            "epoch: 63 train loss: 0.012934446334838867 validation: auc: 0.6448730086036649 --- acc: 82.0 --- loss: 1.9685708284378052\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002073019556701183\n",
            "epoch: 64 train loss: 0.022301876917481422 validation: auc: 0.6444177547204307 --- acc: 82.05 --- loss: 1.9509713649749756\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024092743173241616\n",
            "epoch: 65 train loss: 0.004427925683557987 validation: auc: 0.6443797414737192 --- acc: 82.05 --- loss: 1.944397211074829\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0012855026870965958\n",
            "epoch: 66 train loss: 0.04574555903673172 validation: auc: 0.643380536131591 --- acc: 82.0 --- loss: 1.9362605810165405\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002190194837749004\n",
            "epoch: 67 train loss: 0.009401371702551842 validation: auc: 0.6435072469539624 --- acc: 82.05 --- loss: 1.921278953552246\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024775397032499313\n",
            "epoch: 68 train loss: 0.0003596892929635942 validation: auc: 0.6435561211283056 --- acc: 82.0 --- loss: 1.9078197479248047\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002464212477207184\n",
            "epoch: 69 train loss: 0.0011373863089829683 validation: auc: 0.6441308452154899 --- acc: 82.1 --- loss: 1.896274447441101\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022464461624622347\n",
            "epoch: 70 train loss: 0.009188312105834484 validation: auc: 0.6443688805460874 --- acc: 82.1 --- loss: 1.890335202217102\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024140620604157447\n",
            "epoch: 71 train loss: 0.0017133114160969853 validation: auc: 0.6423614190888043 --- acc: 82.05 --- loss: 1.899233341217041\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001296217180788517\n",
            "epoch: 72 train loss: 0.04578298702836037 validation: auc: 0.6396887258140718 --- acc: 82.1 --- loss: 1.8990862369537354\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019999884068965914\n",
            "epoch: 73 train loss: 0.019465738907456398 validation: auc: 0.6387121474045099 --- acc: 82.05 --- loss: 1.8762316703796387\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019968481734395026\n",
            "epoch: 74 train loss: 0.019803857430815697 validation: auc: 0.6390615072433335 --- acc: 82.05 --- loss: 1.8466441631317139\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023170953616499903\n",
            "epoch: 75 train loss: 0.00519405584782362 validation: auc: 0.6401874234078332 --- acc: 82.0 --- loss: 1.8237208127975464\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022825371474027635\n",
            "epoch: 76 train loss: 0.010152880102396011 validation: auc: 0.642620271197363 --- acc: 82.15 --- loss: 1.8002361059188843\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017966073006391524\n",
            "epoch: 77 train loss: 0.026060029864311218 validation: auc: 0.6454993220971003 --- acc: 82.25 --- loss: 1.7757325172424316\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002102501317858696\n",
            "epoch: 78 train loss: 0.013828255236148834 validation: auc: 0.6452721476941344 --- acc: 82.19999999999999 --- loss: 1.758155345916748\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024443380534648895\n",
            "epoch: 79 train loss: 0.0037361944559961557 validation: auc: 0.6437027436513353 --- acc: 82.39999999999999 --- loss: 1.7496260404586792\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020892074331641196\n",
            "epoch: 80 train loss: 0.020201250910758972 validation: auc: 0.643653869476992 --- acc: 82.39999999999999 --- loss: 1.7295281887054443\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002406594716012478\n",
            "epoch: 81 train loss: 0.010901079513132572 validation: auc: 0.644039432407922 --- acc: 82.39999999999999 --- loss: 1.7144619226455688\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023820094764232637\n",
            "epoch: 82 train loss: 0.009944895282387733 validation: auc: 0.6434076884506706 --- acc: 82.39999999999999 --- loss: 1.7100236415863037\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021265802904963495\n",
            "epoch: 83 train loss: 0.018401319161057472 validation: auc: 0.6424048627993317 --- acc: 82.39999999999999 --- loss: 1.7023746967315674\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020962672308087347\n",
            "epoch: 84 train loss: 0.022034481167793274 validation: auc: 0.6413423020460177 --- acc: 82.35 --- loss: 1.6849511861801147\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010590865276753902\n",
            "epoch: 85 train loss: 0.05909455940127373 validation: auc: 0.6408951938585076 --- acc: 82.39999999999999 --- loss: 1.6652863025665283\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025182461366057398\n",
            "epoch: 86 train loss: 0.001803604420274496 validation: auc: 0.642724355087168 --- acc: 82.39999999999999 --- loss: 1.647566556930542\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023216137662529944\n",
            "epoch: 87 train loss: 0.01292482390999794 validation: auc: 0.6460966731168509 --- acc: 82.5 --- loss: 1.6467134952545166\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002007558196783066\n",
            "epoch: 88 train loss: 0.01953830011188984 validation: auc: 0.648950381852114 --- acc: 82.5 --- loss: 1.6373157501220703\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0012484485283493995\n",
            "epoch: 89 train loss: 0.05138605460524559 validation: auc: 0.6497377991054216 --- acc: 82.5 --- loss: 1.626030683517456\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022162172943353655\n",
            "epoch: 90 train loss: 0.01701335236430168 validation: auc: 0.6503930750725418 --- acc: 82.5 --- loss: 1.6126465797424316\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021941686049103738\n",
            "epoch: 91 train loss: 0.01349733117967844 validation: auc: 0.6505577991416247 --- acc: 82.6 --- loss: 1.6063481569290161\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002133631147444248\n",
            "epoch: 92 train loss: 0.013209488242864609 validation: auc: 0.6502591236317494 --- acc: 82.5 --- loss: 1.5982376337051392\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021073512732982637\n",
            "epoch: 93 train loss: 0.018214447423815727 validation: auc: 0.6497613311152906 --- acc: 82.65 --- loss: 1.5900332927703857\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023316048085689546\n",
            "epoch: 94 train loss: 0.00990225188434124 validation: auc: 0.6520086380577765 --- acc: 82.69999999999999 --- loss: 1.575355052947998\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016448454931378364\n",
            "epoch: 95 train loss: 0.03193766996264458 validation: auc: 0.6531390796087894 --- acc: 82.75 --- loss: 1.574683666229248\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001673225313425064\n",
            "epoch: 96 train loss: 0.03337208926677704 validation: auc: 0.6538559008324901 --- acc: 82.85 --- loss: 1.5882641077041626\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021674666553735734\n",
            "epoch: 97 train loss: 0.014133360236883163 validation: auc: 0.6543138699476323 --- acc: 82.8 --- loss: 1.5892313718795776\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002159465104341507\n",
            "epoch: 98 train loss: 0.009129884652793407 validation: auc: 0.6548677772568556 --- acc: 82.69999999999999 --- loss: 1.5798237323760986\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002202627621591091\n",
            "epoch: 99 train loss: 0.006451526191085577 validation: auc: 0.6558289693522724 --- acc: 82.5 --- loss: 1.5644221305847168\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016209743916988373\n",
            "epoch: 100 train loss: 0.024859488010406494 validation: auc: 0.6561982408917546 --- acc: 82.45 --- loss: 1.5413389205932617\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019166836515069007\n",
            "epoch: 101 train loss: 0.012739703990519047 validation: auc: 0.6561303600940556 --- acc: 82.45 --- loss: 1.5204499959945679\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022101061418652534\n",
            "epoch: 102 train loss: 0.006256974767893553 validation: auc: 0.6557049737618089 --- acc: 82.5 --- loss: 1.5025566816329956\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002391773648560047\n",
            "epoch: 103 train loss: 0.0022131791338324547 validation: auc: 0.6526168500051588 --- acc: 82.39999999999999 --- loss: 1.4992729425430298\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021875200793147085\n",
            "epoch: 104 train loss: 0.012950878590345383 validation: auc: 0.6528675564179937 --- acc: 82.35 --- loss: 1.4908957481384277\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016900155693292617\n",
            "epoch: 105 train loss: 0.029028205201029778 validation: auc: 0.6538712871466352 --- acc: 82.35 --- loss: 1.472406029701233\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017657123506069183\n",
            "epoch: 106 train loss: 0.02596193552017212 validation: auc: 0.6547745542946822 --- acc: 82.3 --- loss: 1.4537413120269775\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0015852782875299455\n",
            "epoch: 107 train loss: 0.031009159982204437 validation: auc: 0.6552343335644298 --- acc: 82.35 --- loss: 1.43919837474823\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013051078654825687\n",
            "epoch: 108 train loss: 0.04580453038215637 validation: auc: 0.6561910002733333 --- acc: 82.39999999999999 --- loss: 1.4257102012634277\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002451895736157894\n",
            "epoch: 109 train loss: 0.005124750081449747 validation: auc: 0.6590962984148476 --- acc: 82.45 --- loss: 1.4188952445983887\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017157118767499925\n",
            "epoch: 110 train loss: 0.030683910474181175 validation: auc: 0.6637990800794296 --- acc: 82.39999999999999 --- loss: 1.4183940887451172\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023603525012731554\n",
            "epoch: 111 train loss: 0.0005786860710941255 validation: auc: 0.6682574908722954 --- acc: 82.39999999999999 --- loss: 1.4193260669708252\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001353011466562748\n",
            "epoch: 112 train loss: 0.04450361430644989 validation: auc: 0.6690322370433659 --- acc: 82.5 --- loss: 1.4188772439956665\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018695110455155372\n",
            "epoch: 113 train loss: 0.029050996527075768 validation: auc: 0.6695173584775875 --- acc: 82.39999999999999 --- loss: 1.4061439037322998\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002195645123720169\n",
            "epoch: 114 train loss: 0.010746907442808151 validation: auc: 0.669044908125603 --- acc: 82.3 --- loss: 1.3889814615249634\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016568772494792937\n",
            "epoch: 115 train loss: 0.0314650759100914 validation: auc: 0.6693119059298854 --- acc: 82.19999999999999 --- loss: 1.3727577924728394\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021673081442713737\n",
            "epoch: 116 train loss: 0.009632589295506477 validation: auc: 0.6677081089495854 --- acc: 82.15 --- loss: 1.3744193315505981\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020888183265924455\n",
            "epoch: 117 train loss: 0.011495820246636868 validation: auc: 0.6665586607752167 --- acc: 82.35 --- loss: 1.377113699913025\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023176975548267363\n",
            "epoch: 118 train loss: 0.0006821398274041712 validation: auc: 0.6668971596864088 --- acc: 82.39999999999999 --- loss: 1.3788071870803833\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021242890506982803\n",
            "epoch: 119 train loss: 0.010808160528540611 validation: auc: 0.6649087048524814 --- acc: 82.5 --- loss: 1.375313401222229\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002295161783695221\n",
            "epoch: 120 train loss: 0.0034521007910370827 validation: auc: 0.6612757245596346 --- acc: 82.45 --- loss: 1.3773938417434692\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016295455396175384\n",
            "epoch: 121 train loss: 0.028205161914229393 validation: auc: 0.6594999628918305 --- acc: 82.45 --- loss: 1.3678640127182007\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018040155991911888\n",
            "epoch: 122 train loss: 0.027348782867193222 validation: auc: 0.657465349115468 --- acc: 82.39999999999999 --- loss: 1.371280312538147\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0015437791123986244\n",
            "epoch: 123 train loss: 0.03367648646235466 validation: auc: 0.6532015299426723 --- acc: 82.45 --- loss: 1.3826665878295898\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019640643149614334\n",
            "epoch: 124 train loss: 0.01846178248524666 validation: auc: 0.6525064305742354 --- acc: 82.6 --- loss: 1.3952386379241943\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020506205037236214\n",
            "epoch: 125 train loss: 0.018331890925765038 validation: auc: 0.6530006027814836 --- acc: 82.69999999999999 --- loss: 1.399375319480896\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023167096078395844\n",
            "epoch: 126 train loss: 0.005572934169322252 validation: auc: 0.6529444879887192 --- acc: 82.6 --- loss: 1.4078667163848877\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0012961779721081257\n",
            "epoch: 127 train loss: 0.03951358050107956 validation: auc: 0.652647622633449 --- acc: 82.65 --- loss: 1.42617928981781\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016572341322898866\n",
            "epoch: 128 train loss: 0.027577504515647888 validation: auc: 0.6548523909427104 --- acc: 82.69999999999999 --- loss: 1.4328373670578003\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022342633455991745\n",
            "epoch: 129 train loss: 0.009793062694370747 validation: auc: 0.6570643998703929 --- acc: 82.75 --- loss: 1.4208608865737915\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002237364836037159\n",
            "epoch: 130 train loss: 0.011796252802014351 validation: auc: 0.6578029429493574 --- acc: 82.55 --- loss: 1.3996917009353638\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001501120813190937\n",
            "epoch: 131 train loss: 0.03662837669253349 validation: auc: 0.6574137597092168 --- acc: 82.35 --- loss: 1.386895775794983\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001848137564957142\n",
            "epoch: 132 train loss: 0.02929152175784111 validation: auc: 0.6572707574953978 --- acc: 82.3 --- loss: 1.391067385673523\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022140011191368104\n",
            "epoch: 133 train loss: 0.017887888476252556 validation: auc: 0.6572417950217129 --- acc: 82.39999999999999 --- loss: 1.3982808589935303\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023777777329087257\n",
            "epoch: 134 train loss: 0.0060045248828828335 validation: auc: 0.6572327442486863 --- acc: 82.6 --- loss: 1.4200372695922852\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022767962887883185\n",
            "epoch: 135 train loss: 0.004399364814162254 validation: auc: 0.656559366735513 --- acc: 82.65 --- loss: 1.4364888668060303\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019809171557426454\n",
            "epoch: 136 train loss: 0.023139601573348045 validation: auc: 0.6569087265743367 --- acc: 82.55 --- loss: 1.4421212673187256\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023919038474559786\n",
            "epoch: 137 train loss: 0.013749170117080212 validation: auc: 0.6576889032092231 --- acc: 82.45 --- loss: 1.4323607683181763\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002601723372936249\n",
            "epoch: 138 train loss: 0.010434179566800594 validation: auc: 0.6579015963753464 --- acc: 82.45 --- loss: 1.4209258556365967\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025788716971874236\n",
            "epoch: 139 train loss: 0.010543566197156906 validation: auc: 0.656952170284864 --- acc: 82.39999999999999 --- loss: 1.419716477394104\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002578520216047764\n",
            "epoch: 140 train loss: 0.01067583542317152 validation: auc: 0.6549953931565295 --- acc: 82.45 --- loss: 1.4062445163726807\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026248745620250703\n",
            "epoch: 141 train loss: 0.012953490950167179 validation: auc: 0.6549048854262643 --- acc: 82.45 --- loss: 1.3950804471969604\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022203166037797926\n",
            "epoch: 142 train loss: 0.028564510866999626 validation: auc: 0.6562787927716908 --- acc: 82.39999999999999 --- loss: 1.395097255706787\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002169051207602024\n",
            "epoch: 143 train loss: 0.027577051892876625 validation: auc: 0.65674943296907 --- acc: 82.3 --- loss: 1.3868409395217896\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022612715139985083\n",
            "epoch: 144 train loss: 0.023059803992509842 validation: auc: 0.6577432078473823 --- acc: 82.35 --- loss: 1.3721308708190918\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026371540501713754\n",
            "epoch: 145 train loss: 0.012652995996177197 validation: auc: 0.6591542233622173 --- acc: 82.35 --- loss: 1.3537323474884033\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018201060593128204\n",
            "epoch: 146 train loss: 0.040432438254356384 validation: auc: 0.660493737770143 --- acc: 82.45 --- loss: 1.3478937149047852\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001766977459192276\n",
            "epoch: 147 train loss: 0.04017816111445427 validation: auc: 0.661678483959315 --- acc: 82.55 --- loss: 1.3448498249053955\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002577652223408222\n",
            "epoch: 148 train loss: 0.0007423985516652465 validation: auc: 0.6617762323080014 --- acc: 82.69999999999999 --- loss: 1.3497296571731567\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001993524841964245\n",
            "epoch: 149 train loss: 0.020834241062402725 validation: auc: 0.6614359232422041 --- acc: 82.55 --- loss: 1.3520424365997314\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016537833958864211\n",
            "epoch: 150 train loss: 0.03543565794825554 validation: auc: 0.6606738481533707 --- acc: 82.39999999999999 --- loss: 1.329995036125183\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018481168895959853\n",
            "epoch: 151 train loss: 0.024159681051969528 validation: auc: 0.6611707355925269 --- acc: 82.5 --- loss: 1.3142073154449463\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016743650659918786\n",
            "epoch: 152 train loss: 0.031445182859897614 validation: auc: 0.6626822146879565 --- acc: 82.5 --- loss: 1.3078244924545288\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002216963469982147\n",
            "epoch: 153 train loss: 0.01152113452553749 validation: auc: 0.6647584620202412 --- acc: 82.39999999999999 --- loss: 1.2969716787338257\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001991230994462967\n",
            "epoch: 154 train loss: 0.02247455157339573 validation: auc: 0.6674736939281984 --- acc: 82.39999999999999 --- loss: 1.2871321439743042\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001228578481823206\n",
            "epoch: 155 train loss: 0.05220817029476166 validation: auc: 0.6697816410499621 --- acc: 82.35 --- loss: 1.2805149555206299\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016873219981789588\n",
            "epoch: 156 train loss: 0.03201692923903465 validation: auc: 0.6716008464282935 --- acc: 82.3 --- loss: 1.2796998023986816\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023133356124162676\n",
            "epoch: 157 train loss: 0.009424128569662571 validation: auc: 0.6728082195500318 --- acc: 82.45 --- loss: 1.2784212827682495\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002372090332210064\n",
            "epoch: 158 train loss: 0.012464914470911026 validation: auc: 0.6739151290911758 --- acc: 82.35 --- loss: 1.2735525369644165\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021558647975325586\n",
            "epoch: 159 train loss: 0.022047335281968117 validation: auc: 0.6730625462720772 --- acc: 82.35 --- loss: 1.279531478881836\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022691145539283754\n",
            "epoch: 160 train loss: 0.015184750780463219 validation: auc: 0.6696884180877889 --- acc: 82.5 --- loss: 1.2794605493545532\n",
            "<Timer(Thread-6193, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002596445009112358\n",
            "epoch: 161 train loss: 0.0031352159567177296 validation: auc: 0.6657060779561182 --- acc: 82.6 --- loss: 1.2872321605682373\n",
            "time up, break\n",
            "test auc: 0.6512784754620411 test acc: 82.8 test loss 1.2583823204040527\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.25495181083679197\n",
            "epoch: 0 train loss: 0.9227175116539001 validation: auc: 0.5003545949893271 --- acc: 17.2 --- loss: 22.83136749267578\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 2.297131156921387\n",
            "epoch: 1 train loss: 22.92800521850586 validation: auc: 0.5046448432760363 --- acc: 18.0 --- loss: 22.597156524658203\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 2.27333927154541\n",
            "epoch: 2 train loss: 22.71942901611328 validation: auc: 0.5056252457589034 --- acc: 18.65 --- loss: 22.42595100402832\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 2.256224822998047\n",
            "epoch: 3 train loss: 22.026708602905273 validation: auc: 0.5055646837433997 --- acc: 20.349999999999998 --- loss: 21.902090072631836\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 2.185517501831055\n",
            "epoch: 4 train loss: 21.116437911987305 validation: auc: 0.5199547452533424 --- acc: 25.35 --- loss: 20.588438034057617\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 2.0302007675170897\n",
            "epoch: 5 train loss: 19.933992385864258 validation: auc: 0.525556292832266 --- acc: 30.75 --- loss: 19.1807918548584\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 1.8683847427368163\n",
            "epoch: 6 train loss: 16.923561096191406 validation: auc: 0.5699605381417818 --- acc: 42.85 --- loss: 15.610065460205078\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 1.5251921653747558\n",
            "epoch: 7 train loss: 12.2068510055542 validation: auc: 0.6224976477362093 --- acc: 65.45 --- loss: 9.250779151916504\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.8771206855773925\n",
            "epoch: 8 train loss: 5.748352527618408 validation: auc: 0.5689976898663072 --- acc: 83.15 --- loss: 4.964923858642578\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4427828788757324\n",
            "epoch: 9 train loss: 4.326027870178223 validation: auc: 0.5240167888439502 --- acc: 82.89999999999999 --- loss: 4.865257263183594\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.454998254776001\n",
            "epoch: 10 train loss: 4.2571821212768555 validation: auc: 0.5210790922368274 --- acc: 82.89999999999999 --- loss: 4.7274017333984375\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.44278674125671386\n",
            "epoch: 11 train loss: 4.3967814445495605 validation: auc: 0.5326578474328726 --- acc: 83.05 --- loss: 4.814449787139893\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4312326431274414\n",
            "epoch: 12 train loss: 4.305884838104248 validation: auc: 0.5523571789686552 --- acc: 83.35000000000001 --- loss: 4.682075500488281\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4309051513671875\n",
            "epoch: 13 train loss: 4.448596000671387 validation: auc: 0.563483035613976 --- acc: 83.45 --- loss: 4.583676338195801\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4313089370727539\n",
            "epoch: 14 train loss: 4.442193508148193 validation: auc: 0.5663496376811594 --- acc: 83.45 --- loss: 4.613772392272949\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4414808750152588\n",
            "epoch: 15 train loss: 4.088123321533203 validation: auc: 0.5672264703404112 --- acc: 83.15 --- loss: 4.613635540008545\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.42935662269592284\n",
            "epoch: 16 train loss: 4.356334209442139 validation: auc: 0.5712841253791708 --- acc: 83.15 --- loss: 4.699257850646973\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.43609418869018557\n",
            "epoch: 17 train loss: 4.434608459472656 validation: auc: 0.5684982726659925 --- acc: 82.65 --- loss: 4.756994247436523\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4401679992675781\n",
            "epoch: 18 train loss: 4.300789833068848 validation: auc: 0.5721486700932479 --- acc: 82.3 --- loss: 5.006351470947266\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.43354091644287107\n",
            "epoch: 19 train loss: 4.427009105682373 validation: auc: 0.5740840214582631 --- acc: 82.25 --- loss: 4.988670349121094\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.42464613914489746\n",
            "epoch: 20 train loss: 4.614651679992676 validation: auc: 0.5768689964610717 --- acc: 82.25 --- loss: 4.905816555023193\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.43582825660705565\n",
            "epoch: 21 train loss: 4.146683216094971 validation: auc: 0.5771823390630266 --- acc: 81.95 --- loss: 5.01792573928833\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4349184036254883\n",
            "epoch: 22 train loss: 4.212639331817627 validation: auc: 0.5800980929109089 --- acc: 81.55 --- loss: 5.082842826843262\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4300790786743164\n",
            "epoch: 23 train loss: 4.490705490112305 validation: auc: 0.5841364383215369 --- acc: 81.3 --- loss: 5.166276454925537\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4352424144744873\n",
            "epoch: 24 train loss: 4.322580814361572 validation: auc: 0.5746053814178183 --- acc: 81.55 --- loss: 5.101728439331055\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4245391845703125\n",
            "epoch: 25 train loss: 4.207108974456787 validation: auc: 0.5717089371980676 --- acc: 82.1 --- loss: 4.990714073181152\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4153994083404541\n",
            "epoch: 26 train loss: 4.182480335235596 validation: auc: 0.573483667565442 --- acc: 81.69999999999999 --- loss: 5.102760314941406\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.414399242401123\n",
            "epoch: 27 train loss: 4.320115089416504 validation: auc: 0.5669315596562182 --- acc: 81.89999999999999 --- loss: 5.003914833068848\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4076539993286133\n",
            "epoch: 28 train loss: 4.365220546722412 validation: auc: 0.5660740366250984 --- acc: 82.15 --- loss: 4.890011787414551\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4008176803588867\n",
            "epoch: 29 train loss: 4.29434061050415 validation: auc: 0.5709602502527805 --- acc: 82.25 --- loss: 4.930026054382324\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.40254888534545896\n",
            "epoch: 30 train loss: 4.110308647155762 validation: auc: 0.5717668660824626 --- acc: 82.35 --- loss: 4.917240619659424\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.39395880699157715\n",
            "epoch: 31 train loss: 4.385809421539307 validation: auc: 0.5739444655094934 --- acc: 82.39999999999999 --- loss: 4.921072959899902\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4102003574371338\n",
            "epoch: 32 train loss: 3.66087007522583 validation: auc: 0.5717010378047409 --- acc: 82.45 --- loss: 4.831289291381836\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4029237747192383\n",
            "epoch: 33 train loss: 3.9821889400482178 validation: auc: 0.5718616588023817 --- acc: 82.55 --- loss: 4.873205184936523\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4052002429962158\n",
            "epoch: 34 train loss: 3.944650888442993 validation: auc: 0.5707101027974385 --- acc: 82.55 --- loss: 4.866893768310547\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4028801441192627\n",
            "epoch: 35 train loss: 3.83707332611084 validation: auc: 0.5656843332209864 --- acc: 82.75 --- loss: 4.736227512359619\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3949763774871826\n",
            "epoch: 36 train loss: 3.9564881324768066 validation: auc: 0.5675880870126953 --- acc: 82.95 --- loss: 4.73059606552124\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3894472122192383\n",
            "epoch: 37 train loss: 3.955822229385376 validation: auc: 0.5645696410515673 --- acc: 83.15 --- loss: 4.684779167175293\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.385715126991272\n",
            "epoch: 38 train loss: 4.076498031616211 validation: auc: 0.5625447632288507 --- acc: 83.05 --- loss: 4.719182968139648\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.39467449188232423\n",
            "epoch: 39 train loss: 3.823974609375 validation: auc: 0.5543320273003033 --- acc: 82.95 --- loss: 4.734464168548584\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.39587714672088625\n",
            "epoch: 40 train loss: 3.8484396934509277 validation: auc: 0.5516365787551961 --- acc: 82.75 --- loss: 4.811385154724121\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.39341919422149657\n",
            "epoch: 41 train loss: 3.8631088733673096 validation: auc: 0.550674608190091 --- acc: 82.75 --- loss: 4.8373589515686035\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38408718109130857\n",
            "epoch: 42 train loss: 4.051031589508057 validation: auc: 0.5511555934726435 --- acc: 82.6 --- loss: 4.823470115661621\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38065526485443113\n",
            "epoch: 43 train loss: 4.07572603225708 validation: auc: 0.5565903760813391 --- acc: 82.6 --- loss: 4.84945011138916\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3887049674987793\n",
            "epoch: 44 train loss: 3.8102097511291504 validation: auc: 0.5552887316031907 --- acc: 82.8 --- loss: 4.82938289642334\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3865537643432617\n",
            "epoch: 45 train loss: 3.880272150039673 validation: auc: 0.5559566691944726 --- acc: 82.85 --- loss: 4.769092082977295\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38631105422973633\n",
            "epoch: 46 train loss: 3.8429291248321533 validation: auc: 0.5603724300640378 --- acc: 82.75 --- loss: 4.800542831420898\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38463940620422366\n",
            "epoch: 47 train loss: 3.8117477893829346 validation: auc: 0.561538029434895 --- acc: 83.05 --- loss: 4.71621561050415\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3806741952896118\n",
            "epoch: 48 train loss: 3.8856887817382812 validation: auc: 0.5652568882709809 --- acc: 83.25 --- loss: 4.712424278259277\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3827205181121826\n",
            "epoch: 49 train loss: 3.8432369232177734 validation: auc: 0.5652305569598922 --- acc: 83.1 --- loss: 4.64906644821167\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3908968925476074\n",
            "epoch: 50 train loss: 3.5859878063201904 validation: auc: 0.5658396879564094 --- acc: 83.25 --- loss: 4.603823184967041\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37777960300445557\n",
            "epoch: 51 train loss: 4.073732376098633 validation: auc: 0.5646749662959218 --- acc: 83.25 --- loss: 4.6588826179504395\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37682056427001953\n",
            "epoch: 52 train loss: 4.009286403656006 validation: auc: 0.5627141613301876 --- acc: 83.35000000000001 --- loss: 4.610044002532959\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3828604221343994\n",
            "epoch: 53 train loss: 3.7562241554260254 validation: auc: 0.5626702758117066 --- acc: 83.3 --- loss: 4.649251461029053\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38070087432861327\n",
            "epoch: 54 train loss: 3.860504150390625 validation: auc: 0.5610965411189754 --- acc: 83.0 --- loss: 4.6939377784729\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3795607328414917\n",
            "epoch: 55 train loss: 3.820608615875244 validation: auc: 0.5596351533535558 --- acc: 83.0 --- loss: 4.700010776519775\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.378814697265625\n",
            "epoch: 56 train loss: 3.809072971343994 validation: auc: 0.5596737726098192 --- acc: 83.0 --- loss: 4.703885078430176\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3786247968673706\n",
            "epoch: 57 train loss: 3.809922456741333 validation: auc: 0.5595921455454443 --- acc: 83.05 --- loss: 4.707720756530762\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38729674816131593\n",
            "epoch: 58 train loss: 3.4278807640075684 validation: auc: 0.5624578699022582 --- acc: 83.05 --- loss: 4.755037307739258\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3864114284515381\n",
            "epoch: 59 train loss: 3.4823315143585205 validation: auc: 0.5634400278058646 --- acc: 83.1 --- loss: 4.699941635131836\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38199450969696047\n",
            "epoch: 60 train loss: 3.758063316345215 validation: auc: 0.5636954415234243 --- acc: 83.2 --- loss: 4.671080112457275\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38473846912384035\n",
            "epoch: 61 train loss: 3.6353979110717773 validation: auc: 0.5633048604089429 --- acc: 83.3 --- loss: 4.637099742889404\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37413864135742186\n",
            "epoch: 62 train loss: 4.011236667633057 validation: auc: 0.5629836184136614 --- acc: 83.15 --- loss: 4.677889823913574\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3826289176940918\n",
            "epoch: 63 train loss: 3.7478644847869873 validation: auc: 0.5616170233681609 --- acc: 83.2 --- loss: 4.662898540496826\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38208966255187987\n",
            "epoch: 64 train loss: 3.7532577514648438 validation: auc: 0.5604092938995618 --- acc: 83.25 --- loss: 4.624828338623047\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37443017959594727\n",
            "epoch: 65 train loss: 4.015087604522705 validation: auc: 0.5599563953488372 --- acc: 83.2 --- loss: 4.625072002410889\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3855557918548584\n",
            "epoch: 66 train loss: 3.5682413578033447 validation: auc: 0.5611000519604539 --- acc: 83.05 --- loss: 4.702518939971924\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3845192909240723\n",
            "epoch: 67 train loss: 3.608880043029785 validation: auc: 0.5617793997865409 --- acc: 83.0 --- loss: 4.723019599914551\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37985405921936033\n",
            "epoch: 68 train loss: 3.777388334274292 validation: auc: 0.5612299530951579 --- acc: 83.0 --- loss: 4.696521759033203\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3843525886535645\n",
            "epoch: 69 train loss: 3.74617862701416 validation: auc: 0.5629809852825525 --- acc: 83.0 --- loss: 4.681804656982422\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38607561588287354\n",
            "epoch: 70 train loss: 3.5417654514312744 validation: auc: 0.5619496755982475 --- acc: 83.0 --- loss: 4.701077461242676\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37940452098846433\n",
            "epoch: 71 train loss: 3.974400520324707 validation: auc: 0.5660231294236603 --- acc: 83.1 --- loss: 4.694561004638672\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.385417103767395\n",
            "epoch: 72 train loss: 3.705209255218506 validation: auc: 0.5667463627682282 --- acc: 83.15 --- loss: 4.691715717315674\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38349759578704834\n",
            "epoch: 73 train loss: 3.892791748046875 validation: auc: 0.5671694191663859 --- acc: 83.05 --- loss: 4.713779926300049\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3857719421386719\n",
            "epoch: 74 train loss: 3.7652997970581055 validation: auc: 0.5676732249185485 --- acc: 83.1 --- loss: 4.682885646820068\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3849250555038452\n",
            "epoch: 75 train loss: 3.782020330429077 validation: auc: 0.5677241321199866 --- acc: 83.2 --- loss: 4.651062488555908\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.384088134765625\n",
            "epoch: 76 train loss: 3.8510749340057373 validation: auc: 0.5691758650713403 --- acc: 83.2 --- loss: 4.67770528793335\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3830798387527466\n",
            "epoch: 77 train loss: 3.869332790374756 validation: auc: 0.569093360296596 --- acc: 83.1 --- loss: 4.699298858642578\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3853808641433716\n",
            "epoch: 78 train loss: 3.756100654602051 validation: auc: 0.567980423547916 --- acc: 83.15 --- loss: 4.683584213256836\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3820913076400757\n",
            "epoch: 79 train loss: 3.897472620010376 validation: auc: 0.566874508482193 --- acc: 83.2 --- loss: 4.672750949859619\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.390028715133667\n",
            "epoch: 80 train loss: 3.5701348781585693 validation: auc: 0.5658133566453207 --- acc: 83.3 --- loss: 4.642000675201416\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3860083818435669\n",
            "epoch: 81 train loss: 3.7000434398651123 validation: auc: 0.5647504493877092 --- acc: 83.35000000000001 --- loss: 4.594783306121826\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3802853345870972\n",
            "epoch: 82 train loss: 3.9409737586975098 validation: auc: 0.5651550738681047 --- acc: 83.3 --- loss: 4.644532680511475\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37704131603240965\n",
            "epoch: 83 train loss: 3.9532439708709717 validation: auc: 0.5609640068531626 --- acc: 83.2 --- loss: 4.645883560180664\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3758741855621338\n",
            "epoch: 84 train loss: 3.9381914138793945 validation: auc: 0.561078986911583 --- acc: 83.1 --- loss: 4.741150856018066\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38031888008117676\n",
            "epoch: 85 train loss: 3.621354579925537 validation: auc: 0.56064539798899 --- acc: 83.2 --- loss: 4.71875524520874\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38187096118927\n",
            "epoch: 86 train loss: 3.5802295207977295 validation: auc: 0.5586802044714076 --- acc: 83.15 --- loss: 4.683248519897461\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37102370262145995\n",
            "epoch: 87 train loss: 3.993027925491333 validation: auc: 0.5642694641051567 --- acc: 83.2 --- loss: 4.74542760848999\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3702263593673706\n",
            "epoch: 88 train loss: 3.975989580154419 validation: auc: 0.5642606870014606 --- acc: 83.5 --- loss: 4.584334373474121\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37680835723876954\n",
            "epoch: 89 train loss: 3.588080406188965 validation: auc: 0.5605444612964836 --- acc: 83.35000000000001 --- loss: 4.602145195007324\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.366679310798645\n",
            "epoch: 90 train loss: 4.045569896697998 validation: auc: 0.5581746432985057 --- acc: 83.25 --- loss: 4.674497604370117\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37074205875396726\n",
            "epoch: 91 train loss: 3.8687222003936768 validation: auc: 0.5569879788787777 --- acc: 83.25 --- loss: 4.648582458496094\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37172865867614746\n",
            "epoch: 92 train loss: 3.774468421936035 validation: auc: 0.5552448460847096 --- acc: 83.25 --- loss: 4.6638312339782715\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3726794242858887\n",
            "epoch: 93 train loss: 3.7255094051361084 validation: auc: 0.5549385251657116 --- acc: 83.15 --- loss: 4.670384883880615\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37829272747039794\n",
            "epoch: 94 train loss: 3.5214426517486572 validation: auc: 0.5535780740927986 --- acc: 83.2 --- loss: 4.613724708557129\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3750668287277222\n",
            "epoch: 95 train loss: 3.6506454944610596 validation: auc: 0.5530101954836536 --- acc: 83.25 --- loss: 4.631361484527588\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3786630153656006\n",
            "epoch: 96 train loss: 3.496227502822876 validation: auc: 0.5531927592405348 --- acc: 83.2 --- loss: 4.659281253814697\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3694899559020996\n",
            "epoch: 97 train loss: 3.809114933013916 validation: auc: 0.553491180766206 --- acc: 83.15 --- loss: 4.675946235656738\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3638655424118042\n",
            "epoch: 98 train loss: 3.9967682361602783 validation: auc: 0.5540774912931131 --- acc: 83.15 --- loss: 4.672921657562256\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37631223201751707\n",
            "epoch: 99 train loss: 3.4928858280181885 validation: auc: 0.5546243048533873 --- acc: 82.95 --- loss: 4.689942359924316\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3700298070907593\n",
            "epoch: 100 train loss: 3.7733702659606934 validation: auc: 0.555132499157398 --- acc: 82.89999999999999 --- loss: 4.746633529663086\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3726254940032959\n",
            "epoch: 101 train loss: 3.6897027492523193 validation: auc: 0.5535912397483428 --- acc: 82.69999999999999 --- loss: 4.783674716949463\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36870739459991453\n",
            "epoch: 102 train loss: 3.8178741931915283 validation: auc: 0.5533630350522414 --- acc: 82.69999999999999 --- loss: 4.776391506195068\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37411048412323\n",
            "epoch: 103 train loss: 3.596212863922119 validation: auc: 0.5519499213571508 --- acc: 82.75 --- loss: 4.753419399261475\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3684859037399292\n",
            "epoch: 104 train loss: 3.7787537574768066 validation: auc: 0.5530900671272891 --- acc: 82.8 --- loss: 4.807834148406982\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3649431228637695\n",
            "epoch: 105 train loss: 3.9196653366088867 validation: auc: 0.5516848528255253 --- acc: 82.75 --- loss: 4.768285751342773\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3712796211242676\n",
            "epoch: 106 train loss: 3.7426421642303467 validation: auc: 0.5515224764071452 --- acc: 82.95 --- loss: 4.760220527648926\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37022287845611573\n",
            "epoch: 107 train loss: 3.700274705886841 validation: auc: 0.5496485647680036 --- acc: 82.95 --- loss: 4.74783182144165\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3671047925949097\n",
            "epoch: 108 train loss: 3.7234997749328613 validation: auc: 0.5473129774744411 --- acc: 82.85 --- loss: 4.7465105056762695\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3647733211517334\n",
            "epoch: 109 train loss: 3.8100409507751465 validation: auc: 0.5470154336591394 --- acc: 82.8 --- loss: 4.740683555603027\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3670817375183105\n",
            "epoch: 110 train loss: 3.79182767868042 validation: auc: 0.5492061987417145 --- acc: 82.89999999999999 --- loss: 4.74352502822876\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3693371057510376\n",
            "epoch: 111 train loss: 3.8236968517303467 validation: auc: 0.551991173744523 --- acc: 82.95 --- loss: 4.776575088500977\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3732920169830322\n",
            "epoch: 112 train loss: 3.6393613815307617 validation: auc: 0.5508624382091899 --- acc: 83.1 --- loss: 4.709213733673096\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37486658096313474\n",
            "epoch: 113 train loss: 3.5753917694091797 validation: auc: 0.5515330089315806 --- acc: 83.1 --- loss: 4.670637607574463\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36334378719329835\n",
            "epoch: 114 train loss: 3.977806329727173 validation: auc: 0.553187492978317 --- acc: 83.1 --- loss: 4.700613498687744\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36733269691467285\n",
            "epoch: 115 train loss: 3.8281259536743164 validation: auc: 0.5543416821143692 --- acc: 83.05 --- loss: 4.712356090545654\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3715912580490112\n",
            "epoch: 116 train loss: 3.6446335315704346 validation: auc: 0.5541020671834626 --- acc: 83.05 --- loss: 4.706584930419922\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3701006889343262\n",
            "epoch: 117 train loss: 3.670823574066162 validation: auc: 0.553671989102348 --- acc: 82.95 --- loss: 4.732937335968018\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37243924140930174\n",
            "epoch: 118 train loss: 3.5655038356781006 validation: auc: 0.5541204991012246 --- acc: 82.89999999999999 --- loss: 4.784641265869141\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3642155647277832\n",
            "epoch: 119 train loss: 3.900871515274048 validation: auc: 0.5507211268396809 --- acc: 82.89999999999999 --- loss: 4.745295524597168\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3711176872253418\n",
            "epoch: 120 train loss: 3.6407477855682373 validation: auc: 0.5532515658352993 --- acc: 82.85 --- loss: 4.792998313903809\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3757005214691162\n",
            "epoch: 121 train loss: 3.4536542892456055 validation: auc: 0.5526766655431974 --- acc: 82.8 --- loss: 4.783621311187744\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3681567907333374\n",
            "epoch: 122 train loss: 3.8161730766296387 validation: auc: 0.5516708094596113 --- acc: 82.95 --- loss: 4.758451461791992\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3671478986740112\n",
            "epoch: 123 train loss: 3.815559148788452 validation: auc: 0.556050584204022 --- acc: 82.89999999999999 --- loss: 4.8655853271484375\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3690894365310669\n",
            "epoch: 124 train loss: 3.7447524070739746 validation: auc: 0.5532840411189753 --- acc: 82.89999999999999 --- loss: 4.754371166229248\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.370147705078125\n",
            "epoch: 125 train loss: 3.615868330001831 validation: auc: 0.5524309066397034 --- acc: 83.1 --- loss: 4.695262908935547\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3642417907714844\n",
            "epoch: 126 train loss: 3.7354509830474854 validation: auc: 0.552424762667116 --- acc: 82.95 --- loss: 4.7046308517456055\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36411609649658205\n",
            "epoch: 127 train loss: 3.6947927474975586 validation: auc: 0.5544355971239187 --- acc: 83.05 --- loss: 4.7329230308532715\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36604163646697996\n",
            "epoch: 128 train loss: 3.6611487865448 validation: auc: 0.5560821817773285 --- acc: 83.1 --- loss: 4.736916542053223\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3660648107528687\n",
            "epoch: 129 train loss: 3.6623380184173584 validation: auc: 0.5555555555555556 --- acc: 82.95 --- loss: 4.713634490966797\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36677801609039307\n",
            "epoch: 130 train loss: 3.6500208377838135 validation: auc: 0.5567079892708684 --- acc: 83.05 --- loss: 4.7142863273620605\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37224373817443845\n",
            "epoch: 131 train loss: 3.4603476524353027 validation: auc: 0.5597992851926751 --- acc: 82.89999999999999 --- loss: 4.743815898895264\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3673420429229736\n",
            "epoch: 132 train loss: 3.6276330947875977 validation: auc: 0.5611079513537806 --- acc: 82.8 --- loss: 4.776453495025635\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37224340438842773\n",
            "epoch: 133 train loss: 3.5121755599975586 validation: auc: 0.5594253805752162 --- acc: 82.69999999999999 --- loss: 4.793126106262207\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3658994197845459\n",
            "epoch: 134 train loss: 3.8029065132141113 validation: auc: 0.5590839512414336 --- acc: 82.8 --- loss: 4.798983573913574\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3649690866470337\n",
            "epoch: 135 train loss: 3.914469003677368 validation: auc: 0.5579280066846423 --- acc: 82.8 --- loss: 4.788702011108398\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36750495433807373\n",
            "epoch: 136 train loss: 3.806556463241577 validation: auc: 0.5575312815975734 --- acc: 82.69999999999999 --- loss: 4.834444522857666\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3706932306289673\n",
            "epoch: 137 train loss: 3.7533833980560303 validation: auc: 0.5600231013369285 --- acc: 82.6 --- loss: 4.921028137207031\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3736737728118896\n",
            "epoch: 138 train loss: 3.7157135009765625 validation: auc: 0.5586600171329064 --- acc: 82.5 --- loss: 4.894984245300293\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36161980628967283\n",
            "epoch: 139 train loss: 4.066006660461426 validation: auc: 0.5595649365239861 --- acc: 82.55 --- loss: 4.871810436248779\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3638479232788086\n",
            "epoch: 140 train loss: 3.86834716796875 validation: auc: 0.5551658521514438 --- acc: 82.65 --- loss: 4.773677825927734\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36916394233703614\n",
            "epoch: 141 train loss: 3.6654052734375 validation: auc: 0.556660592910909 --- acc: 82.8 --- loss: 4.775649547576904\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3619016408920288\n",
            "epoch: 142 train loss: 3.9564120769500732 validation: auc: 0.5569116180766206 --- acc: 82.8 --- loss: 4.773208141326904\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3715091943740845\n",
            "epoch: 143 train loss: 3.564164161682129 validation: auc: 0.5566518158072127 --- acc: 82.75 --- loss: 4.782661437988281\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.366916561126709\n",
            "epoch: 144 train loss: 3.7360825538635254 validation: auc: 0.5533419700033704 --- acc: 82.69999999999999 --- loss: 4.836870193481445\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3663148880004883\n",
            "epoch: 145 train loss: 3.613401412963867 validation: auc: 0.5489112880575217 --- acc: 82.6 --- loss: 4.872364521026611\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3660641431808472\n",
            "epoch: 146 train loss: 3.4965906143188477 validation: auc: 0.5525924053477137 --- acc: 82.6 --- loss: 4.888535499572754\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3669631004333496\n",
            "epoch: 147 train loss: 3.4390387535095215 validation: auc: 0.5583993371531288 --- acc: 82.69999999999999 --- loss: 4.904865264892578\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36225578784942625\n",
            "epoch: 148 train loss: 3.6132466793060303 validation: auc: 0.5569897342995168 --- acc: 82.95 --- loss: 4.729585647583008\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36244246959686277\n",
            "epoch: 149 train loss: 3.786539316177368 validation: auc: 0.5563990352207617 --- acc: 83.0 --- loss: 4.76241397857666\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36539466381073\n",
            "epoch: 150 train loss: 3.6901493072509766 validation: auc: 0.5605786920008988 --- acc: 82.95 --- loss: 4.827204704284668\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3664005041122437\n",
            "epoch: 151 train loss: 3.811753511428833 validation: auc: 0.5561497654757892 --- acc: 82.39999999999999 --- loss: 4.859139442443848\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.38102715015411376\n",
            "epoch: 152 train loss: 3.4934961795806885 validation: auc: 0.5580754620267386 --- acc: 82.45 --- loss: 4.890017509460449\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3692011833190918\n",
            "epoch: 153 train loss: 3.6649627685546875 validation: auc: 0.5521965579710145 --- acc: 82.6 --- loss: 4.832911968231201\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.37168407440185547\n",
            "epoch: 154 train loss: 3.562993049621582 validation: auc: 0.5550543829345018 --- acc: 82.45 --- loss: 4.8197550773620605\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36906116008758544\n",
            "epoch: 155 train loss: 3.488962411880493 validation: auc: 0.5510634338838334 --- acc: 82.65 --- loss: 4.77661657333374\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3683571100234985\n",
            "epoch: 156 train loss: 3.4024691581726074 validation: auc: 0.5524045753286148 --- acc: 82.75 --- loss: 4.770374774932861\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36699390411376953\n",
            "epoch: 157 train loss: 3.4075167179107666 validation: auc: 0.5525169222559263 --- acc: 82.6 --- loss: 4.824986457824707\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36357746124267576\n",
            "epoch: 158 train loss: 3.5490148067474365 validation: auc: 0.5526573559150657 --- acc: 82.69999999999999 --- loss: 4.803311824798584\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3693474054336548\n",
            "epoch: 159 train loss: 3.373304843902588 validation: auc: 0.5536754999438266 --- acc: 82.8 --- loss: 4.830057621002197\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3661611318588257\n",
            "epoch: 160 train loss: 3.3963263034820557 validation: auc: 0.5544101435231997 --- acc: 82.69999999999999 --- loss: 4.797204971313477\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.360962438583374\n",
            "epoch: 161 train loss: 3.603119134902954 validation: auc: 0.5551798955173576 --- acc: 82.75 --- loss: 4.8285136222839355\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35863025188446046\n",
            "epoch: 162 train loss: 3.6595075130462646 validation: auc: 0.5550148859678689 --- acc: 82.65 --- loss: 4.894484996795654\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35806047916412354\n",
            "epoch: 163 train loss: 3.5550734996795654 validation: auc: 0.5512802283451297 --- acc: 82.75 --- loss: 4.847292900085449\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3538517475128174\n",
            "epoch: 164 train loss: 3.672795295715332 validation: auc: 0.5525423758566453 --- acc: 82.75 --- loss: 4.80432653427124\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35933513641357423\n",
            "epoch: 165 train loss: 3.412473201751709 validation: auc: 0.5527433715312886 --- acc: 82.85 --- loss: 4.793770790100098\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.36096997261047364\n",
            "epoch: 166 train loss: 3.289818525314331 validation: auc: 0.5504674685428603 --- acc: 82.95 --- loss: 4.737644195556641\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35566823482513427\n",
            "epoch: 167 train loss: 3.4004454612731934 validation: auc: 0.5496239888776542 --- acc: 82.8 --- loss: 4.755838394165039\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35218448638916017\n",
            "epoch: 168 train loss: 3.497889995574951 validation: auc: 0.5499162664307381 --- acc: 82.8 --- loss: 4.789027690887451\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3506124019622803\n",
            "epoch: 169 train loss: 3.7317075729370117 validation: auc: 0.559987115211774 --- acc: 82.5 --- loss: 4.9688191413879395\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3634136438369751\n",
            "epoch: 170 train loss: 3.396616220474243 validation: auc: 0.5597027370520167 --- acc: 82.69999999999999 --- loss: 4.886034965515137\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3580846548080444\n",
            "epoch: 171 train loss: 3.604719638824463 validation: auc: 0.5570441523424334 --- acc: 82.8 --- loss: 4.7716898918151855\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3492666959762573\n",
            "epoch: 172 train loss: 3.7853317260742188 validation: auc: 0.5610403676553196 --- acc: 82.89999999999999 --- loss: 4.824723243713379\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35146236419677734\n",
            "epoch: 173 train loss: 3.5888671875 validation: auc: 0.5650260504437704 --- acc: 82.75 --- loss: 4.877583980560303\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3601935386657715\n",
            "epoch: 174 train loss: 3.269169330596924 validation: auc: 0.5649514450623526 --- acc: 82.45 --- loss: 4.968461036682129\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34989428520202637\n",
            "epoch: 175 train loss: 3.7486183643341064 validation: auc: 0.5598949556229638 --- acc: 82.39999999999999 --- loss: 4.8098015785217285\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3581425189971924\n",
            "epoch: 176 train loss: 3.3164803981781006 validation: auc: 0.5588223935512864 --- acc: 82.65 --- loss: 4.9148993492126465\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35435245037078855\n",
            "epoch: 177 train loss: 3.3218164443969727 validation: auc: 0.5527995449949444 --- acc: 82.55 --- loss: 4.9100117683410645\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3556199073791504\n",
            "epoch: 178 train loss: 3.225344181060791 validation: auc: 0.5519174460734748 --- acc: 82.65 --- loss: 4.8356099128723145\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3404496431350708\n",
            "epoch: 179 train loss: 3.7969043254852295 validation: auc: 0.5568273578811369 --- acc: 82.75 --- loss: 4.884085655212402\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34889795780181887\n",
            "epoch: 180 train loss: 3.4833173751831055 validation: auc: 0.5559900221885182 --- acc: 82.75 --- loss: 4.835801124572754\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3500579833984375\n",
            "epoch: 181 train loss: 3.4007794857025146 validation: auc: 0.5537948685540951 --- acc: 82.69999999999999 --- loss: 4.855644702911377\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3489652156829834\n",
            "epoch: 182 train loss: 3.4211950302124023 validation: auc: 0.5531708164812943 --- acc: 82.75 --- loss: 4.872348308563232\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3456968069076538\n",
            "epoch: 183 train loss: 3.5873863697052 validation: auc: 0.5533138832715426 --- acc: 82.69999999999999 --- loss: 4.814624786376953\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34616429805755616\n",
            "epoch: 184 train loss: 3.5242066383361816 validation: auc: 0.5534727488484441 --- acc: 82.85 --- loss: 4.801469802856445\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35080010890960694\n",
            "epoch: 185 train loss: 3.2570908069610596 validation: auc: 0.5545374115267948 --- acc: 83.0 --- loss: 4.7809672355651855\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3435548782348633\n",
            "epoch: 186 train loss: 3.522188425064087 validation: auc: 0.5576927803055837 --- acc: 82.85 --- loss: 4.817431449890137\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3479180097579956\n",
            "epoch: 187 train loss: 3.3916330337524414 validation: auc: 0.5564016683518707 --- acc: 82.89999999999999 --- loss: 4.801285266876221\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34533307552337644\n",
            "epoch: 188 train loss: 3.4494495391845703 validation: auc: 0.5561655642624425 --- acc: 82.85 --- loss: 4.781216144561768\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34915728569030763\n",
            "epoch: 189 train loss: 3.2713382244110107 validation: auc: 0.5547234861251545 --- acc: 82.85 --- loss: 4.7895355224609375\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3475193500518799\n",
            "epoch: 190 train loss: 3.3465991020202637 validation: auc: 0.5509984833164813 --- acc: 82.8 --- loss: 4.7223076820373535\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34385855197906495\n",
            "epoch: 191 train loss: 3.456357479095459 validation: auc: 0.5510651893045725 --- acc: 82.89999999999999 --- loss: 4.710213661193848\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34413025379180906\n",
            "epoch: 192 train loss: 3.4251043796539307 validation: auc: 0.5521500393214246 --- acc: 82.89999999999999 --- loss: 4.7396697998046875\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3387650966644287\n",
            "epoch: 193 train loss: 3.6179370880126953 validation: auc: 0.553931791371756 --- acc: 82.75 --- loss: 4.8178253173828125\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3429172277450562\n",
            "epoch: 194 train loss: 3.47050404548645 validation: auc: 0.5546067506459949 --- acc: 82.69999999999999 --- loss: 4.836652755737305\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3516408443450928\n",
            "epoch: 195 train loss: 3.1253576278686523 validation: auc: 0.5557047663183913 --- acc: 82.6 --- loss: 4.856593608856201\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.35135626792907715\n",
            "epoch: 196 train loss: 3.1658031940460205 validation: auc: 0.5573899702280642 --- acc: 82.65 --- loss: 4.862436294555664\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34449498653411864\n",
            "epoch: 197 train loss: 3.4418604373931885 validation: auc: 0.5612931482417706 --- acc: 82.6 --- loss: 4.877712726593018\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3438620090484619\n",
            "epoch: 198 train loss: 3.503141164779663 validation: auc: 0.5656299151780699 --- acc: 82.5 --- loss: 4.93472957611084\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3404353618621826\n",
            "epoch: 199 train loss: 3.6654040813446045 validation: auc: 0.5629081353218741 --- acc: 82.55 --- loss: 4.882310390472412\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.346894645690918\n",
            "epoch: 200 train loss: 3.4052915573120117 validation: auc: 0.55823432760364 --- acc: 82.85 --- loss: 4.787510395050049\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3426706314086914\n",
            "epoch: 201 train loss: 3.540864944458008 validation: auc: 0.5568958192899675 --- acc: 83.0 --- loss: 4.783285617828369\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.349005913734436\n",
            "epoch: 202 train loss: 3.2784972190856934 validation: auc: 0.5582650474665768 --- acc: 83.0 --- loss: 4.83137321472168\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3434879779815674\n",
            "epoch: 203 train loss: 3.4817819595336914 validation: auc: 0.5574127906976745 --- acc: 83.05 --- loss: 4.8497772216796875\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3423788070678711\n",
            "epoch: 204 train loss: 3.516690254211426 validation: auc: 0.5536588234468037 --- acc: 83.05 --- loss: 4.798770427703857\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3461275577545166\n",
            "epoch: 205 train loss: 3.429597854614258 validation: auc: 0.5572907889562971 --- acc: 83.1 --- loss: 4.833764553070068\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34605553150177004\n",
            "epoch: 206 train loss: 3.324357271194458 validation: auc: 0.5592454499494439 --- acc: 83.05 --- loss: 4.820032119750977\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34331748485565183\n",
            "epoch: 207 train loss: 3.466137647628784 validation: auc: 0.558371250421301 --- acc: 82.95 --- loss: 4.7882609367370605\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34869134426116943\n",
            "epoch: 208 train loss: 3.202984571456909 validation: auc: 0.5553879128749579 --- acc: 82.95 --- loss: 4.784987449645996\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3377323389053345\n",
            "epoch: 209 train loss: 3.608994960784912 validation: auc: 0.5559382372767105 --- acc: 82.89999999999999 --- loss: 4.7964091300964355\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34503912925720215\n",
            "epoch: 210 train loss: 3.3583076000213623 validation: auc: 0.5572512919896642 --- acc: 82.89999999999999 --- loss: 4.819076061248779\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.34451744556427\n",
            "epoch: 211 train loss: 3.39803409576416 validation: auc: 0.5536061608246264 --- acc: 82.55 --- loss: 4.851619720458984\n",
            "<Timer(Thread-6519, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3458909273147583\n",
            "epoch: 212 train loss: 3.5496878623962402 validation: auc: 0.5537035866756544 --- acc: 82.6 --- loss: 4.83506965637207\n",
            "time up, break\n",
            "test auc: 0.5446245457331758 test acc: 81.8 test loss 5.0931267738342285\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.48428964614868164\n",
            "epoch: 0 train loss: 2.8949689865112305 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45871810913085936\n",
            "epoch: 1 train loss: 4.6281962394714355 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4591498851776123\n",
            "epoch: 2 train loss: 4.610926628112793 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4561276912689209\n",
            "epoch: 3 train loss: 4.731812477111816 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4634672164916992\n",
            "epoch: 4 train loss: 4.438232898712158 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45828638076782224\n",
            "epoch: 5 train loss: 4.645465850830078 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4507310390472412\n",
            "epoch: 6 train loss: 4.9476799964904785 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.463683032989502\n",
            "epoch: 7 train loss: 4.429597854614258 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45828638076782224\n",
            "epoch: 8 train loss: 4.645465850830078 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45396900177001953\n",
            "epoch: 9 train loss: 4.818159580230713 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46174030303955077\n",
            "epoch: 10 train loss: 4.507310390472412 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.459365701675415\n",
            "epoch: 11 train loss: 4.602292060852051 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4621719837188721\n",
            "epoch: 12 train loss: 4.4900407791137695 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4535373210906982\n",
            "epoch: 13 train loss: 4.835428237915039 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4600132942199707\n",
            "epoch: 14 train loss: 4.576387882232666 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4591498374938965\n",
            "epoch: 15 train loss: 4.610926628112793 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45591182708740235\n",
            "epoch: 16 train loss: 4.740447521209717 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4533214092254639\n",
            "epoch: 17 train loss: 4.8440632820129395 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45569596290588377\n",
            "epoch: 18 train loss: 4.749081611633301 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46152434349060056\n",
            "epoch: 19 train loss: 4.515944957733154 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46174030303955077\n",
            "epoch: 20 train loss: 4.507310390472412 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4626037120819092\n",
            "epoch: 21 train loss: 4.472771644592285 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46303544044494627\n",
            "epoch: 22 train loss: 4.455502033233643 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46152434349060056\n",
            "epoch: 23 train loss: 4.515944957733154 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46281957626342773\n",
            "epoch: 24 train loss: 4.464136600494385 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46713695526123045\n",
            "epoch: 25 train loss: 4.29144287109375 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4587181568145752\n",
            "epoch: 26 train loss: 4.6281962394714355 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46195616722106936\n",
            "epoch: 27 train loss: 4.49867582321167 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45418496131896974\n",
            "epoch: 28 train loss: 4.8095245361328125 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4608767986297607\n",
            "epoch: 29 train loss: 4.541849136352539 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4610926628112793\n",
            "epoch: 30 train loss: 4.533214569091797 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4600132942199707\n",
            "epoch: 31 train loss: 4.576387405395508 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4600132942199707\n",
            "epoch: 32 train loss: 4.576387882232666 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46044507026672366\n",
            "epoch: 33 train loss: 4.559118270874023 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45008344650268556\n",
            "epoch: 34 train loss: 4.973584175109863 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4636830806732178\n",
            "epoch: 35 train loss: 4.429598331451416 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46433067321777344\n",
            "epoch: 36 train loss: 4.403693675994873 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4606609344482422\n",
            "epoch: 37 train loss: 4.550483703613281 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46001338958740234\n",
            "epoch: 38 train loss: 4.576387405395508 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4565594673156738\n",
            "epoch: 39 train loss: 4.714542865753174 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4580705165863037\n",
            "epoch: 40 train loss: 4.654099941253662 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4548325061798096\n",
            "epoch: 41 train loss: 4.783620834350586 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4552642345428467\n",
            "epoch: 42 train loss: 4.766351222991943 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45742292404174806\n",
            "epoch: 43 train loss: 4.680004119873047 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.462603759765625\n",
            "epoch: 44 train loss: 4.472771167755127 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.465410041809082\n",
            "epoch: 45 train loss: 4.360520839691162 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4600132942199707\n",
            "epoch: 46 train loss: 4.576387882232666 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46174025535583496\n",
            "epoch: 47 train loss: 4.507310390472412 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4610926628112793\n",
            "epoch: 48 train loss: 4.533214569091797 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4667051792144775\n",
            "epoch: 49 train loss: 4.308712482452393 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4649782657623291\n",
            "epoch: 50 train loss: 4.377790451049805 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46152439117431643\n",
            "epoch: 51 train loss: 4.515944957733154 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4490041255950928\n",
            "epoch: 52 train loss: 5.016757011413574 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4619561195373535\n",
            "epoch: 53 train loss: 4.498675346374512 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45699119567871094\n",
            "epoch: 54 train loss: 4.6972737312316895 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4608768463134766\n",
            "epoch: 55 train loss: 4.541849136352539 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45850224494934083\n",
            "epoch: 56 train loss: 4.636830806732178 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4636831283569336\n",
            "epoch: 57 train loss: 4.429598331451416 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4623878955841064\n",
            "epoch: 58 train loss: 4.481406211853027 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4623878955841064\n",
            "epoch: 59 train loss: 4.481406211853027 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45440077781677246\n",
            "epoch: 60 train loss: 4.80088996887207 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45850224494934083\n",
            "epoch: 61 train loss: 4.636830806732178 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46174030303955077\n",
            "epoch: 62 train loss: 4.507310390472412 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45720701217651366\n",
            "epoch: 63 train loss: 4.688639163970947 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4638989448547363\n",
            "epoch: 64 train loss: 4.420963287353516 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46433067321777344\n",
            "epoch: 65 train loss: 4.4036946296691895 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45548014640808104\n",
            "epoch: 66 train loss: 4.757716178894043 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45440077781677246\n",
            "epoch: 67 train loss: 4.80088996887207 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45742292404174806\n",
            "epoch: 68 train loss: 4.680004119873047 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4621719837188721\n",
            "epoch: 69 train loss: 4.4900407791137695 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4576387405395508\n",
            "epoch: 70 train loss: 4.671369552612305 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46433067321777344\n",
            "epoch: 71 train loss: 4.403693675994873 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46454653739929197\n",
            "epoch: 72 train loss: 4.395059108734131 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4634672164916992\n",
            "epoch: 73 train loss: 4.438232898712158 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46281962394714354\n",
            "epoch: 74 train loss: 4.464137077331543 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45699119567871094\n",
            "epoch: 75 train loss: 4.697273254394531 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4619561195373535\n",
            "epoch: 76 train loss: 4.498675346374512 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45871810913085936\n",
            "epoch: 77 train loss: 4.6281962394714355 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4634672164916992\n",
            "epoch: 78 train loss: 4.438232898712158 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45742292404174806\n",
            "epoch: 79 train loss: 4.680004596710205 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.449004077911377\n",
            "epoch: 80 train loss: 5.016757488250732 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46303548812866213\n",
            "epoch: 81 train loss: 4.455502510070801 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46281962394714354\n",
            "epoch: 82 train loss: 4.464136600494385 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45288972854614257\n",
            "epoch: 83 train loss: 4.861332893371582 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45742292404174806\n",
            "epoch: 84 train loss: 4.680004119873047 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45440077781677246\n",
            "epoch: 85 train loss: 4.80088996887207 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45591182708740235\n",
            "epoch: 86 train loss: 4.7404465675354 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4546165943145752\n",
            "epoch: 87 train loss: 4.792255401611328 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45591182708740235\n",
            "epoch: 88 train loss: 4.740447521209717 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46195616722106936\n",
            "epoch: 89 train loss: 4.498675346374512 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46195616722106936\n",
            "epoch: 90 train loss: 4.498675346374512 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4660576343536377\n",
            "epoch: 91 train loss: 4.334616184234619 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46713695526123045\n",
            "epoch: 92 train loss: 4.291443347930908 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45548014640808104\n",
            "epoch: 93 train loss: 4.757716655731201 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4608767509460449\n",
            "epoch: 94 train loss: 4.541849136352539 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46627345085144045\n",
            "epoch: 95 train loss: 4.325982093811035 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.465410041809082\n",
            "epoch: 96 train loss: 4.360520839691162 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46281957626342773\n",
            "epoch: 97 train loss: 4.464137077331543 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45288972854614257\n",
            "epoch: 98 train loss: 4.861332893371582 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45893397331237795\n",
            "epoch: 99 train loss: 4.619561672210693 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46713695526123045\n",
            "epoch: 100 train loss: 4.291443347930908 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4561277389526367\n",
            "epoch: 101 train loss: 4.731812000274658 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46195616722106936\n",
            "epoch: 102 train loss: 4.49867582321167 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45569596290588377\n",
            "epoch: 103 train loss: 4.749082088470459 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45828638076782224\n",
            "epoch: 104 train loss: 4.645465850830078 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4561277389526367\n",
            "epoch: 105 train loss: 4.731812477111816 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4494358539581299\n",
            "epoch: 106 train loss: 4.999488353729248 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4651941299438477\n",
            "epoch: 107 train loss: 4.369155406951904 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4580705642700195\n",
            "epoch: 108 train loss: 4.654099941253662 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4578546047210693\n",
            "epoch: 109 train loss: 4.6627349853515625 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4563436508178711\n",
            "epoch: 110 train loss: 4.723177433013916 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4580705165863037\n",
            "epoch: 111 train loss: 4.65410041809082 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4608767986297607\n",
            "epoch: 112 train loss: 4.541849136352539 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46562585830688474\n",
            "epoch: 113 train loss: 4.351885795593262 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45137863159179686\n",
            "epoch: 114 train loss: 4.9217753410339355 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45159454345703126\n",
            "epoch: 115 train loss: 4.913141250610352 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45979747772216795\n",
            "epoch: 116 train loss: 4.585022449493408 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4595816135406494\n",
            "epoch: 117 train loss: 4.59365701675415 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46281957626342773\n",
            "epoch: 118 train loss: 4.464137077331543 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45979747772216795\n",
            "epoch: 119 train loss: 4.585022449493408 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4619561195373535\n",
            "epoch: 120 train loss: 4.498675346374512 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45094690322875974\n",
            "epoch: 121 train loss: 4.939044952392578 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4623878955841064\n",
            "epoch: 122 train loss: 4.481406211853027 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.44814062118530273\n",
            "epoch: 123 train loss: 5.051296234130859 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4623878479003906\n",
            "epoch: 124 train loss: 4.481406211853027 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46130852699279784\n",
            "epoch: 125 train loss: 4.524580001831055 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4591498851776123\n",
            "epoch: 126 train loss: 4.610926151275635 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46152439117431643\n",
            "epoch: 127 train loss: 4.515944957733154 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4565594673156738\n",
            "epoch: 128 train loss: 4.714542865753174 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4595816135406494\n",
            "epoch: 129 train loss: 4.59365701675415 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46325135231018066\n",
            "epoch: 130 train loss: 4.4468674659729 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.44835648536682127\n",
            "epoch: 131 train loss: 5.042661190032959 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4680004596710205\n",
            "epoch: 132 train loss: 4.256904125213623 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45742292404174806\n",
            "epoch: 133 train loss: 4.680003643035889 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4619561195373535\n",
            "epoch: 134 train loss: 4.49867582321167 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46001334190368653\n",
            "epoch: 135 train loss: 4.576387882232666 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4502993106842041\n",
            "epoch: 136 train loss: 4.964949131011963 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4641148090362549\n",
            "epoch: 137 train loss: 4.412328720092773 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4563436031341553\n",
            "epoch: 138 train loss: 4.723177909851074 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45116276741027833\n",
            "epoch: 139 train loss: 4.930410385131836 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45483245849609377\n",
            "epoch: 140 train loss: 4.783620357513428 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45720701217651366\n",
            "epoch: 141 train loss: 4.688638687133789 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.459365701675415\n",
            "epoch: 142 train loss: 4.602292060852051 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4520262241363525\n",
            "epoch: 143 train loss: 4.895871162414551 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4567753314971924\n",
            "epoch: 144 train loss: 4.705908298492432 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45720705986022947\n",
            "epoch: 145 train loss: 4.688638687133789 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4608767986297607\n",
            "epoch: 146 train loss: 4.541849136352539 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4505151271820068\n",
            "epoch: 147 train loss: 4.9563140869140625 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4619561195373535\n",
            "epoch: 148 train loss: 4.49867582321167 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46130852699279784\n",
            "epoch: 149 train loss: 4.524580001831055 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.459365701675415\n",
            "epoch: 150 train loss: 4.602292060852051 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778213500977\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4580705165863037\n",
            "epoch: 151 train loss: 4.654099941253662 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4578546524047852\n",
            "epoch: 152 train loss: 4.6627349853515625 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46130852699279784\n",
            "epoch: 153 train loss: 4.524580001831055 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4552642345428467\n",
            "epoch: 154 train loss: 4.766351222991943 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4591498374938965\n",
            "epoch: 155 train loss: 4.610926628112793 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4660576343536377\n",
            "epoch: 156 train loss: 4.334616661071777 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4610927104949951\n",
            "epoch: 157 train loss: 4.533214092254639 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45979747772216795\n",
            "epoch: 158 train loss: 4.585022449493408 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4578546524047852\n",
            "epoch: 159 train loss: 4.6627349853515625 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46130852699279784\n",
            "epoch: 160 train loss: 4.524580001831055 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45440077781677246\n",
            "epoch: 161 train loss: 4.80088996887207 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4580705165863037\n",
            "epoch: 162 train loss: 4.65410041809082 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45893397331237795\n",
            "epoch: 163 train loss: 4.619561672210693 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4565594673156738\n",
            "epoch: 164 train loss: 4.714542865753174 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4591498851776123\n",
            "epoch: 165 train loss: 4.610927104949951 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4600132942199707\n",
            "epoch: 166 train loss: 4.576387405395508 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46044507026672366\n",
            "epoch: 167 train loss: 4.559118270874023 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46044507026672366\n",
            "epoch: 168 train loss: 4.559118747711182 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4651941299438477\n",
            "epoch: 169 train loss: 4.369155406951904 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4533214569091797\n",
            "epoch: 170 train loss: 4.8440632820129395 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45850224494934083\n",
            "epoch: 171 train loss: 4.636830806732178 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46562585830688474\n",
            "epoch: 172 train loss: 4.351885795593262 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4552642345428467\n",
            "epoch: 173 train loss: 4.766351222991943 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45850224494934083\n",
            "epoch: 174 train loss: 4.636831283569336 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46433067321777344\n",
            "epoch: 175 train loss: 4.403694152832031 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45396904945373534\n",
            "epoch: 176 train loss: 4.818159580230713 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434779167175293\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.45569596290588377\n",
            "epoch: 177 train loss: 4.749082088470459 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4621719837188721\n",
            "epoch: 178 train loss: 4.4900407791137695 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4561277389526367\n",
            "epoch: 179 train loss: 4.731812477111816 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4610926628112793\n",
            "epoch: 180 train loss: 4.533214569091797 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4606608867645264\n",
            "epoch: 181 train loss: 4.550483703613281 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4623878955841064\n",
            "epoch: 182 train loss: 4.481406211853027 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.46001338958740234\n",
            "epoch: 183 train loss: 4.576387405395508 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "<Timer(Thread-6947, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.4548325061798096\n",
            "epoch: 184 train loss: 4.783620834350586 validation: auc: 0.5 --- acc: 83.95 --- loss: 4.434778690338135\n",
            "time up, break\n",
            "test auc: 0.5 test acc: 82.89999999999999 test loss 4.724905014038086\n",
            "0.001\n",
            "0.6478292467138524\n",
            "79.60000000000001\n",
            "0.5730694532394409\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachineModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FeaturesLinear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachine. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，无法输出最后 5000 行内容。\u001b[0m\n",
            "epoch: 143 train loss: 0.003494871314615011 validation: auc: 0.6872335021749084 --- acc: 81.8 --- loss: 0.8037245273590088\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003232558025047183\n",
            "epoch: 144 train loss: 0.004346265457570553 validation: auc: 0.687176267742674 --- acc: 81.85 --- loss: 0.8054715394973755\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003469451563432813\n",
            "epoch: 145 train loss: 0.0031166395638138056 validation: auc: 0.6870331816620879 --- acc: 81.8 --- loss: 0.80647212266922\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00036766803823411464\n",
            "epoch: 146 train loss: 0.0018407483585178852 validation: auc: 0.6870331816620879 --- acc: 81.8 --- loss: 0.8082684278488159\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031595409382134676\n",
            "epoch: 147 train loss: 0.0036805691197514534 validation: auc: 0.6871100904304028 --- acc: 81.8 --- loss: 0.8108076453208923\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032351273111999037\n",
            "epoch: 148 train loss: 0.003517397679388523 validation: auc: 0.686945541437729 --- acc: 81.85 --- loss: 0.8131119012832642\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031526994425803425\n",
            "epoch: 149 train loss: 0.0037602437660098076 validation: auc: 0.68701171875 --- acc: 81.85 --- loss: 0.8147725462913513\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035645656753331424\n",
            "epoch: 150 train loss: 0.002173854038119316 validation: auc: 0.6870367588141026 --- acc: 81.89999999999999 --- loss: 0.8156248927116394\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003615648252889514\n",
            "epoch: 151 train loss: 0.0025189362931996584 validation: auc: 0.6871637477106227 --- acc: 81.89999999999999 --- loss: 0.815874457359314\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00029972963966429234\n",
            "epoch: 152 train loss: 0.005489787086844444 validation: auc: 0.6873926854395604 --- acc: 81.89999999999999 --- loss: 0.8156493306159973\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003798946039751172\n",
            "epoch: 153 train loss: 0.0017752381972968578 validation: auc: 0.6880580357142856 --- acc: 81.89999999999999 --- loss: 0.8146954774856567\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033148806542158125\n",
            "epoch: 154 train loss: 0.0031318189576268196 validation: auc: 0.6887090773809522 --- acc: 82.0 --- loss: 0.8134158253669739\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003421306610107422\n",
            "epoch: 155 train loss: 0.003004422876983881 validation: auc: 0.689261747367216 --- acc: 82.0 --- loss: 0.8124697804450989\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0002982591046020389\n",
            "epoch: 156 train loss: 0.005007643718272448 validation: auc: 0.6894119877518315 --- acc: 82.05 --- loss: 0.8128254413604736\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003501018276438117\n",
            "epoch: 157 train loss: 0.002702914411202073 validation: auc: 0.6894567021520147 --- acc: 82.05 --- loss: 0.8136809468269348\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003506618086248636\n",
            "epoch: 158 train loss: 0.0022851917892694473 validation: auc: 0.6892599587912088 --- acc: 82.0 --- loss: 0.8148811459541321\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000333153665997088\n",
            "epoch: 159 train loss: 0.002915110904723406 validation: auc: 0.6892206101190477 --- acc: 82.0 --- loss: 0.8151716589927673\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003240366000682116\n",
            "epoch: 160 train loss: 0.003950404468923807 validation: auc: 0.6892635359432234 --- acc: 82.0 --- loss: 0.8139699697494507\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003282916033640504\n",
            "epoch: 161 train loss: 0.003472845768555999 validation: auc: 0.6895103594322345 --- acc: 82.05 --- loss: 0.8124024868011475\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033279519993811846\n",
            "epoch: 162 train loss: 0.0031853734981268644 validation: auc: 0.689562228136447 --- acc: 82.05 --- loss: 0.8119107484817505\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003474681172519922\n",
            "epoch: 163 train loss: 0.002700026147067547 validation: auc: 0.689678485576923 --- acc: 82.05 --- loss: 0.8143380880355835\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003176536876708269\n",
            "epoch: 164 train loss: 0.004080890212208033 validation: auc: 0.6897947430173994 --- acc: 82.15 --- loss: 0.819222629070282\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003337286412715912\n",
            "epoch: 165 train loss: 0.0032624087762087584 validation: auc: 0.6900397779304028 --- acc: 82.15 --- loss: 0.824665904045105\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003426900133490562\n",
            "epoch: 166 train loss: 0.0031359035056084394 validation: auc: 0.6900022178342491 --- acc: 82.15 --- loss: 0.8270623683929443\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003524539992213249\n",
            "epoch: 167 train loss: 0.0031408355571329594 validation: auc: 0.6898591317536631 --- acc: 82.15 --- loss: 0.8251829743385315\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00035877889022231104\n",
            "epoch: 168 train loss: 0.003281280165538192 validation: auc: 0.6898627089056776 --- acc: 82.25 --- loss: 0.8217896819114685\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034465331118553876\n",
            "epoch: 169 train loss: 0.0032400719355791807 validation: auc: 0.6901488810668499 --- acc: 82.19999999999999 --- loss: 0.820317804813385\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003322885138913989\n",
            "epoch: 170 train loss: 0.002914914395660162 validation: auc: 0.69027586996337 --- acc: 82.15 --- loss: 0.8196290135383606\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000318519095890224\n",
            "epoch: 171 train loss: 0.0030265559908002615 validation: auc: 0.6904386303800366 --- acc: 82.05 --- loss: 0.8179180026054382\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00030655181035399437\n",
            "epoch: 172 train loss: 0.0034675158094614744 validation: auc: 0.6904636704441391 --- acc: 82.0 --- loss: 0.8161869049072266\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00032212131191045046\n",
            "epoch: 173 train loss: 0.0035176652017980814 validation: auc: 0.6905352134844323 --- acc: 82.05 --- loss: 0.8158979415893555\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033269056584686043\n",
            "epoch: 174 train loss: 0.0035145548172295094 validation: auc: 0.6906353737408425 --- acc: 82.0 --- loss: 0.8176937103271484\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034644743427634237\n",
            "epoch: 175 train loss: 0.003007765393704176 validation: auc: 0.6907301682692308 --- acc: 82.1 --- loss: 0.8208519816398621\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0003420046530663967\n",
            "epoch: 176 train loss: 0.003178904764354229 validation: auc: 0.6909805689102564 --- acc: 82.1 --- loss: 0.8224706649780273\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00036004388239234685\n",
            "epoch: 177 train loss: 0.0023504632990807295 validation: auc: 0.6914080385760074 --- acc: 82.05 --- loss: 0.8231205344200134\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00033036440145224334\n",
            "epoch: 178 train loss: 0.003477713791653514 validation: auc: 0.6916387648809523 --- acc: 82.05 --- loss: 0.8239885568618774\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00036416118964552877\n",
            "epoch: 179 train loss: 0.0019045111257582903 validation: auc: 0.6918766454899268 --- acc: 82.15 --- loss: 0.822898268699646\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00034486588556319474\n",
            "epoch: 180 train loss: 0.002815723419189453 validation: auc: 0.6919356684981685 --- acc: 82.19999999999999 --- loss: 0.8215966820716858\n",
            "<Timer(Thread-7319, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00031135277822613716\n",
            "epoch: 181 train loss: 0.004320040345191956 validation: auc: 0.6917675423534798 --- acc: 82.19999999999999 --- loss: 0.8221129179000854\n",
            "time up, break\n",
            "test auc: 0.6943614236509759 test acc: 83.1 test loss 0.7486572265625\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.15463634729385375\n",
            "epoch: 0 train loss: 1.4257510900497437 validation: auc: 0.49456884560716197 --- acc: 57.05 --- loss: 1.2416163682937622\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.12326231002807617\n",
            "epoch: 1 train loss: 1.105708360671997 validation: auc: 0.5119025687915238 --- acc: 66.95 --- loss: 0.9529305696487427\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08957831859588623\n",
            "epoch: 2 train loss: 0.764779806137085 validation: auc: 0.5455103189398305 --- acc: 77.25 --- loss: 0.8452709913253784\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07195922136306762\n",
            "epoch: 3 train loss: 0.6857157349586487 validation: auc: 0.5873235318475052 --- acc: 80.7 --- loss: 0.8494787812232971\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06677906513214112\n",
            "epoch: 4 train loss: 0.6130619049072266 validation: auc: 0.6241999462546415 --- acc: 80.7 --- loss: 0.759093165397644\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05350613594055176\n",
            "epoch: 5 train loss: 0.46451133489608765 validation: auc: 0.6502140518761017 --- acc: 79.55 --- loss: 0.6445896029472351\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.037747761607170104\n",
            "epoch: 6 train loss: 0.31730756163597107 validation: auc: 0.6619518353701923 --- acc: 76.3 --- loss: 0.6071711182594299\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.028042510151863098\n",
            "epoch: 7 train loss: 0.27589577436447144 validation: auc: 0.6610898815066955 --- acc: 72.35000000000001 --- loss: 0.6229426264762878\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.023293112218379975\n",
            "epoch: 8 train loss: 0.20946994423866272 validation: auc: 0.6541959407043345 --- acc: 74.25 --- loss: 0.5942181348800659\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015694035589694975\n",
            "epoch: 9 train loss: 0.12749747931957245 validation: auc: 0.6484140893964464 --- acc: 78.14999999999999 --- loss: 0.5688542127609253\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009846271574497223\n",
            "epoch: 10 train loss: 0.0860312283039093 validation: auc: 0.645645696399568 --- acc: 79.60000000000001 --- loss: 0.5760625600814819\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007156869769096375\n",
            "epoch: 11 train loss: 0.06869789212942123 validation: auc: 0.6468997547656753 --- acc: 80.35 --- loss: 0.5941379070281982\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005869562923908234\n",
            "epoch: 12 train loss: 0.05240226164460182 validation: auc: 0.6515120529881914 --- acc: 80.7 --- loss: 0.6049065589904785\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0046824198216199875\n",
            "epoch: 13 train loss: 0.04923247918486595 validation: auc: 0.6564234999045091 --- acc: 80.55 --- loss: 0.6060476303100586\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004122185334563255\n",
            "epoch: 14 train loss: 0.032646745443344116 validation: auc: 0.6613586082994326 --- acc: 80.30000000000001 --- loss: 0.6012566685676575\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034938111901283266\n",
            "epoch: 15 train loss: 0.02988501824438572 validation: auc: 0.6663866725031649 --- acc: 80.4 --- loss: 0.5957025289535522\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003180937469005585\n",
            "epoch: 16 train loss: 0.025182493031024933 validation: auc: 0.6709533378740836 --- acc: 80.35 --- loss: 0.5914706587791443\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028742196038365362\n",
            "epoch: 17 train loss: 0.027302689850330353 validation: auc: 0.6748236797317464 --- acc: 80.4 --- loss: 0.5894110202789307\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027098724618554114\n",
            "epoch: 18 train loss: 0.026935825124382973 validation: auc: 0.678278255608193 --- acc: 80.45 --- loss: 0.5898735523223877\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026005564257502558\n",
            "epoch: 19 train loss: 0.024334482848644257 validation: auc: 0.6809046797334364 --- acc: 80.80000000000001 --- loss: 0.5935196876525879\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002418692409992218\n",
            "epoch: 20 train loss: 0.02530152164399624 validation: auc: 0.6821621183107732 --- acc: 81.05 --- loss: 0.6000955700874329\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002370802313089371\n",
            "epoch: 21 train loss: 0.02338656224310398 validation: auc: 0.6831170279830786 --- acc: 81.05 --- loss: 0.6082270741462708\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023722980171442033\n",
            "epoch: 22 train loss: 0.021095074713230133 validation: auc: 0.6842172867382483 --- acc: 81.10000000000001 --- loss: 0.6158503293991089\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002294390834867954\n",
            "epoch: 23 train loss: 0.02265419065952301 validation: auc: 0.6853428970776384 --- acc: 81.05 --- loss: 0.621674120426178\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022638728842139244\n",
            "epoch: 24 train loss: 0.022609209641814232 validation: auc: 0.686454986572111 --- acc: 81.10000000000001 --- loss: 0.6256056427955627\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021716440096497536\n",
            "epoch: 25 train loss: 0.024755923077464104 validation: auc: 0.6879642508860379 --- acc: 81.15 --- loss: 0.6277982592582703\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021996134892106056\n",
            "epoch: 26 train loss: 0.02231631986796856 validation: auc: 0.6895546402694704 --- acc: 80.95 --- loss: 0.6287181973457336\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022017758339643477\n",
            "epoch: 27 train loss: 0.02026505023241043 validation: auc: 0.6909726388802035 --- acc: 80.95 --- loss: 0.6295062303543091\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002141272462904453\n",
            "epoch: 28 train loss: 0.02039807289838791 validation: auc: 0.6918565641166916 --- acc: 80.95 --- loss: 0.6308638453483582\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002081090398132801\n",
            "epoch: 29 train loss: 0.020066022872924805 validation: auc: 0.6928148540002265 --- acc: 81.0 --- loss: 0.6329772472381592\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002034921571612358\n",
            "epoch: 30 train loss: 0.019199945032596588 validation: auc: 0.6937275110321645 --- acc: 81.05 --- loss: 0.6348497271537781\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001958976686000824\n",
            "epoch: 31 train loss: 0.019455937668681145 validation: auc: 0.6946807305988552 --- acc: 80.95 --- loss: 0.6359134912490845\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019357984885573388\n",
            "epoch: 32 train loss: 0.01721247285604477 validation: auc: 0.6954531088647729 --- acc: 81.05 --- loss: 0.6365929245948792\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018235059455037118\n",
            "epoch: 33 train loss: 0.01874675042927265 validation: auc: 0.6961578829061027 --- acc: 81.2 --- loss: 0.6368465423583984\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017767483368515968\n",
            "epoch: 34 train loss: 0.017952416092157364 validation: auc: 0.6966902661747332 --- acc: 81.15 --- loss: 0.6365982294082642\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017348460853099823\n",
            "epoch: 35 train loss: 0.016849540174007416 validation: auc: 0.6973460271532368 --- acc: 81.15 --- loss: 0.6359460949897766\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016518454998731614\n",
            "epoch: 36 train loss: 0.017831413075327873 validation: auc: 0.6978598192601055 --- acc: 81.15 --- loss: 0.6354730129241943\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016037525609135627\n",
            "epoch: 37 train loss: 0.01729566790163517 validation: auc: 0.6979713662306757 --- acc: 81.15 --- loss: 0.6357095837593079\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016099868342280387\n",
            "epoch: 38 train loss: 0.014876436442136765 validation: auc: 0.6975556002494596 --- acc: 81.10000000000001 --- loss: 0.6367947459220886\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0014845802448689937\n",
            "epoch: 39 train loss: 0.01822543516755104 validation: auc: 0.6974322225395865 --- acc: 81.10000000000001 --- loss: 0.6382364630699158\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0015454991720616818\n",
            "epoch: 40 train loss: 0.01394856907427311 validation: auc: 0.697283493245493 --- acc: 81.15 --- loss: 0.6398195624351501\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001460855919867754\n",
            "epoch: 41 train loss: 0.015773627907037735 validation: auc: 0.6972750427174194 --- acc: 81.15 --- loss: 0.6411226987838745\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0014630556106567383\n",
            "epoch: 42 train loss: 0.014776628464460373 validation: auc: 0.6973122250409428 --- acc: 81.05 --- loss: 0.6416563987731934\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0014722593128681182\n",
            "epoch: 43 train loss: 0.013123811222612858 validation: auc: 0.6969657533899294 --- acc: 81.10000000000001 --- loss: 0.6415823698043823\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013838479295372962\n",
            "epoch: 44 train loss: 0.015795808285474777 validation: auc: 0.6970468784594349 --- acc: 81.15 --- loss: 0.6407599449157715\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013801496475934981\n",
            "epoch: 45 train loss: 0.01454337127506733 validation: auc: 0.6973291260970899 --- acc: 81.15 --- loss: 0.6394113898277283\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013827234506607056\n",
            "epoch: 46 train loss: 0.013067337684333324 validation: auc: 0.697682358170562 --- acc: 81.15 --- loss: 0.6390228867530823\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013280939310789108\n",
            "epoch: 47 train loss: 0.01389375887811184 validation: auc: 0.6981724887988251 --- acc: 81.2 --- loss: 0.6399105787277222\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001308789849281311\n",
            "epoch: 48 train loss: 0.013509189710021019 validation: auc: 0.69883416514698 --- acc: 81.15 --- loss: 0.6413783431053162\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0012848872691392898\n",
            "epoch: 49 train loss: 0.013471297919750214 validation: auc: 0.6994671096996852 --- acc: 81.39999999999999 --- loss: 0.6425240635871887\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0013030023314058782\n",
            "epoch: 50 train loss: 0.011807008646428585 validation: auc: 0.700210756170153 --- acc: 81.39999999999999 --- loss: 0.6434518098831177\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001273735798895359\n",
            "epoch: 51 train loss: 0.012226659804582596 validation: auc: 0.7006721550029662 --- acc: 81.39999999999999 --- loss: 0.6440227031707764\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0011973735876381398\n",
            "epoch: 52 train loss: 0.014561448246240616 validation: auc: 0.7011774965817614 --- acc: 81.35 --- loss: 0.6441307663917542\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001212198007851839\n",
            "epoch: 53 train loss: 0.012924229726195335 validation: auc: 0.7012434107007346 --- acc: 81.3 --- loss: 0.6435341835021973\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0011870057322084903\n",
            "epoch: 54 train loss: 0.012906912714242935 validation: auc: 0.701123413202091 --- acc: 81.35 --- loss: 0.6425297260284424\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001198429986834526\n",
            "epoch: 55 train loss: 0.011768016032874584 validation: auc: 0.7009510224293916 --- acc: 81.35 --- loss: 0.6414729952812195\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0011690445244312286\n",
            "epoch: 56 train loss: 0.012351375073194504 validation: auc: 0.7008208842970597 --- acc: 81.39999999999999 --- loss: 0.640598475933075\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001166086085140705\n",
            "epoch: 57 train loss: 0.012128577567636967 validation: auc: 0.7005403267650195 --- acc: 81.45 --- loss: 0.6411711573600769\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001141313649713993\n",
            "epoch: 58 train loss: 0.012697450816631317 validation: auc: 0.7005893398278459 --- acc: 81.45 --- loss: 0.6427550315856934\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0011721608228981496\n",
            "epoch: 59 train loss: 0.010357585735619068 validation: auc: 0.7008648270430419 --- acc: 81.6 --- loss: 0.6446141600608826\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0011094570159912109\n",
            "epoch: 60 train loss: 0.01200538408011198 validation: auc: 0.7009915849641445 --- acc: 81.6 --- loss: 0.6467943787574768\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010985727421939373\n",
            "epoch: 61 train loss: 0.011683843098580837 validation: auc: 0.700891868732877 --- acc: 81.55 --- loss: 0.6486993432044983\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010522215627133847\n",
            "epoch: 62 train loss: 0.013185935094952583 validation: auc: 0.7010355277101266 --- acc: 81.6 --- loss: 0.6501373052597046\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010746508836746217\n",
            "epoch: 63 train loss: 0.011750435456633568 validation: auc: 0.7012772128130286 --- acc: 81.35 --- loss: 0.6511255502700806\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010443296283483504\n",
            "epoch: 64 train loss: 0.012057687155902386 validation: auc: 0.7012941138691756 --- acc: 81.45 --- loss: 0.6506536602973938\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010825514793395997\n",
            "epoch: 65 train loss: 0.010182341560721397 validation: auc: 0.7010135563371355 --- acc: 81.35 --- loss: 0.6497392058372498\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010806671343743801\n",
            "epoch: 66 train loss: 0.010178008116781712 validation: auc: 0.7004744126460463 --- acc: 81.3 --- loss: 0.649473249912262\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010357968509197234\n",
            "epoch: 67 train loss: 0.012008044868707657 validation: auc: 0.6996614718453756 --- acc: 81.35 --- loss: 0.6503809690475464\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010667698457837104\n",
            "epoch: 68 train loss: 0.010252795182168484 validation: auc: 0.6987217731236025 --- acc: 81.35 --- loss: 0.6524842977523804\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001051278319209814\n",
            "epoch: 69 train loss: 0.01044935267418623 validation: auc: 0.6981758690100546 --- acc: 81.39999999999999 --- loss: 0.6542004346847534\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010451079346239568\n",
            "epoch: 70 train loss: 0.009917992167174816 validation: auc: 0.6983524850467906 --- acc: 81.45 --- loss: 0.6555063724517822\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009967288933694363\n",
            "epoch: 71 train loss: 0.011156480759382248 validation: auc: 0.6991121875205982 --- acc: 81.35 --- loss: 0.6564037203788757\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010010682046413422\n",
            "epoch: 72 train loss: 0.010137729346752167 validation: auc: 0.6999809018065538 --- acc: 81.45 --- loss: 0.6565531492233276\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009971396066248417\n",
            "epoch: 73 train loss: 0.009856046177446842 validation: auc: 0.7007380691219395 --- acc: 81.39999999999999 --- loss: 0.6563037037849426\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009719958528876304\n",
            "epoch: 74 train loss: 0.011273614130914211 validation: auc: 0.70124172059512 --- acc: 81.55 --- loss: 0.6554805040359497\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0010071482509374618\n",
            "epoch: 75 train loss: 0.009685908444225788 validation: auc: 0.701478335381178 --- acc: 81.39999999999999 --- loss: 0.6542852520942688\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009755728766322136\n",
            "epoch: 76 train loss: 0.01051995251327753 validation: auc: 0.7014952364373249 --- acc: 81.39999999999999 --- loss: 0.6540647149085999\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009826408699154855\n",
            "epoch: 77 train loss: 0.00979628600180149 validation: auc: 0.7013211555590109 --- acc: 81.55 --- loss: 0.6554409861564636\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009852956049144269\n",
            "epoch: 78 train loss: 0.008985552005469799 validation: auc: 0.7009729938023828 --- acc: 81.6 --- loss: 0.6573712825775146\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009493326768279075\n",
            "epoch: 79 train loss: 0.010157751850783825 validation: auc: 0.7003763865203936 --- acc: 81.55 --- loss: 0.6592492461204529\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009608731605112552\n",
            "epoch: 80 train loss: 0.009239217266440392 validation: auc: 0.6999775215953244 --- acc: 81.55 --- loss: 0.6604828834533691\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009617769159376621\n",
            "epoch: 81 train loss: 0.008750664070248604 validation: auc: 0.7000265346581507 --- acc: 81.69999999999999 --- loss: 0.6611334681510925\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009185326285660267\n",
            "epoch: 82 train loss: 0.010115011595189571 validation: auc: 0.7004051183158435 --- acc: 81.6 --- loss: 0.6616300940513611\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009356839582324028\n",
            "epoch: 83 train loss: 0.009182807989418507 validation: auc: 0.7004904686493859 --- acc: 81.75 --- loss: 0.6614819169044495\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009381469339132309\n",
            "epoch: 84 train loss: 0.008966113440692425 validation: auc: 0.7006299023625987 --- acc: 81.65 --- loss: 0.6608075499534607\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009028154425323009\n",
            "epoch: 85 train loss: 0.01074547041207552 validation: auc: 0.7007262383826366 --- acc: 81.65 --- loss: 0.6605621576309204\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009166948497295379\n",
            "epoch: 86 train loss: 0.009969072416424751 validation: auc: 0.700577509088543 --- acc: 81.65 --- loss: 0.6610328555107117\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008990442380309105\n",
            "epoch: 87 train loss: 0.010657228529453278 validation: auc: 0.7007431394387835 --- acc: 81.6 --- loss: 0.6620126962661743\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009435221552848815\n",
            "epoch: 88 train loss: 0.007963125593960285 validation: auc: 0.7005065246527256 --- acc: 81.69999999999999 --- loss: 0.6633329391479492\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008924059569835663\n",
            "epoch: 89 train loss: 0.00983958225697279 validation: auc: 0.6998304824068455 --- acc: 81.6 --- loss: 0.6646318435668945\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000899384543299675\n",
            "epoch: 90 train loss: 0.009058220311999321 validation: auc: 0.6989837394938809 --- acc: 81.65 --- loss: 0.6654260158538818\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008797632530331611\n",
            "epoch: 91 train loss: 0.009589233435690403 validation: auc: 0.6984631869645535 --- acc: 81.75 --- loss: 0.6658003926277161\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0009048596955835819\n",
            "epoch: 92 train loss: 0.008011904545128345 validation: auc: 0.6986744501663908 --- acc: 81.69999999999999 --- loss: 0.6657423973083496\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000874569546431303\n",
            "epoch: 93 train loss: 0.00929077435284853 validation: auc: 0.6991037369925246 --- acc: 81.55 --- loss: 0.6655265688896179\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008977440185844899\n",
            "epoch: 94 train loss: 0.008725243620574474 validation: auc: 0.6998068209282399 --- acc: 81.55 --- loss: 0.6658132076263428\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008663742803037167\n",
            "epoch: 95 train loss: 0.009884492494165897 validation: auc: 0.700570748666084 --- acc: 81.55 --- loss: 0.666305422782898\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008532625623047351\n",
            "epoch: 96 train loss: 0.009782722219824791 validation: auc: 0.7012620018624964 --- acc: 81.6 --- loss: 0.6669459342956543\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008561453782021999\n",
            "epoch: 97 train loss: 0.008978045545518398 validation: auc: 0.701630444886501 --- acc: 81.5 --- loss: 0.6672652363777161\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008600777015089989\n",
            "epoch: 98 train loss: 0.008771667256951332 validation: auc: 0.7012569315456523 --- acc: 81.65 --- loss: 0.6686049103736877\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000857061892747879\n",
            "epoch: 99 train loss: 0.010101024061441422 validation: auc: 0.7003037119789616 --- acc: 81.69999999999999 --- loss: 0.6698919534683228\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008885109797120095\n",
            "epoch: 100 train loss: 0.008500502444803715 validation: auc: 0.6992262696495904 --- acc: 81.8 --- loss: 0.6708295345306396\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008754722774028778\n",
            "epoch: 101 train loss: 0.008630744181573391 validation: auc: 0.6986254371035645 --- acc: 81.8 --- loss: 0.6714314222335815\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008584966883063317\n",
            "epoch: 102 train loss: 0.008837496861815453 validation: auc: 0.6983786816838184 --- acc: 81.8 --- loss: 0.6716010570526123\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008361726067960262\n",
            "epoch: 103 train loss: 0.009084617719054222 validation: auc: 0.6983634707332862 --- acc: 81.8 --- loss: 0.6714521646499634\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000845412164926529\n",
            "epoch: 104 train loss: 0.00844381470233202 validation: auc: 0.698628817314794 --- acc: 81.8 --- loss: 0.6713268160820007\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008609984070062637\n",
            "epoch: 105 train loss: 0.008179733529686928 validation: auc: 0.6990074009724867 --- acc: 81.75 --- loss: 0.671380877494812\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008214579895138741\n",
            "epoch: 106 train loss: 0.010526860132813454 validation: auc: 0.6989651483321192 --- acc: 81.75 --- loss: 0.671612560749054\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000841105729341507\n",
            "epoch: 107 train loss: 0.008923986926674843 validation: auc: 0.6988992342131459 --- acc: 81.65 --- loss: 0.6716045141220093\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008126793429255486\n",
            "epoch: 108 train loss: 0.00900809932500124 validation: auc: 0.6990074009724868 --- acc: 81.65 --- loss: 0.6724272966384888\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008206142112612725\n",
            "epoch: 109 train loss: 0.00877625122666359 validation: auc: 0.6986710699551616 --- acc: 81.75 --- loss: 0.6741923093795776\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008489836938679219\n",
            "epoch: 110 train loss: 0.008005215786397457 validation: auc: 0.6986017756249588 --- acc: 81.75 --- loss: 0.6755245327949524\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008735486306250095\n",
            "epoch: 111 train loss: 0.0072590941563248634 validation: auc: 0.6989921900219545 --- acc: 81.69999999999999 --- loss: 0.6770294308662415\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008675060234963894\n",
            "epoch: 112 train loss: 0.008156674914062023 validation: auc: 0.6994738701221439 --- acc: 81.69999999999999 --- loss: 0.6777969002723694\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000821103248745203\n",
            "epoch: 113 train loss: 0.009876524098217487 validation: auc: 0.699800060505781 --- acc: 81.8 --- loss: 0.6775477528572083\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008087320253252983\n",
            "epoch: 114 train loss: 0.009717651642858982 validation: auc: 0.699901466842663 --- acc: 81.85 --- loss: 0.676348090171814\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008253966458141804\n",
            "epoch: 115 train loss: 0.008134657517075539 validation: auc: 0.699732456281193 --- acc: 81.8 --- loss: 0.6755473613739014\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007962191477417946\n",
            "epoch: 116 train loss: 0.008654091507196426 validation: auc: 0.6992811980820681 --- acc: 81.8 --- loss: 0.6752225160598755\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008006692864000797\n",
            "epoch: 117 train loss: 0.00845312513411045 validation: auc: 0.6987014918562261 --- acc: 81.65 --- loss: 0.6752543449401855\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008318648673593998\n",
            "epoch: 118 train loss: 0.007018466480076313 validation: auc: 0.698324598304148 --- acc: 81.8 --- loss: 0.6752168536186218\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007720217574387789\n",
            "epoch: 119 train loss: 0.00988121796399355 validation: auc: 0.6982840357693952 --- acc: 81.6 --- loss: 0.6757030487060547\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000807861890643835\n",
            "epoch: 120 train loss: 0.0077203731052577496 validation: auc: 0.6986085360474177 --- acc: 81.75 --- loss: 0.6775059103965759\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007884663529694081\n",
            "epoch: 121 train loss: 0.008039643056690693 validation: auc: 0.6993961252638677 --- acc: 81.8 --- loss: 0.6800033450126648\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007977027446031571\n",
            "epoch: 122 train loss: 0.007385707926005125 validation: auc: 0.6994299273761617 --- acc: 81.75 --- loss: 0.6824986338615417\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007654809392988682\n",
            "epoch: 123 train loss: 0.008721244521439075 validation: auc: 0.6993352814617385 --- acc: 81.65 --- loss: 0.68365079164505\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008275531232357025\n",
            "epoch: 124 train loss: 0.006474258378148079 validation: auc: 0.6991645807946538 --- acc: 81.69999999999999 --- loss: 0.6840035319328308\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008311478421092033\n",
            "epoch: 125 train loss: 0.006881173700094223 validation: auc: 0.6987133225955291 --- acc: 81.75 --- loss: 0.6839277744293213\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008002346381545067\n",
            "epoch: 126 train loss: 0.008867680095136166 validation: auc: 0.6981978403830456 --- acc: 81.8 --- loss: 0.6829593777656555\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008099619299173355\n",
            "epoch: 127 train loss: 0.008050767704844475 validation: auc: 0.6975403892989273 --- acc: 81.75 --- loss: 0.6812089085578918\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007783726323395967\n",
            "epoch: 128 train loss: 0.008461276069283485 validation: auc: 0.6973375766251634 --- acc: 81.6 --- loss: 0.6808287501335144\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007518688216805458\n",
            "epoch: 129 train loss: 0.008606893941760063 validation: auc: 0.6972429307107401 --- acc: 81.5 --- loss: 0.6825433969497681\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007788592483848333\n",
            "epoch: 130 train loss: 0.007609497290104628 validation: auc: 0.6972818031398782 --- acc: 81.65 --- loss: 0.6845187544822693\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007673591375350952\n",
            "epoch: 131 train loss: 0.0079481927677989 validation: auc: 0.697392505057641 --- acc: 81.75 --- loss: 0.684897243976593\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007681360468268395\n",
            "epoch: 132 train loss: 0.00792764127254486 validation: auc: 0.6975961627842124 --- acc: 81.65 --- loss: 0.6847671270370483\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0008110014721751213\n",
            "epoch: 133 train loss: 0.006505606230348349 validation: auc: 0.6983144576704599 --- acc: 81.65 --- loss: 0.6843612194061279\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007679169066250324\n",
            "epoch: 134 train loss: 0.008609035983681679 validation: auc: 0.699000640550028 --- acc: 81.69999999999999 --- loss: 0.6838258504867554\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007598242722451687\n",
            "epoch: 135 train loss: 0.008765745908021927 validation: auc: 0.6996209093106227 --- acc: 81.69999999999999 --- loss: 0.6836618185043335\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007709748111665249\n",
            "epoch: 136 train loss: 0.008027558214962482 validation: auc: 0.6997358364924224 --- acc: 81.75 --- loss: 0.6839900016784668\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007607670966535806\n",
            "epoch: 137 train loss: 0.007936378009617329 validation: auc: 0.6990581041409277 --- acc: 81.65 --- loss: 0.6848587989807129\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007613154593855143\n",
            "epoch: 138 train loss: 0.007944725453853607 validation: auc: 0.6987048720674556 --- acc: 81.6 --- loss: 0.6860320568084717\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007411511149257421\n",
            "epoch: 139 train loss: 0.00857271533459425 validation: auc: 0.6978429182039585 --- acc: 81.55 --- loss: 0.6862092614173889\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007365226279944182\n",
            "epoch: 140 train loss: 0.008244399912655354 validation: auc: 0.6968254746239093 --- acc: 81.5 --- loss: 0.6863237023353577\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007148613221943378\n",
            "epoch: 141 train loss: 0.00868892390280962 validation: auc: 0.6965060446627309 --- acc: 81.5 --- loss: 0.6881729364395142\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007257478311657906\n",
            "epoch: 142 train loss: 0.007530763745307922 validation: auc: 0.6967342089207154 --- acc: 81.6 --- loss: 0.6909030079841614\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007260789163410664\n",
            "epoch: 143 train loss: 0.0073759895749390125 validation: auc: 0.6964790029728958 --- acc: 81.65 --- loss: 0.6929200291633606\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007163051515817643\n",
            "epoch: 144 train loss: 0.008077047765254974 validation: auc: 0.69642660969884 --- acc: 81.6 --- loss: 0.6929996609687805\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006910413969308138\n",
            "epoch: 145 train loss: 0.009021513164043427 validation: auc: 0.6967494198712478 --- acc: 81.55 --- loss: 0.6927120089530945\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007064285222440958\n",
            "epoch: 146 train loss: 0.008405416272580624 validation: auc: 0.6967764615610831 --- acc: 81.6 --- loss: 0.6929336190223694\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007483033929020167\n",
            "epoch: 147 train loss: 0.007353328634053469 validation: auc: 0.6967680110330094 --- acc: 81.55 --- loss: 0.6914790868759155\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007009046152234077\n",
            "epoch: 148 train loss: 0.010279146023094654 validation: auc: 0.6970181466639851 --- acc: 81.45 --- loss: 0.6896584630012512\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000769552355632186\n",
            "epoch: 149 train loss: 0.007344541605561972 validation: auc: 0.6971803968029961 --- acc: 81.5 --- loss: 0.6885761618614197\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007652362808585166\n",
            "epoch: 150 train loss: 0.007112476043403149 validation: auc: 0.6973629282093837 --- acc: 81.55 --- loss: 0.6877582669258118\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007736212108284235\n",
            "epoch: 151 train loss: 0.00670351879671216 validation: auc: 0.6975572903550742 --- acc: 81.65 --- loss: 0.6878007650375366\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007537615485489369\n",
            "epoch: 152 train loss: 0.007992067374289036 validation: auc: 0.6973781391599161 --- acc: 81.55 --- loss: 0.6900324821472168\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007520419545471668\n",
            "epoch: 153 train loss: 0.007631240878254175 validation: auc: 0.6972260296545932 --- acc: 81.6 --- loss: 0.6927710771560669\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006914126686751842\n",
            "epoch: 154 train loss: 0.008602573536336422 validation: auc: 0.6970756102548847 --- acc: 81.5 --- loss: 0.6941239237785339\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007230106741189957\n",
            "epoch: 155 train loss: 0.007475706748664379 validation: auc: 0.6968153339902211 --- acc: 81.6 --- loss: 0.6944391131401062\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007004818879067897\n",
            "epoch: 156 train loss: 0.008417454548180103 validation: auc: 0.6963437945237199 --- acc: 81.6 --- loss: 0.6936752796173096\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007234119344502688\n",
            "epoch: 157 train loss: 0.007471947465091944 validation: auc: 0.6959905624502475 --- acc: 81.6 --- loss: 0.6927821040153503\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006970205344259739\n",
            "epoch: 158 train loss: 0.007850498892366886 validation: auc: 0.6957826794596395 --- acc: 81.55 --- loss: 0.6918443441390991\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007166736293584108\n",
            "epoch: 159 train loss: 0.006874254904687405 validation: auc: 0.6957843695652541 --- acc: 81.55 --- loss: 0.6925286054611206\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007241425570100546\n",
            "epoch: 160 train loss: 0.006862661801278591 validation: auc: 0.6960987292095884 --- acc: 81.65 --- loss: 0.6942457556724548\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006936375983059406\n",
            "epoch: 161 train loss: 0.007958843372762203 validation: auc: 0.6962761902991318 --- acc: 81.65 --- loss: 0.6962652802467346\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007392368279397488\n",
            "epoch: 162 train loss: 0.005529910326004028 validation: auc: 0.696069152361331 --- acc: 81.75 --- loss: 0.6978834867477417\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007054419722408057\n",
            "epoch: 163 train loss: 0.007135533262044191 validation: auc: 0.6963742164247844 --- acc: 81.69999999999999 --- loss: 0.6996376514434814\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007078004069626332\n",
            "epoch: 164 train loss: 0.007808353751897812 validation: auc: 0.6962626694542142 --- acc: 81.6 --- loss: 0.7009149789810181\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007535671815276146\n",
            "epoch: 165 train loss: 0.005720785818994045 validation: auc: 0.6962964715665082 --- acc: 81.6 --- loss: 0.6989134550094604\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006897117011249066\n",
            "epoch: 166 train loss: 0.007965554483234882 validation: auc: 0.6961105599488911 --- acc: 81.5 --- loss: 0.6948402523994446\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007069434970617295\n",
            "epoch: 167 train loss: 0.006847575772553682 validation: auc: 0.695909437380742 --- acc: 81.39999999999999 --- loss: 0.691893458366394\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007203644607216119\n",
            "epoch: 168 train loss: 0.00635878462344408 validation: auc: 0.696392807586546 --- acc: 81.35 --- loss: 0.6931966543197632\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007211203686892986\n",
            "epoch: 169 train loss: 0.006825519725680351 validation: auc: 0.6975674309887625 --- acc: 81.55 --- loss: 0.6973998546600342\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007119889371097088\n",
            "epoch: 170 train loss: 0.0070519037544727325 validation: auc: 0.6981471372146044 --- acc: 81.75 --- loss: 0.701352059841156\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006659207865595818\n",
            "epoch: 171 train loss: 0.008735278621315956 validation: auc: 0.6976958790154797 --- acc: 81.8 --- loss: 0.7033345699310303\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007284809369593858\n",
            "epoch: 172 train loss: 0.005859022028744221 validation: auc: 0.6966192817389159 --- acc: 81.85 --- loss: 0.7033435106277466\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006400992162525654\n",
            "epoch: 173 train loss: 0.00991331972181797 validation: auc: 0.6957463421889234 --- acc: 81.69999999999999 --- loss: 0.7028087973594666\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007082126103341579\n",
            "epoch: 174 train loss: 0.007308831438422203 validation: auc: 0.6951404393260535 --- acc: 81.69999999999999 --- loss: 0.7034523487091064\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007205253932625055\n",
            "epoch: 175 train loss: 0.006838357541710138 validation: auc: 0.6939404643396166 --- acc: 81.69999999999999 --- loss: 0.7052307724952698\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.000699147442355752\n",
            "epoch: 176 train loss: 0.00772316288203001 validation: auc: 0.6933117450509483 --- acc: 81.65 --- loss: 0.7063630819320679\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006789523176848888\n",
            "epoch: 177 train loss: 0.008413901552557945 validation: auc: 0.6940215894091222 --- acc: 81.55 --- loss: 0.7041845917701721\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007099403068423271\n",
            "epoch: 178 train loss: 0.006943422369658947 validation: auc: 0.6955088823500581 --- acc: 81.45 --- loss: 0.7008398771286011\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006992436479777098\n",
            "epoch: 179 train loss: 0.007165791932493448 validation: auc: 0.6957632432450704 --- acc: 81.5 --- loss: 0.6985820531845093\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007237731013447046\n",
            "epoch: 180 train loss: 0.006205657031387091 validation: auc: 0.6950677647846215 --- acc: 81.6 --- loss: 0.6998552083969116\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007153375074267388\n",
            "epoch: 181 train loss: 0.007417670451104641 validation: auc: 0.6945252408823028 --- acc: 81.55 --- loss: 0.7029005289077759\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007399910129606724\n",
            "epoch: 182 train loss: 0.006511711981147528 validation: auc: 0.693786664728679 --- acc: 81.45 --- loss: 0.7070226073265076\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007473967503756285\n",
            "epoch: 183 train loss: 0.0055792913772165775 validation: auc: 0.6932230145061764 --- acc: 81.6 --- loss: 0.7109626531600952\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007032629102468491\n",
            "epoch: 184 train loss: 0.007385155186057091 validation: auc: 0.6932720275690029 --- acc: 81.55 --- loss: 0.7130693197250366\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0007069392129778862\n",
            "epoch: 185 train loss: 0.006686973385512829 validation: auc: 0.6941255309044263 --- acc: 81.5 --- loss: 0.711529552936554\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006783573422580958\n",
            "epoch: 186 train loss: 0.007143753115087748 validation: auc: 0.6956660621722253 --- acc: 81.39999999999999 --- loss: 0.7057065367698669\n",
            "<Timer(Thread-7685, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0006836177781224251\n",
            "epoch: 187 train loss: 0.006496685557067394 validation: auc: 0.696609986158035 --- acc: 81.45 --- loss: 0.7007165551185608\n",
            "time up, break\n",
            "test auc: 0.6976248945796623 test acc: 81.55 test loss 0.6928102970123291\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.19384100437164306\n",
            "epoch: 0 train loss: 1.8901957273483276 validation: auc: 0.4962439686804774 --- acc: 56.3 --- loss: 1.607340931892395\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1677505612373352\n",
            "epoch: 1 train loss: 1.4526393413543701 validation: auc: 0.5179531073936032 --- acc: 63.05 --- loss: 1.2366158962249756\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.12729804515838622\n",
            "epoch: 2 train loss: 1.0728319883346558 validation: auc: 0.5611294097252318 --- acc: 72.39999999999999 --- loss: 0.8962016701698303\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08920909166336059\n",
            "epoch: 3 train loss: 0.8129461407661438 validation: auc: 0.6155846407211579 --- acc: 79.45 --- loss: 0.7360406517982483\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07183239459991456\n",
            "epoch: 4 train loss: 0.6515805125236511 validation: auc: 0.6430939585680853 --- acc: 81.3 --- loss: 0.7162054777145386\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06351218223571778\n",
            "epoch: 5 train loss: 0.6478962898254395 validation: auc: 0.6510899713963767 --- acc: 78.2 --- loss: 0.7495951056480408\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06111178398132324\n",
            "epoch: 6 train loss: 0.6373637914657593 validation: auc: 0.6600809713674843 --- acc: 77.9 --- loss: 0.6942874789237976\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05187216997146606\n",
            "epoch: 7 train loss: 0.4817233681678772 validation: auc: 0.6699441651498078 --- acc: 80.95 --- loss: 0.5975513458251953\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03981824219226837\n",
            "epoch: 8 train loss: 0.364006906747818 validation: auc: 0.6733950189246193 --- acc: 81.45 --- loss: 0.5528891682624817\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.033324986696243286\n",
            "epoch: 9 train loss: 0.29147377610206604 validation: auc: 0.6710601398399352 --- acc: 81.05 --- loss: 0.5268791317939758\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027684634923934935\n",
            "epoch: 10 train loss: 0.25895029306411743 validation: auc: 0.6697094131923378 --- acc: 79.75 --- loss: 0.5134819746017456\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.023409849405288695\n",
            "epoch: 11 train loss: 0.22372226417064667 validation: auc: 0.6750256421368929 --- acc: 79.75 --- loss: 0.5046377182006836\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01969494968652725\n",
            "epoch: 12 train loss: 0.16990652680397034 validation: auc: 0.6834586547629367 --- acc: 80.10000000000001 --- loss: 0.49068760871887207\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015322022140026093\n",
            "epoch: 13 train loss: 0.13623511791229248 validation: auc: 0.689562205657161 --- acc: 80.95 --- loss: 0.48592039942741394\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012418517470359802\n",
            "epoch: 14 train loss: 0.12790845334529877 validation: auc: 0.694168761376441 --- acc: 81.55 --- loss: 0.491556853055954\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011263689398765564\n",
            "epoch: 15 train loss: 0.11684038490056992 validation: auc: 0.6979013175002167 --- acc: 82.0 --- loss: 0.4923761785030365\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010489318519830704\n",
            "epoch: 16 train loss: 0.09860993176698685 validation: auc: 0.7009296177515818 --- acc: 81.95 --- loss: 0.4855254292488098\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00951230600476265\n",
            "epoch: 17 train loss: 0.09328713268041611 validation: auc: 0.7030369680159486 --- acc: 81.89999999999999 --- loss: 0.4783608615398407\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008993856608867645\n",
            "epoch: 18 train loss: 0.09071514755487442 validation: auc: 0.704393112016411 --- acc: 81.65 --- loss: 0.4750921130180359\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008845169097185135\n",
            "epoch: 19 train loss: 0.08542915433645248 validation: auc: 0.7043317153506111 --- acc: 81.69999999999999 --- loss: 0.4769475758075714\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008559257537126542\n",
            "epoch: 20 train loss: 0.08879218995571136 validation: auc: 0.7032500505619601 --- acc: 82.0 --- loss: 0.4821913540363312\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008407919108867646\n",
            "epoch: 21 train loss: 0.0863700658082962 validation: auc: 0.7022875675363323 --- acc: 82.1 --- loss: 0.4861904978752136\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008252011984586716\n",
            "epoch: 22 train loss: 0.08227216452360153 validation: auc: 0.7018054231313744 --- acc: 81.89999999999999 --- loss: 0.4880983233451843\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007842127978801728\n",
            "epoch: 23 train loss: 0.08532016724348068 validation: auc: 0.7021268527346798 --- acc: 81.89999999999999 --- loss: 0.4892587959766388\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00776810273528099\n",
            "epoch: 24 train loss: 0.07500097900629044 validation: auc: 0.7026234434139436 --- acc: 81.8 --- loss: 0.49066096544265747\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007321926206350327\n",
            "epoch: 25 train loss: 0.07876300066709518 validation: auc: 0.7021665799890209 --- acc: 81.85 --- loss: 0.49286556243896484\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0071537822484970095\n",
            "epoch: 26 train loss: 0.07249724119901657 validation: auc: 0.700891696281529 --- acc: 82.0 --- loss: 0.4943162500858307\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006856291741132736\n",
            "epoch: 27 train loss: 0.07085651159286499 validation: auc: 0.7000790933518246 --- acc: 81.75 --- loss: 0.49390241503715515\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0065789870917797085\n",
            "epoch: 28 train loss: 0.06890743970870972 validation: auc: 0.6996998786512959 --- acc: 81.69999999999999 --- loss: 0.49204474687576294\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006479572504758835\n",
            "epoch: 29 train loss: 0.06073211506009102 validation: auc: 0.6997305769841958 --- acc: 81.85 --- loss: 0.4910487234592438\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0061562109738588335\n",
            "epoch: 30 train loss: 0.06315720826387405 validation: auc: 0.700619022854006 --- acc: 82.05 --- loss: 0.49176275730133057\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006003853306174279\n",
            "epoch: 31 train loss: 0.0619351752102375 validation: auc: 0.7017656958770333 --- acc: 82.1 --- loss: 0.49243590235710144\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00586094856262207\n",
            "epoch: 32 train loss: 0.06077122688293457 validation: auc: 0.7020907370489151 --- acc: 82.25 --- loss: 0.4933215379714966\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00580141730606556\n",
            "epoch: 33 train loss: 0.0592220276594162 validation: auc: 0.7015706711739043 --- acc: 82.3 --- loss: 0.49419426918029785\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005691337212920189\n",
            "epoch: 34 train loss: 0.05999525636434555 validation: auc: 0.7006677790297884 --- acc: 82.25 --- loss: 0.4943372905254364\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005576934292912483\n",
            "epoch: 35 train loss: 0.05950489640235901 validation: auc: 0.6999400479616307 --- acc: 82.45 --- loss: 0.495430052280426\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005432490259408951\n",
            "epoch: 36 train loss: 0.060332585126161575 validation: auc: 0.7003030106035654 --- acc: 82.39999999999999 --- loss: 0.4967822730541229\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005390571430325508\n",
            "epoch: 37 train loss: 0.05825303867459297 validation: auc: 0.7013323076478576 --- acc: 82.35 --- loss: 0.49761682748794556\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005351182818412781\n",
            "epoch: 38 train loss: 0.055527959018945694 validation: auc: 0.703226575366213 --- acc: 82.19999999999999 --- loss: 0.4964223802089691\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0052946265786886215\n",
            "epoch: 39 train loss: 0.05427704006433487 validation: auc: 0.703833318887059 --- acc: 82.15 --- loss: 0.4958885908126831\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005166251957416534\n",
            "epoch: 40 train loss: 0.05493104085326195 validation: auc: 0.703499248793736 --- acc: 82.19999999999999 --- loss: 0.496956467628479\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005141828581690789\n",
            "epoch: 41 train loss: 0.05144103989005089 validation: auc: 0.7028997284100431 --- acc: 82.3 --- loss: 0.498544305562973\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004950691759586334\n",
            "epoch: 42 train loss: 0.05404988303780556 validation: auc: 0.7022008898904972 --- acc: 82.39999999999999 --- loss: 0.5001325011253357\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004913263395428658\n",
            "epoch: 43 train loss: 0.051426518708467484 validation: auc: 0.7016248447025513 --- acc: 82.3 --- loss: 0.500090479850769\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004847943410277366\n",
            "epoch: 44 train loss: 0.05038198456168175 validation: auc: 0.7017909768570686 --- acc: 82.65 --- loss: 0.5000672936439514\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004732486605644226\n",
            "epoch: 45 train loss: 0.050839390605688095 validation: auc: 0.7021864436161913 --- acc: 82.39999999999999 --- loss: 0.5002606511116028\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004719918221235275\n",
            "epoch: 46 train loss: 0.04754161462187767 validation: auc: 0.7018686255814627 --- acc: 82.55 --- loss: 0.5020658373832703\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00458972118794918\n",
            "epoch: 47 train loss: 0.04949091002345085 validation: auc: 0.7017259686226923 --- acc: 82.6 --- loss: 0.5044698715209961\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004579155892133713\n",
            "epoch: 48 train loss: 0.047467660158872604 validation: auc: 0.7022243650862443 --- acc: 82.45 --- loss: 0.5054592490196228\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004588910192251205\n",
            "epoch: 49 train loss: 0.045711297541856766 validation: auc: 0.7035353644795007 --- acc: 82.35 --- loss: 0.5055517554283142\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004499882087111473\n",
            "epoch: 50 train loss: 0.047044314444065094 validation: auc: 0.7039795874144059 --- acc: 82.15 --- loss: 0.5066472291946411\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004511359333992005\n",
            "epoch: 51 train loss: 0.04494418948888779 validation: auc: 0.7034793851665656 --- acc: 82.25 --- loss: 0.5077610611915588\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004420028999447823\n",
            "epoch: 52 train loss: 0.04861726239323616 validation: auc: 0.7030297448787958 --- acc: 82.19999999999999 --- loss: 0.50653076171875\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004412412643432617\n",
            "epoch: 53 train loss: 0.04582829400897026 validation: auc: 0.703062248995984 --- acc: 82.25 --- loss: 0.5029268264770508\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004328448325395584\n",
            "epoch: 54 train loss: 0.047808244824409485 validation: auc: 0.7027697119412902 --- acc: 82.05 --- loss: 0.5023247003555298\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0043753799051046375\n",
            "epoch: 55 train loss: 0.045211516320705414 validation: auc: 0.7034486868336655 --- acc: 82.1 --- loss: 0.5054187178611755\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004285125806927681\n",
            "epoch: 56 train loss: 0.04538438469171524 validation: auc: 0.704487012799399 --- acc: 82.25 --- loss: 0.5084389448165894\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004221626371145248\n",
            "epoch: 57 train loss: 0.04588612914085388 validation: auc: 0.7059316402299847 --- acc: 82.45 --- loss: 0.5047028064727783\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004168793186545372\n",
            "epoch: 58 train loss: 0.04406815394759178 validation: auc: 0.707443081679235 --- acc: 82.39999999999999 --- loss: 0.5013276934623718\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004176299273967743\n",
            "epoch: 59 train loss: 0.041503287851810455 validation: auc: 0.7068706480598654 --- acc: 82.35 --- loss: 0.506364107131958\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004019856825470925\n",
            "epoch: 60 train loss: 0.045968666672706604 validation: auc: 0.7056535494495969 --- acc: 82.45 --- loss: 0.513501763343811\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004053719341754913\n",
            "epoch: 61 train loss: 0.042382076382637024 validation: auc: 0.7043407442720522 --- acc: 82.35 --- loss: 0.5144189596176147\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00407380722463131\n",
            "epoch: 62 train loss: 0.03986923769116402 validation: auc: 0.70331144722776 --- acc: 82.19999999999999 --- loss: 0.5138737559318542\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004024738445878029\n",
            "epoch: 63 train loss: 0.04076540097594261 validation: auc: 0.7027335962555257 --- acc: 82.19999999999999 --- loss: 0.5160080194473267\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0039980530738830565\n",
            "epoch: 64 train loss: 0.04204808548092842 validation: auc: 0.7029466788015372 --- acc: 82.15 --- loss: 0.5179571509361267\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003992890939116478\n",
            "epoch: 65 train loss: 0.04275071620941162 validation: auc: 0.7050341654387333 --- acc: 82.05 --- loss: 0.5148047804832458\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003982962667942047\n",
            "epoch: 66 train loss: 0.041306547820568085 validation: auc: 0.7079450897113635 --- acc: 82.19999999999999 --- loss: 0.5094459056854248\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003904523327946663\n",
            "epoch: 67 train loss: 0.042288731783628464 validation: auc: 0.7100054895842363 --- acc: 82.45 --- loss: 0.5078303813934326\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003936366736888885\n",
            "epoch: 68 train loss: 0.0397627055644989 validation: auc: 0.7099765970356245 --- acc: 82.39999999999999 --- loss: 0.5106046795845032\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003847751021385193\n",
            "epoch: 69 train loss: 0.04353731498122215 validation: auc: 0.7084001473519979 --- acc: 82.5 --- loss: 0.5109620690345764\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003906382247805595\n",
            "epoch: 70 train loss: 0.040119096636772156 validation: auc: 0.7067225737482303 --- acc: 82.25 --- loss: 0.5085307359695435\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003848402947187424\n",
            "epoch: 71 train loss: 0.040748100727796555 validation: auc: 0.7050720869087862 --- acc: 81.89999999999999 --- loss: 0.5114366412162781\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0037105049937963485\n",
            "epoch: 72 train loss: 0.043155211955308914 validation: auc: 0.7036689925168299 --- acc: 82.15 --- loss: 0.5192520022392273\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003763314336538315\n",
            "epoch: 73 train loss: 0.038236722350120544 validation: auc: 0.7027913813527492 --- acc: 81.95 --- loss: 0.5221338272094727\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003669252246618271\n",
            "epoch: 74 train loss: 0.04157800227403641 validation: auc: 0.70325005056196 --- acc: 81.95 --- loss: 0.5204196572303772\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003681579977273941\n",
            "epoch: 75 train loss: 0.0399588868021965 validation: auc: 0.7037683106526826 --- acc: 81.89999999999999 --- loss: 0.5199523568153381\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0036630693823099135\n",
            "epoch: 76 train loss: 0.039567410945892334 validation: auc: 0.7047885787755339 --- acc: 82.3 --- loss: 0.5222657322883606\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003663695976138115\n",
            "epoch: 77 train loss: 0.03953667730093002 validation: auc: 0.7070367802143828 --- acc: 82.25 --- loss: 0.5208868384361267\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0036479342728853227\n",
            "epoch: 78 train loss: 0.03837147355079651 validation: auc: 0.7096298864522839 --- acc: 82.19999999999999 --- loss: 0.5166627168655396\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003637002408504486\n",
            "epoch: 79 train loss: 0.03852693736553192 validation: auc: 0.7104984686949236 --- acc: 82.35 --- loss: 0.5141971111297607\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003591287508606911\n",
            "epoch: 80 train loss: 0.03903478384017944 validation: auc: 0.7086818497009622 --- acc: 82.15 --- loss: 0.5150570869445801\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003604130819439888\n",
            "epoch: 81 train loss: 0.03672831505537033 validation: auc: 0.7057348097425673 --- acc: 82.25 --- loss: 0.5175358057022095\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0035417333245277407\n",
            "epoch: 82 train loss: 0.03829634189605713 validation: auc: 0.7049781861257982 --- acc: 82.15 --- loss: 0.5174626708030701\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003510138392448425\n",
            "epoch: 83 train loss: 0.038687918335199356 validation: auc: 0.7061790326774724 --- acc: 82.05 --- loss: 0.5167333483695984\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0035252202302217483\n",
            "epoch: 84 train loss: 0.03592687100172043 validation: auc: 0.7083477796076392 --- acc: 82.19999999999999 --- loss: 0.517162024974823\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034691218286752703\n",
            "epoch: 85 train loss: 0.038514189422130585 validation: auc: 0.7090412007743203 --- acc: 82.25 --- loss: 0.5186551809310913\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003536609560251236\n",
            "epoch: 86 train loss: 0.03541853278875351 validation: auc: 0.7091170437144261 --- acc: 82.05 --- loss: 0.5199508666992188\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034809935837984084\n",
            "epoch: 87 train loss: 0.03650541231036186 validation: auc: 0.7070367802143827 --- acc: 82.15 --- loss: 0.5224552154541016\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003484734892845154\n",
            "epoch: 88 train loss: 0.035850994288921356 validation: auc: 0.70521113229898 --- acc: 81.89999999999999 --- loss: 0.5239799618721008\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003465072065591812\n",
            "epoch: 89 train loss: 0.03614484891295433 validation: auc: 0.7049131778914219 --- acc: 82.05 --- loss: 0.52485591173172\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034409694373607634\n",
            "epoch: 90 train loss: 0.0354170985519886 validation: auc: 0.7052761405333564 --- acc: 82.05 --- loss: 0.5260775685310364\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033293768763542177\n",
            "epoch: 91 train loss: 0.03968265652656555 validation: auc: 0.7048246944612983 --- acc: 82.1 --- loss: 0.5242526531219482\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033544756472110747\n",
            "epoch: 92 train loss: 0.03633086755871773 validation: auc: 0.7051695992603508 --- acc: 82.0 --- loss: 0.522599458694458\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003406783938407898\n",
            "epoch: 93 train loss: 0.03333934023976326 validation: auc: 0.7057420328797204 --- acc: 82.1 --- loss: 0.5255748629570007\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033471759408712385\n",
            "epoch: 94 train loss: 0.03758647292852402 validation: auc: 0.7059984542486493 --- acc: 82.25 --- loss: 0.5290253162384033\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033916816115379333\n",
            "epoch: 95 train loss: 0.036317337304353714 validation: auc: 0.7059406691514258 --- acc: 82.0 --- loss: 0.5276025533676147\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00339202843606472\n",
            "epoch: 96 train loss: 0.03414560854434967 validation: auc: 0.7042035046661467 --- acc: 81.89999999999999 --- loss: 0.5247313976287842\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033565275371074677\n",
            "epoch: 97 train loss: 0.0351218543946743 validation: auc: 0.7030766952702898 --- acc: 81.85 --- loss: 0.5250824689865112\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033345934003591537\n",
            "epoch: 98 train loss: 0.03538530692458153 validation: auc: 0.7039362485914883 --- acc: 82.05 --- loss: 0.528202474117279\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003267037123441696\n",
            "epoch: 99 train loss: 0.035210054367780685 validation: auc: 0.7055054751379619 --- acc: 82.19999999999999 --- loss: 0.5305975675582886\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032902713865041733\n",
            "epoch: 100 train loss: 0.03285053372383118 validation: auc: 0.706020123660108 --- acc: 82.3 --- loss: 0.5307188034057617\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032298095524311067\n",
            "epoch: 101 train loss: 0.034794025123119354 validation: auc: 0.7054603305307562 --- acc: 82.1 --- loss: 0.5319718718528748\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003226778656244278\n",
            "epoch: 102 train loss: 0.03408027067780495 validation: auc: 0.7055163098436913 --- acc: 81.95 --- loss: 0.5331541895866394\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003249456360936165\n",
            "epoch: 103 train loss: 0.03379330784082413 validation: auc: 0.7056138221952558 --- acc: 81.89999999999999 --- loss: 0.5334447026252747\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032028164714574814\n",
            "epoch: 104 train loss: 0.03509921208024025 validation: auc: 0.706114024443096 --- acc: 81.89999999999999 --- loss: 0.5332838296890259\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003215091675519943\n",
            "epoch: 105 train loss: 0.03302798420190811 validation: auc: 0.7063559995377193 --- acc: 82.15 --- loss: 0.5364251136779785\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003215824440121651\n",
            "epoch: 106 train loss: 0.03333750367164612 validation: auc: 0.7055903469995088 --- acc: 82.19999999999999 --- loss: 0.5355066061019897\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031613193452358244\n",
            "epoch: 107 train loss: 0.03486236184835434 validation: auc: 0.7057871774869261 --- acc: 82.15 --- loss: 0.5283697247505188\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031840935349464416\n",
            "epoch: 108 train loss: 0.03385887295007706 validation: auc: 0.7063794747334662 --- acc: 82.19999999999999 --- loss: 0.5281732678413391\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031541749835014344\n",
            "epoch: 109 train loss: 0.03410424292087555 validation: auc: 0.7057889832712144 --- acc: 82.3 --- loss: 0.5359557271003723\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031547319144010545\n",
            "epoch: 110 train loss: 0.03292081505060196 validation: auc: 0.7045159053480108 --- acc: 82.19999999999999 --- loss: 0.5383928418159485\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003144380450248718\n",
            "epoch: 111 train loss: 0.0357530452311039 validation: auc: 0.7039958394729999 --- acc: 82.05 --- loss: 0.5339561104774475\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031151752918958664\n",
            "epoch: 112 train loss: 0.035910800099372864 validation: auc: 0.7042937938805582 --- acc: 82.15 --- loss: 0.5322405099868774\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031414452940225603\n",
            "epoch: 113 train loss: 0.03256576880812645 validation: auc: 0.7044545086822109 --- acc: 82.1 --- loss: 0.538186252117157\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030349817126989363\n",
            "epoch: 114 train loss: 0.037187717854976654 validation: auc: 0.7048210828927219 --- acc: 82.15 --- loss: 0.540608823299408\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003095807135105133\n",
            "epoch: 115 train loss: 0.03506110608577728 validation: auc: 0.705740227095432 --- acc: 81.89999999999999 --- loss: 0.5353906750679016\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030940627679228784\n",
            "epoch: 116 train loss: 0.03364725410938263 validation: auc: 0.7050793100459392 --- acc: 81.89999999999999 --- loss: 0.5358149409294128\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029860371723771095\n",
            "epoch: 117 train loss: 0.0349070280790329 validation: auc: 0.702051009794574 --- acc: 82.1 --- loss: 0.5424513220787048\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029878368601202964\n",
            "epoch: 118 train loss: 0.03340640664100647 validation: auc: 0.7008429401057468 --- acc: 82.3 --- loss: 0.5408691167831421\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030288128182291985\n",
            "epoch: 119 train loss: 0.03381061181426048 validation: auc: 0.7029502903701135 --- acc: 82.0 --- loss: 0.5335263013839722\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031195826828479766\n",
            "epoch: 120 train loss: 0.032563529908657074 validation: auc: 0.704245037704776 --- acc: 82.25 --- loss: 0.5367617011070251\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003049609251320362\n",
            "epoch: 121 train loss: 0.03261661157011986 validation: auc: 0.7036527404582358 --- acc: 82.1 --- loss: 0.5452014803886414\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030272850766777993\n",
            "epoch: 122 train loss: 0.03229363635182381 validation: auc: 0.7044490913293462 --- acc: 82.0 --- loss: 0.5419321060180664\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030043831095099447\n",
            "epoch: 123 train loss: 0.033401623368263245 validation: auc: 0.7040698766288174 --- acc: 82.1 --- loss: 0.5389834642410278\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030189806595444678\n",
            "epoch: 124 train loss: 0.03257305547595024 validation: auc: 0.7020528155788621 --- acc: 81.89999999999999 --- loss: 0.5433366894721985\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002970406971871853\n",
            "epoch: 125 train loss: 0.0329635813832283 validation: auc: 0.6996150067897489 --- acc: 82.05 --- loss: 0.5463266968727112\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029708009213209154\n",
            "epoch: 126 train loss: 0.03140832111239433 validation: auc: 0.6976503134841525 --- acc: 82.1 --- loss: 0.5488417744636536\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002974662370979786\n",
            "epoch: 127 train loss: 0.03278861194849014 validation: auc: 0.6986759989598683 --- acc: 82.15 --- loss: 0.5501284599304199\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002985897473990917\n",
            "epoch: 128 train loss: 0.03235114738345146 validation: auc: 0.7031037820346133 --- acc: 82.19999999999999 --- loss: 0.544198751449585\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028870172798633577\n",
            "epoch: 129 train loss: 0.03345080465078354 validation: auc: 0.7060056773858021 --- acc: 82.1 --- loss: 0.5390920639038086\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002874714322388172\n",
            "epoch: 130 train loss: 0.03335142880678177 validation: auc: 0.7067749414925891 --- acc: 82.19999999999999 --- loss: 0.5383740067481995\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029598431661725043\n",
            "epoch: 131 train loss: 0.031055422499775887 validation: auc: 0.7052499566611771 --- acc: 82.25 --- loss: 0.5384799838066101\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003032081201672554\n",
            "epoch: 132 train loss: 0.03076961264014244 validation: auc: 0.7048770622056572 --- acc: 81.95 --- loss: 0.5400320291519165\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002938923612236977\n",
            "epoch: 133 train loss: 0.033866025507450104 validation: auc: 0.7051533472017567 --- acc: 82.25 --- loss: 0.544134259223938\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028275275602936746\n",
            "epoch: 134 train loss: 0.036090169101953506 validation: auc: 0.7046080003467106 --- acc: 82.15 --- loss: 0.5451551675796509\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028509611263871195\n",
            "epoch: 135 train loss: 0.03301483765244484 validation: auc: 0.7046386986796105 --- acc: 82.19999999999999 --- loss: 0.5402617454528809\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029394252225756643\n",
            "epoch: 136 train loss: 0.029469475150108337 validation: auc: 0.7050955621045334 --- acc: 82.3 --- loss: 0.5397455096244812\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029908981174230575\n",
            "epoch: 137 train loss: 0.030360905453562737 validation: auc: 0.7063903094391957 --- acc: 82.1 --- loss: 0.5424515008926392\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029832854866981505\n",
            "epoch: 138 train loss: 0.030091645196080208 validation: auc: 0.7081527549045101 --- acc: 82.05 --- loss: 0.5401661396026611\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002947850711643696\n",
            "epoch: 139 train loss: 0.029984455555677414 validation: auc: 0.7083730605876744 --- acc: 82.25 --- loss: 0.5372692942619324\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028314383700489996\n",
            "epoch: 140 train loss: 0.032409314066171646 validation: auc: 0.7070259455086534 --- acc: 82.25 --- loss: 0.5375810861587524\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002787159010767937\n",
            "epoch: 141 train loss: 0.03255220502614975 validation: auc: 0.7038152610441767 --- acc: 82.15 --- loss: 0.5429757833480835\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028794465586543085\n",
            "epoch: 142 train loss: 0.029817277565598488 validation: auc: 0.6999481739909277 --- acc: 82.25 --- loss: 0.5537533164024353\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002902340330183506\n",
            "epoch: 143 train loss: 0.0330149345099926 validation: auc: 0.6994642238016816 --- acc: 82.1 --- loss: 0.5545145869255066\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029380850493907927\n",
            "epoch: 144 train loss: 0.02849321812391281 validation: auc: 0.7008826673600879 --- acc: 82.0 --- loss: 0.5488856434822083\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002872958220541477\n",
            "epoch: 145 train loss: 0.02918710745871067 validation: auc: 0.7021828320476149 --- acc: 82.0 --- loss: 0.5517445802688599\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002873730845749378\n",
            "epoch: 146 train loss: 0.03100861981511116 validation: auc: 0.7030649576724163 --- acc: 82.05 --- loss: 0.5547671914100647\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028707021847367287\n",
            "epoch: 147 train loss: 0.031140517443418503 validation: auc: 0.7040256349137557 --- acc: 81.69999999999999 --- loss: 0.5492954254150391\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002894555777311325\n",
            "epoch: 148 train loss: 0.02838466688990593 validation: auc: 0.7048662274999278 --- acc: 81.89999999999999 --- loss: 0.548881471157074\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002845715917646885\n",
            "epoch: 149 train loss: 0.03125833347439766 validation: auc: 0.7037854656034208 --- acc: 82.15 --- loss: 0.5522316098213196\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002851158380508423\n",
            "epoch: 150 train loss: 0.03058837167918682 validation: auc: 0.7034974430094478 --- acc: 82.19999999999999 --- loss: 0.5482619404792786\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002815684117376804\n",
            "epoch: 151 train loss: 0.0314408540725708 validation: auc: 0.7046757172575193 --- acc: 82.19999999999999 --- loss: 0.5433633327484131\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002813347615301609\n",
            "epoch: 152 train loss: 0.03172312304377556 validation: auc: 0.704682037502528 --- acc: 82.15 --- loss: 0.5478708148002625\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028529737144708635\n",
            "epoch: 153 train loss: 0.030064163729548454 validation: auc: 0.7041466224610673 --- acc: 82.15 --- loss: 0.5554103851318359\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028746690601110458\n",
            "epoch: 154 train loss: 0.02977735549211502 validation: auc: 0.7026207347375111 --- acc: 82.05 --- loss: 0.5548382997512817\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029102770611643793\n",
            "epoch: 155 train loss: 0.029605863615870476 validation: auc: 0.7011896506890873 --- acc: 82.19999999999999 --- loss: 0.5500432848930359\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027941804379224776\n",
            "epoch: 156 train loss: 0.03423209488391876 validation: auc: 0.7009097541244113 --- acc: 82.0 --- loss: 0.5526741743087769\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027586016803979874\n",
            "epoch: 157 train loss: 0.03280295431613922 validation: auc: 0.7010081693681199 --- acc: 81.95 --- loss: 0.559669017791748\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027708997949957848\n",
            "epoch: 158 train loss: 0.03286799043416977 validation: auc: 0.702167482881165 --- acc: 82.05 --- loss: 0.5524267554283142\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027800049632787705\n",
            "epoch: 159 train loss: 0.030795471742749214 validation: auc: 0.7048581014706308 --- acc: 82.0 --- loss: 0.5494074821472168\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027128737419843674\n",
            "epoch: 160 train loss: 0.032390642911195755 validation: auc: 0.706066171159458 --- acc: 81.89999999999999 --- loss: 0.55643230676651\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027981171384453773\n",
            "epoch: 161 train loss: 0.029475558549165726 validation: auc: 0.7068300179133802 --- acc: 82.1 --- loss: 0.5545003414154053\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027896584942936897\n",
            "epoch: 162 train loss: 0.030202124267816544 validation: auc: 0.7071406128109561 --- acc: 81.89999999999999 --- loss: 0.5475538969039917\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002688177116215229\n",
            "epoch: 163 train loss: 0.03372673690319061 validation: auc: 0.7066205469359452 --- acc: 81.95 --- loss: 0.5458003878593445\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027376547455787658\n",
            "epoch: 164 train loss: 0.029850361868739128 validation: auc: 0.7054955433243766 --- acc: 81.95 --- loss: 0.5522850751876831\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027054652571678163\n",
            "epoch: 165 train loss: 0.03438166528940201 validation: auc: 0.7051000765652539 --- acc: 81.85 --- loss: 0.5521845817565918\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027901921421289444\n",
            "epoch: 166 train loss: 0.029713701456785202 validation: auc: 0.7047262792175899 --- acc: 82.0 --- loss: 0.5491871237754822\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002768702805042267\n",
            "epoch: 167 train loss: 0.029117174446582794 validation: auc: 0.7040527216780792 --- acc: 81.95 --- loss: 0.5578480958938599\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027825718745589255\n",
            "epoch: 168 train loss: 0.02875571697950363 validation: auc: 0.7051849484268007 --- acc: 81.8 --- loss: 0.5602055788040161\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027453463524580004\n",
            "epoch: 169 train loss: 0.029389936476945877 validation: auc: 0.7061961876282106 --- acc: 81.85 --- loss: 0.5496160984039307\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026889974251389503\n",
            "epoch: 170 train loss: 0.029707442969083786 validation: auc: 0.7062548756175783 --- acc: 81.8 --- loss: 0.5485407710075378\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002705523557960987\n",
            "epoch: 171 train loss: 0.027892909944057465 validation: auc: 0.705894621652076 --- acc: 82.19999999999999 --- loss: 0.5603495836257935\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027121953666210175\n",
            "epoch: 172 train loss: 0.029066743329167366 validation: auc: 0.7045041677501372 --- acc: 82.3 --- loss: 0.5622799396514893\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002726581133902073\n",
            "epoch: 173 train loss: 0.027952400967478752 validation: auc: 0.7049194981364306 --- acc: 82.0 --- loss: 0.5530039668083191\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002710398659110069\n",
            "epoch: 174 train loss: 0.029613027349114418 validation: auc: 0.7051686963682066 --- acc: 82.0 --- loss: 0.5536642074584961\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026518307626247407\n",
            "epoch: 175 train loss: 0.03036598488688469 validation: auc: 0.7041971844211378 --- acc: 82.1 --- loss: 0.5571827292442322\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027067603543400763\n",
            "epoch: 176 train loss: 0.028789162635803223 validation: auc: 0.7047028040218427 --- acc: 82.19999999999999 --- loss: 0.5527524352073669\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027591509744524954\n",
            "epoch: 177 train loss: 0.028898656368255615 validation: auc: 0.7075649721186906 --- acc: 82.0 --- loss: 0.5507296919822693\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002722989022731781\n",
            "epoch: 178 train loss: 0.028766274452209473 validation: auc: 0.7075740010401318 --- acc: 82.15 --- loss: 0.5540024042129517\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002692911587655544\n",
            "epoch: 179 train loss: 0.029597368091344833 validation: auc: 0.7046143205917194 --- acc: 82.15 --- loss: 0.558708667755127\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002712929993867874\n",
            "epoch: 180 train loss: 0.02949131466448307 validation: auc: 0.7006145083932853 --- acc: 82.19999999999999 --- loss: 0.5599873065948486\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027601664885878565\n",
            "epoch: 181 train loss: 0.029992325231432915 validation: auc: 0.6965406590390337 --- acc: 82.0 --- loss: 0.5593866109848022\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002748197317123413\n",
            "epoch: 182 train loss: 0.029128609225153923 validation: auc: 0.6948739201409957 --- acc: 81.8 --- loss: 0.5663245916366577\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002731432765722275\n",
            "epoch: 183 train loss: 0.02761002816259861 validation: auc: 0.6973496504001617 --- acc: 81.89999999999999 --- loss: 0.5701934695243835\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027160560712218285\n",
            "epoch: 184 train loss: 0.03028179705142975 validation: auc: 0.7015426815174366 --- acc: 81.8 --- loss: 0.562755286693573\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002707892470061779\n",
            "epoch: 185 train loss: 0.030753137543797493 validation: auc: 0.7050440972523186 --- acc: 81.85 --- loss: 0.5519680380821228\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002681702934205532\n",
            "epoch: 186 train loss: 0.029556291177868843 validation: auc: 0.7064471916442749 --- acc: 81.8 --- loss: 0.5510885715484619\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026723263785243035\n",
            "epoch: 187 train loss: 0.027615277096629143 validation: auc: 0.704168291872526 --- acc: 82.25 --- loss: 0.5580453872680664\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026856185868382454\n",
            "epoch: 188 train loss: 0.030071794986724854 validation: auc: 0.7008673181936378 --- acc: 82.15 --- loss: 0.5601246953010559\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027244411408901214\n",
            "epoch: 189 train loss: 0.027339838445186615 validation: auc: 0.6998199633064632 --- acc: 82.3 --- loss: 0.5576074719429016\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002637704834342003\n",
            "epoch: 190 train loss: 0.028308836743235588 validation: auc: 0.7004176779058681 --- acc: 82.35 --- loss: 0.5600001215934753\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002659374475479126\n",
            "epoch: 191 train loss: 0.026024622842669487 validation: auc: 0.7008059215278379 --- acc: 82.25 --- loss: 0.5602574348449707\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002610066905617714\n",
            "epoch: 192 train loss: 0.02947111427783966 validation: auc: 0.7025557265031348 --- acc: 82.05 --- loss: 0.5542646646499634\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025719797238707544\n",
            "epoch: 193 train loss: 0.029843175783753395 validation: auc: 0.7039263167779031 --- acc: 82.1 --- loss: 0.553796648979187\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002582736313343048\n",
            "epoch: 194 train loss: 0.029364101588726044 validation: auc: 0.7028265941463696 --- acc: 82.05 --- loss: 0.5615819692611694\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002533480152487755\n",
            "epoch: 195 train loss: 0.02995411865413189 validation: auc: 0.6992041908641761 --- acc: 82.1 --- loss: 0.5638769865036011\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002556355483829975\n",
            "epoch: 196 train loss: 0.030179524794220924 validation: auc: 0.6952639695472538 --- acc: 82.1 --- loss: 0.5602493286132812\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00265345424413681\n",
            "epoch: 197 train loss: 0.02709299325942993 validation: auc: 0.6949028126896074 --- acc: 82.19999999999999 --- loss: 0.5658891201019287\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002611059509217739\n",
            "epoch: 198 train loss: 0.028444020077586174 validation: auc: 0.6965894152148161 --- acc: 81.89999999999999 --- loss: 0.5727358460426331\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026368074119091033\n",
            "epoch: 199 train loss: 0.029188284650444984 validation: auc: 0.6982254557799543 --- acc: 81.89999999999999 --- loss: 0.5663766860961914\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002559236437082291\n",
            "epoch: 200 train loss: 0.03157345578074455 validation: auc: 0.6998903888937044 --- acc: 81.89999999999999 --- loss: 0.5586071610450745\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026057573035359383\n",
            "epoch: 201 train loss: 0.029086165130138397 validation: auc: 0.7019959333737829 --- acc: 82.05 --- loss: 0.5588648915290833\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026542220264673235\n",
            "epoch: 202 train loss: 0.02643449790775776 validation: auc: 0.7032491476698159 --- acc: 82.15 --- loss: 0.5613469481468201\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025713548064231874\n",
            "epoch: 203 train loss: 0.029149489477276802 validation: auc: 0.703973267169397 --- acc: 82.3 --- loss: 0.5607572197914124\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002636517956852913\n",
            "epoch: 204 train loss: 0.02621845155954361 validation: auc: 0.7040545274623675 --- acc: 82.3 --- loss: 0.5597428679466248\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025591909885406493\n",
            "epoch: 205 train loss: 0.029883675277233124 validation: auc: 0.701934536707983 --- acc: 81.75 --- loss: 0.561409056186676\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025740033015608788\n",
            "epoch: 206 train loss: 0.028660329058766365 validation: auc: 0.6997513435035105 --- acc: 81.75 --- loss: 0.5673527121543884\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026182932779192924\n",
            "epoch: 207 train loss: 0.027733510360121727 validation: auc: 0.6997116162491693 --- acc: 81.89999999999999 --- loss: 0.5699776411056519\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026046819984912872\n",
            "epoch: 208 train loss: 0.029114937409758568 validation: auc: 0.70106956603392 --- acc: 81.89999999999999 --- loss: 0.5669312477111816\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002583611197769642\n",
            "epoch: 209 train loss: 0.029126804322004318 validation: auc: 0.7025051645430643 --- acc: 82.05 --- loss: 0.5647489428520203\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026692090556025504\n",
            "epoch: 210 train loss: 0.02565315179526806 validation: auc: 0.7029638337522753 --- acc: 82.1 --- loss: 0.5639292597770691\n",
            "<Timer(Thread-8063, started 140340967237376)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002609958127140999\n",
            "epoch: 211 train loss: 0.028581008315086365 validation: auc: 0.7037890771719973 --- acc: 81.95 --- loss: 0.5614693760871887\n",
            "time up, break\n",
            "test auc: 0.6893832376578646 test acc: 82.89999999999999 test loss 0.5585106015205383\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.29107916355133057\n",
            "epoch: 0 train loss: 2.740952730178833 validation: auc: 0.48097310536479676 --- acc: 38.0 --- loss: 2.555079460144043\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2466064214706421\n",
            "epoch: 1 train loss: 2.147446870803833 validation: auc: 0.4840186519711742 --- acc: 47.05 --- loss: 1.8897308111190796\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.17997491359710693\n",
            "epoch: 2 train loss: 1.501986026763916 validation: auc: 0.493331449295841 --- acc: 61.0 --- loss: 1.2052539587020874\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1161385416984558\n",
            "epoch: 3 train loss: 0.9574285745620728 validation: auc: 0.5158438132918844 --- acc: 76.44999999999999 --- loss: 0.8804247975349426\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08808166980743408\n",
            "epoch: 4 train loss: 0.9043257236480713 validation: auc: 0.5614883896189533 --- acc: 82.55 --- loss: 0.8741535544395447\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09031575918197632\n",
            "epoch: 5 train loss: 0.9092754125595093 validation: auc: 0.60875323818944 --- acc: 83.0 --- loss: 0.8042194843292236\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08255554437637329\n",
            "epoch: 6 train loss: 0.8383274674415588 validation: auc: 0.6361754038905374 --- acc: 81.6 --- loss: 0.6751366257667542\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0697186529636383\n",
            "epoch: 7 train loss: 0.6847752332687378 validation: auc: 0.6476680325938486 --- acc: 77.0 --- loss: 0.6488828063011169\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06509342193603515\n",
            "epoch: 8 train loss: 0.6687765717506409 validation: auc: 0.6495351137487636 --- acc: 72.6 --- loss: 0.6662260890007019\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06445065140724182\n",
            "epoch: 9 train loss: 0.6031496524810791 validation: auc: 0.6403127502237294 --- acc: 73.75 --- loss: 0.627092182636261\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.057314610481262206\n",
            "epoch: 10 train loss: 0.5613224506378174 validation: auc: 0.6196335547077387 --- acc: 76.8 --- loss: 0.587875485420227\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.052877277135849\n",
            "epoch: 11 train loss: 0.5134382247924805 validation: auc: 0.5982516132071029 --- acc: 78.2 --- loss: 0.5744909644126892\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05141895413398743\n",
            "epoch: 12 train loss: 0.503880500793457 validation: auc: 0.5869228957656257 --- acc: 79.85 --- loss: 0.5654560327529907\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.050360506772994994\n",
            "epoch: 13 train loss: 0.4868796169757843 validation: auc: 0.5896924308793745 --- acc: 80.30000000000001 --- loss: 0.5455083250999451\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.047507447004318235\n",
            "epoch: 14 train loss: 0.46753719449043274 validation: auc: 0.6051189298667043 --- acc: 80.25 --- loss: 0.5177054405212402\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04432309865951538\n",
            "epoch: 15 train loss: 0.418565034866333 validation: auc: 0.6266271018793275 --- acc: 79.14999999999999 --- loss: 0.49793317914009094\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.040565261244773866\n",
            "epoch: 16 train loss: 0.4280773401260376 validation: auc: 0.6431388064622486 --- acc: 78.5 --- loss: 0.4920862913131714\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.039721012115478516\n",
            "epoch: 17 train loss: 0.38011276721954346 validation: auc: 0.6517375535773161 --- acc: 79.4 --- loss: 0.48278993368148804\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03725922107696533\n",
            "epoch: 18 train loss: 0.37801969051361084 validation: auc: 0.65539447034996 --- acc: 80.80000000000001 --- loss: 0.4704117476940155\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03539654612541199\n",
            "epoch: 19 train loss: 0.3631768524646759 validation: auc: 0.6555960623616409 --- acc: 81.55 --- loss: 0.4665815830230713\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03523296415805817\n",
            "epoch: 20 train loss: 0.3278540372848511 validation: auc: 0.6550327351514295 --- acc: 81.6 --- loss: 0.4668632745742798\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03350805342197418\n",
            "epoch: 21 train loss: 0.3579023778438568 validation: auc: 0.6570467712307475 --- acc: 81.15 --- loss: 0.46775344014167786\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.032555514574050905\n",
            "epoch: 22 train loss: 0.3491024374961853 validation: auc: 0.6606735434035137 --- acc: 80.25 --- loss: 0.4688708484172821\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03171903192996979\n",
            "epoch: 23 train loss: 0.3310718238353729 validation: auc: 0.6623333804342707 --- acc: 80.65 --- loss: 0.46427083015441895\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.030446010828018188\n",
            "epoch: 24 train loss: 0.308829128742218 validation: auc: 0.6623220762093166 --- acc: 81.3 --- loss: 0.4568720757961273\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.029351973533630372\n",
            "epoch: 25 train loss: 0.2889457941055298 validation: auc: 0.6625688851208139 --- acc: 82.15 --- loss: 0.4529293179512024\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.028563761711120607\n",
            "epoch: 26 train loss: 0.28586283326148987 validation: auc: 0.662813809994819 --- acc: 82.25 --- loss: 0.45091769099235535\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027646484971046447\n",
            "epoch: 27 train loss: 0.2863149642944336 validation: auc: 0.66442089397579 --- acc: 81.65 --- loss: 0.45127740502357483\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027236178517341614\n",
            "epoch: 28 train loss: 0.2702668309211731 validation: auc: 0.6650539305732185 --- acc: 81.55 --- loss: 0.4517122805118561\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.026344287395477294\n",
            "epoch: 29 train loss: 0.2687835693359375 validation: auc: 0.6636352503414817 --- acc: 82.19999999999999 --- loss: 0.4501914083957672\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02566796839237213\n",
            "epoch: 30 train loss: 0.25320613384246826 validation: auc: 0.6647430643869813 --- acc: 82.5 --- loss: 0.4498528838157654\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02496087849140167\n",
            "epoch: 31 train loss: 0.2531391680240631 validation: auc: 0.6695530121049409 --- acc: 82.19999999999999 --- loss: 0.44774770736694336\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.024242129921913148\n",
            "epoch: 32 train loss: 0.2524406313896179 validation: auc: 0.6736847063256559 --- acc: 81.5 --- loss: 0.44719934463500977\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.023831015825271605\n",
            "epoch: 33 train loss: 0.24802365899085999 validation: auc: 0.6757816400546371 --- acc: 81.25 --- loss: 0.445029616355896\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02347324788570404\n",
            "epoch: 34 train loss: 0.2341383993625641 validation: auc: 0.6755291790306627 --- acc: 82.05 --- loss: 0.4411380887031555\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.022662556171417235\n",
            "epoch: 35 train loss: 0.2378782480955124 validation: auc: 0.6747359992463849 --- acc: 82.6 --- loss: 0.43956834077835083\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021806882321834566\n",
            "epoch: 36 train loss: 0.2541230320930481 validation: auc: 0.6761358391031982 --- acc: 82.35 --- loss: 0.4384382665157318\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021986865997314455\n",
            "epoch: 37 train loss: 0.22794252634048462 validation: auc: 0.6771437991616034 --- acc: 82.25 --- loss: 0.43804216384887695\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021248477697372436\n",
            "epoch: 38 train loss: 0.23391465842723846 validation: auc: 0.6762714898026471 --- acc: 82.65 --- loss: 0.43724697828292847\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020917922258377075\n",
            "epoch: 39 train loss: 0.22155748307704926 validation: auc: 0.6768913381376289 --- acc: 82.39999999999999 --- loss: 0.4381338357925415\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02067771404981613\n",
            "epoch: 40 train loss: 0.21168115735054016 validation: auc: 0.6787132023927276 --- acc: 82.39999999999999 --- loss: 0.4387545585632324\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020249181985855104\n",
            "epoch: 41 train loss: 0.21842266619205475 validation: auc: 0.6783326268192738 --- acc: 82.19999999999999 --- loss: 0.4387132525444031\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019993700087070465\n",
            "epoch: 42 train loss: 0.21804757416248322 validation: auc: 0.6766746738260091 --- acc: 82.45 --- loss: 0.4375615119934082\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019872085750102998\n",
            "epoch: 43 train loss: 0.20942403376102448 validation: auc: 0.6753803400687673 --- acc: 82.69999999999999 --- loss: 0.43563517928123474\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019778329133987426\n",
            "epoch: 44 train loss: 0.20513063669204712 validation: auc: 0.6786849418303423 --- acc: 82.75 --- loss: 0.4323672652244568\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019509784877300262\n",
            "epoch: 45 train loss: 0.2075682133436203 validation: auc: 0.6818538928924687 --- acc: 82.6 --- loss: 0.43083488941192627\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019365493953227998\n",
            "epoch: 46 train loss: 0.20671093463897705 validation: auc: 0.6826696811266544 --- acc: 82.69999999999999 --- loss: 0.43095940351486206\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019260124862194063\n",
            "epoch: 47 train loss: 0.20043113827705383 validation: auc: 0.6793349347652018 --- acc: 82.65 --- loss: 0.43308791518211365\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019205480813980103\n",
            "epoch: 48 train loss: 0.19572784006595612 validation: auc: 0.6770514813244783 --- acc: 82.15 --- loss: 0.4351842701435089\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01861359030008316\n",
            "epoch: 49 train loss: 0.21192167699337006 validation: auc: 0.6795459469643446 --- acc: 82.0 --- loss: 0.4351853132247925\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01896635890007019\n",
            "epoch: 50 train loss: 0.18863631784915924 validation: auc: 0.6841524186331308 --- acc: 82.15 --- loss: 0.43166810274124146\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018563181161880493\n",
            "epoch: 51 train loss: 0.19501647353172302 validation: auc: 0.6858725448636429 --- acc: 82.95 --- loss: 0.4301793873310089\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01865130215883255\n",
            "epoch: 52 train loss: 0.18365947902202606 validation: auc: 0.684670528943526 --- acc: 83.0 --- loss: 0.43041983246803284\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018265913426876067\n",
            "epoch: 53 train loss: 0.19315575063228607 validation: auc: 0.6824209881776647 --- acc: 82.5 --- loss: 0.4320380389690399\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018176379799842834\n",
            "epoch: 54 train loss: 0.19534559547901154 validation: auc: 0.6804031840233621 --- acc: 82.6 --- loss: 0.4325394034385681\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018120637536048888\n",
            "epoch: 55 train loss: 0.18914707005023956 validation: auc: 0.6813696952569357 --- acc: 82.95 --- loss: 0.43199509382247925\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01802440285682678\n",
            "epoch: 56 train loss: 0.1883639544248581 validation: auc: 0.6834364843860393 --- acc: 82.95 --- loss: 0.4303930103778839\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017859001457691193\n",
            "epoch: 57 train loss: 0.1929609477519989 validation: auc: 0.6848250200178984 --- acc: 82.95 --- loss: 0.42973917722702026\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01787472516298294\n",
            "epoch: 58 train loss: 0.18982180953025818 validation: auc: 0.6834006876736847 --- acc: 82.55 --- loss: 0.43171149492263794\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017850090563297272\n",
            "epoch: 59 train loss: 0.18990057706832886 validation: auc: 0.6797795676133955 --- acc: 82.69999999999999 --- loss: 0.4341883063316345\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017681579291820525\n",
            "epoch: 60 train loss: 0.19092054665088654 validation: auc: 0.6776562573595215 --- acc: 82.95 --- loss: 0.43365606665611267\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017608921229839324\n",
            "epoch: 61 train loss: 0.183772012591362 validation: auc: 0.6794875418020818 --- acc: 82.89999999999999 --- loss: 0.43116095662117004\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017553693056106566\n",
            "epoch: 62 train loss: 0.17970579862594604 validation: auc: 0.6793179784277708 --- acc: 82.75 --- loss: 0.43051332235336304\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01737290769815445\n",
            "epoch: 63 train loss: 0.18268734216690063 validation: auc: 0.6781197305826386 --- acc: 83.15 --- loss: 0.4314616918563843\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01709803193807602\n",
            "epoch: 64 train loss: 0.1898050308227539 validation: auc: 0.681840704630022 --- acc: 82.65 --- loss: 0.429475873708725\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017345835268497468\n",
            "epoch: 65 train loss: 0.17830653488636017 validation: auc: 0.6838773491592482 --- acc: 82.69999999999999 --- loss: 0.42867058515548706\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017219465970993043\n",
            "epoch: 66 train loss: 0.18103566765785217 validation: auc: 0.6818162121426216 --- acc: 83.15 --- loss: 0.4307258725166321\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01708909571170807\n",
            "epoch: 67 train loss: 0.18371161818504333 validation: auc: 0.6819292543921625 --- acc: 83.25 --- loss: 0.431284099817276\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017147250473499298\n",
            "epoch: 68 train loss: 0.1774059236049652 validation: auc: 0.6844369082944749 --- acc: 83.1 --- loss: 0.4282744824886322\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017057377099990844\n",
            "epoch: 69 train loss: 0.17817692458629608 validation: auc: 0.6839206820215723 --- acc: 82.95 --- loss: 0.427584707736969\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01701101064682007\n",
            "epoch: 70 train loss: 0.18072664737701416 validation: auc: 0.6830219961377232 --- acc: 83.15 --- loss: 0.4268418848514557\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01711716651916504\n",
            "epoch: 71 train loss: 0.1756148338317871 validation: auc: 0.6870839809712214 --- acc: 83.3 --- loss: 0.424726665019989\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017117969691753387\n",
            "epoch: 72 train loss: 0.17572931945323944 validation: auc: 0.6929094248975554 --- acc: 83.6 --- loss: 0.421507328748703\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01711856424808502\n",
            "epoch: 73 train loss: 0.17185750603675842 validation: auc: 0.695405774574914 --- acc: 83.35000000000001 --- loss: 0.4203373193740845\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017106381058692933\n",
            "epoch: 74 train loss: 0.17007087171077728 validation: auc: 0.693184494371438 --- acc: 83.05 --- loss: 0.42287299036979675\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016963985562324525\n",
            "epoch: 75 train loss: 0.17288467288017273 validation: auc: 0.6888417879515802 --- acc: 83.25 --- loss: 0.4262726902961731\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01680746078491211\n",
            "epoch: 76 train loss: 0.17770133912563324 validation: auc: 0.6869878950591116 --- acc: 83.15 --- loss: 0.4264028072357178\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016838616132736205\n",
            "epoch: 77 train loss: 0.17388513684272766 validation: auc: 0.6880241156799021 --- acc: 82.89999999999999 --- loss: 0.42504629492759705\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016593864560127257\n",
            "epoch: 78 train loss: 0.17950350046157837 validation: auc: 0.6907936507936507 --- acc: 83.1 --- loss: 0.42320516705513\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016499316692352294\n",
            "epoch: 79 train loss: 0.17539595067501068 validation: auc: 0.6929508737223872 --- acc: 83.35000000000001 --- loss: 0.42196306586265564\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01642094850540161\n",
            "epoch: 80 train loss: 0.1743556559085846 validation: auc: 0.6924158070745607 --- acc: 83.0 --- loss: 0.4211697280406952\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016334988176822662\n",
            "epoch: 81 train loss: 0.17578129470348358 validation: auc: 0.6892412038999576 --- acc: 82.8 --- loss: 0.42254725098609924\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016435697674751282\n",
            "epoch: 82 train loss: 0.1728236973285675 validation: auc: 0.6833667749988225 --- acc: 83.35000000000001 --- loss: 0.4261258542537689\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0165824294090271\n",
            "epoch: 83 train loss: 0.1705702841281891 validation: auc: 0.6828392445009656 --- acc: 83.6 --- loss: 0.4271404445171356\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016766437888145448\n",
            "epoch: 84 train loss: 0.16475138068199158 validation: auc: 0.6901511940087609 --- acc: 83.25 --- loss: 0.4227702021598816\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016259384155273438\n",
            "epoch: 85 train loss: 0.17965717613697052 validation: auc: 0.6948914323395037 --- acc: 83.15 --- loss: 0.41989368200302124\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016312211751937866\n",
            "epoch: 86 train loss: 0.17704500257968903 validation: auc: 0.6937572417691112 --- acc: 83.3 --- loss: 0.4199851453304291\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016395771503448488\n",
            "epoch: 87 train loss: 0.17194807529449463 validation: auc: 0.6908520559559136 --- acc: 83.5 --- loss: 0.4208616316318512\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01627240180969238\n",
            "epoch: 88 train loss: 0.17561405897140503 validation: auc: 0.6915679902030051 --- acc: 83.5 --- loss: 0.41924768686294556\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016433244943618773\n",
            "epoch: 89 train loss: 0.16604705154895782 validation: auc: 0.6960651876972352 --- acc: 83.7 --- loss: 0.41748058795928955\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016158847510814665\n",
            "epoch: 90 train loss: 0.17441648244857788 validation: auc: 0.6955301210494089 --- acc: 83.3 --- loss: 0.4195966422557831\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016050250828266145\n",
            "epoch: 91 train loss: 0.17473328113555908 validation: auc: 0.6905788705195234 --- acc: 83.0 --- loss: 0.42415544390678406\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01621340811252594\n",
            "epoch: 92 train loss: 0.16786044836044312 validation: auc: 0.6863209457868212 --- acc: 83.15 --- loss: 0.4258171021938324\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016239792108535767\n",
            "epoch: 93 train loss: 0.16674639284610748 validation: auc: 0.687197023220762 --- acc: 83.39999999999999 --- loss: 0.4232679307460785\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016281966865062714\n",
            "epoch: 94 train loss: 0.16444306075572968 validation: auc: 0.6915105270594886 --- acc: 83.15 --- loss: 0.42014753818511963\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01610785275697708\n",
            "epoch: 95 train loss: 0.17366856336593628 validation: auc: 0.6952625877254957 --- acc: 83.3 --- loss: 0.41797515749931335\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01631661206483841\n",
            "epoch: 96 train loss: 0.16175197064876556 validation: auc: 0.694898968489473 --- acc: 83.3 --- loss: 0.4190869629383087\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016318762302398683\n",
            "epoch: 97 train loss: 0.162861630320549 validation: auc: 0.6939927464556545 --- acc: 83.2 --- loss: 0.421505868434906\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016261830925941467\n",
            "epoch: 98 train loss: 0.16844013333320618 validation: auc: 0.6942075267297819 --- acc: 82.95 --- loss: 0.4221195578575134\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016235339641571044\n",
            "epoch: 99 train loss: 0.17000870406627655 validation: auc: 0.6937195610192644 --- acc: 83.1 --- loss: 0.4215320646762848\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0162238210439682\n",
            "epoch: 100 train loss: 0.16697034239768982 validation: auc: 0.6945410013659271 --- acc: 82.89999999999999 --- loss: 0.4202100932598114\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01613519638776779\n",
            "epoch: 101 train loss: 0.16584137082099915 validation: auc: 0.6940417314304554 --- acc: 83.15 --- loss: 0.420388400554657\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016046318411827087\n",
            "epoch: 102 train loss: 0.16486848890781403 validation: auc: 0.6916188592152983 --- acc: 83.25 --- loss: 0.42198657989501953\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015991511940956115\n",
            "epoch: 103 train loss: 0.16425655782222748 validation: auc: 0.6904752484574443 --- acc: 82.75 --- loss: 0.4227824807167053\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01578158438205719\n",
            "epoch: 104 train loss: 0.1690071076154709 validation: auc: 0.6901926428335924 --- acc: 83.0 --- loss: 0.4216463565826416\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01584196388721466\n",
            "epoch: 105 train loss: 0.1653112918138504 validation: auc: 0.6911723423296123 --- acc: 83.2 --- loss: 0.41984811425209045\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015846189856529237\n",
            "epoch: 106 train loss: 0.16466298699378967 validation: auc: 0.6943544816541849 --- acc: 83.45 --- loss: 0.41804635524749756\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015852980315685272\n",
            "epoch: 107 train loss: 0.16677992045879364 validation: auc: 0.6972483632424284 --- acc: 83.35000000000001 --- loss: 0.41634032130241394\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01585760563611984\n",
            "epoch: 108 train loss: 0.16769470274448395 validation: auc: 0.6989345767980782 --- acc: 83.3 --- loss: 0.41538169980049133\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015808649361133575\n",
            "epoch: 109 train loss: 0.1662154346704483 validation: auc: 0.6947652018275163 --- acc: 82.95 --- loss: 0.4185604453086853\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0156643345952034\n",
            "epoch: 110 train loss: 0.16580821573734283 validation: auc: 0.6885554142527437 --- acc: 82.95 --- loss: 0.4222713112831116\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015800699591636658\n",
            "epoch: 111 train loss: 0.15902800858020782 validation: auc: 0.6873722387075502 --- acc: 83.2 --- loss: 0.42272546887397766\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015655484795570374\n",
            "epoch: 112 train loss: 0.17001622915267944 validation: auc: 0.6900193113842965 --- acc: 83.25 --- loss: 0.42134350538253784\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015750102698802948\n",
            "epoch: 113 train loss: 0.16771860420703888 validation: auc: 0.6931920305214073 --- acc: 83.15 --- loss: 0.4200234115123749\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015603500604629516\n",
            "epoch: 114 train loss: 0.17408621311187744 validation: auc: 0.6936799962319249 --- acc: 83.25 --- loss: 0.4199630916118622\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01586243510246277\n",
            "epoch: 115 train loss: 0.1647178828716278 validation: auc: 0.6941133248551646 --- acc: 83.3 --- loss: 0.418708473443985\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015887871384620667\n",
            "epoch: 116 train loss: 0.16118261218070984 validation: auc: 0.6958051905232915 --- acc: 83.3 --- loss: 0.4172073006629944\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01572102904319763\n",
            "epoch: 117 train loss: 0.16422849893569946 validation: auc: 0.6974160425792474 --- acc: 83.35000000000001 --- loss: 0.41605547070503235\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015684831142425536\n",
            "epoch: 118 train loss: 0.16396592557430267 validation: auc: 0.6982148744760021 --- acc: 83.25 --- loss: 0.4154720604419708\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01568414717912674\n",
            "epoch: 119 train loss: 0.16228464245796204 validation: auc: 0.697022278743347 --- acc: 83.1 --- loss: 0.4164334535598755\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015532767772674561\n",
            "epoch: 120 train loss: 0.16701771318912506 validation: auc: 0.694397814516509 --- acc: 83.25 --- loss: 0.4184034764766693\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015429572761058807\n",
            "epoch: 121 train loss: 0.1696382761001587 validation: auc: 0.6933276812208563 --- acc: 83.25 --- loss: 0.41994553804397583\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01546657234430313\n",
            "epoch: 122 train loss: 0.16894198954105377 validation: auc: 0.6924026188121143 --- acc: 83.2 --- loss: 0.4206022322177887\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015546707808971405\n",
            "epoch: 123 train loss: 0.16311731934547424 validation: auc: 0.6883801987659555 --- acc: 83.2 --- loss: 0.4239748418331146\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015607453882694244\n",
            "epoch: 124 train loss: 0.15878885984420776 validation: auc: 0.6851471904290896 --- acc: 83.15 --- loss: 0.42772284150123596\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01572575718164444\n",
            "epoch: 125 train loss: 0.1596292406320572 validation: auc: 0.6898817766473553 --- acc: 83.2 --- loss: 0.424956738948822\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015602515637874603\n",
            "epoch: 126 train loss: 0.16646508872509003 validation: auc: 0.6966605435448165 --- acc: 83.05 --- loss: 0.418515145778656\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015738260746002198\n",
            "epoch: 127 train loss: 0.16321884095668793 validation: auc: 0.6968564834440205 --- acc: 83.0 --- loss: 0.4159472584724426\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015822292864322664\n",
            "epoch: 128 train loss: 0.15639890730381012 validation: auc: 0.6914615420846876 --- acc: 83.3 --- loss: 0.41855213046073914\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01560947746038437\n",
            "epoch: 129 train loss: 0.1636129766702652 validation: auc: 0.6884800527530499 --- acc: 83.55 --- loss: 0.41938164830207825\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015698073804378508\n",
            "epoch: 130 train loss: 0.15906083583831787 validation: auc: 0.6897178653855212 --- acc: 83.6 --- loss: 0.41611194610595703\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01558634638786316\n",
            "epoch: 131 train loss: 0.16085191071033478 validation: auc: 0.6921011728133389 --- acc: 83.55 --- loss: 0.4158358573913574\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015393756330013275\n",
            "epoch: 132 train loss: 0.16723082959651947 validation: auc: 0.693647967594555 --- acc: 83.39999999999999 --- loss: 0.4185435473918915\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015271270275115966\n",
            "epoch: 133 train loss: 0.16825711727142334 validation: auc: 0.6945410013659271 --- acc: 83.1 --- loss: 0.41940048336982727\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015415878593921661\n",
            "epoch: 134 train loss: 0.15951913595199585 validation: auc: 0.6959088125853705 --- acc: 82.75 --- loss: 0.4180454909801483\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015418903529644012\n",
            "epoch: 135 train loss: 0.16550104320049286 validation: auc: 0.6963817059959493 --- acc: 82.75 --- loss: 0.4177358150482178\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015395376086235046\n",
            "epoch: 136 train loss: 0.16651326417922974 validation: auc: 0.6959992463850031 --- acc: 83.25 --- loss: 0.4183545708656311\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015372101962566376\n",
            "epoch: 137 train loss: 0.16270124912261963 validation: auc: 0.6943921624040317 --- acc: 83.35000000000001 --- loss: 0.41766926646232605\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0154205322265625\n",
            "epoch: 138 train loss: 0.1598542332649231 validation: auc: 0.6925382695115633 --- acc: 83.1 --- loss: 0.4173200726509094\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015609379112720489\n",
            "epoch: 139 train loss: 0.15364831686019897 validation: auc: 0.6918628420705571 --- acc: 83.0 --- loss: 0.4189921021461487\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015473520755767823\n",
            "epoch: 140 train loss: 0.15995053946971893 validation: auc: 0.6948141868023174 --- acc: 83.3 --- loss: 0.42026659846305847\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015659452974796297\n",
            "epoch: 141 train loss: 0.15376314520835876 validation: auc: 0.6977269087654844 --- acc: 83.3 --- loss: 0.41892310976982117\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015406017005443574\n",
            "epoch: 142 train loss: 0.16423237323760986 validation: auc: 0.6989835617728792 --- acc: 83.35000000000001 --- loss: 0.4171452522277832\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015673358738422394\n",
            "epoch: 143 train loss: 0.1563805639743805 validation: auc: 0.6997993500070652 --- acc: 83.35000000000001 --- loss: 0.4158087968826294\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015596523880958557\n",
            "epoch: 144 train loss: 0.16104251146316528 validation: auc: 0.6994639913334275 --- acc: 83.3 --- loss: 0.41608354449272156\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01560242623090744\n",
            "epoch: 145 train loss: 0.16076987981796265 validation: auc: 0.6994150063586265 --- acc: 83.2 --- loss: 0.41687750816345215\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01548948884010315\n",
            "epoch: 146 train loss: 0.16432419419288635 validation: auc: 0.6979416890396118 --- acc: 83.15 --- loss: 0.41801267862319946\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015424764156341553\n",
            "epoch: 147 train loss: 0.16380904614925385 validation: auc: 0.6943601337666621 --- acc: 83.05 --- loss: 0.42050763964653015\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015584029257297516\n",
            "epoch: 148 train loss: 0.15420445799827576 validation: auc: 0.6897140973105365 --- acc: 83.15 --- loss: 0.4228891134262085\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015562090277671813\n",
            "epoch: 149 train loss: 0.15423265099525452 validation: auc: 0.691775234327163 --- acc: 83.1 --- loss: 0.4201706647872925\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015324048697948456\n",
            "epoch: 150 train loss: 0.1629294455051422 validation: auc: 0.6976006782534973 --- acc: 83.25 --- loss: 0.4162263870239258\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01552504301071167\n",
            "epoch: 151 train loss: 0.1564842164516449 validation: auc: 0.6975074183976262 --- acc: 83.2 --- loss: 0.4172693192958832\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015395745635032654\n",
            "epoch: 152 train loss: 0.16181007027626038 validation: auc: 0.695786350148368 --- acc: 83.35000000000001 --- loss: 0.4190884530544281\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01543390154838562\n",
            "epoch: 153 train loss: 0.15904347598552704 validation: auc: 0.6933276812208563 --- acc: 83.39999999999999 --- loss: 0.4197721481323242\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015360182523727417\n",
            "epoch: 154 train loss: 0.16089023649692535 validation: auc: 0.6948838961895342 --- acc: 83.55 --- loss: 0.4186416268348694\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01550162136554718\n",
            "epoch: 155 train loss: 0.15477293729782104 validation: auc: 0.6970674956431632 --- acc: 83.55 --- loss: 0.4175167679786682\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015338940918445588\n",
            "epoch: 156 train loss: 0.16387450695037842 validation: auc: 0.6961537374593755 --- acc: 83.5 --- loss: 0.4175131022930145\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015438759326934814\n",
            "epoch: 157 train loss: 0.16100814938545227 validation: auc: 0.6924459516744383 --- acc: 83.45 --- loss: 0.4169749617576599\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01526501476764679\n",
            "epoch: 158 train loss: 0.16803310811519623 validation: auc: 0.6890716405256466 --- acc: 83.39999999999999 --- loss: 0.41745686531066895\n",
            "<Timer(Thread-8489, started 140343165622016)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015466335415840148\n",
            "epoch: 159 train loss: 0.15860021114349365 validation: auc: 0.6919184211765814 --- acc: 83.6 --- loss: 0.41657689213752747\n",
            "time up, break\n",
            "test auc: 0.6984357491820179 test acc: 83.15 test loss 0.4312090277671814\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.19273339509963988\n",
            "epoch: 0 train loss: 1.7749000787734985 validation: auc: 0.4924934248977207 --- acc: 49.6 --- loss: 1.629340410232544\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1656430959701538\n",
            "epoch: 1 train loss: 1.4846274852752686 validation: auc: 0.49764305196070246 --- acc: 58.699999999999996 --- loss: 1.2958097457885742\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1320141911506653\n",
            "epoch: 2 train loss: 1.1672871112823486 validation: auc: 0.5106593192507862 --- acc: 68.2 --- loss: 1.020902395248413\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1029901385307312\n",
            "epoch: 3 train loss: 0.9765447378158569 validation: auc: 0.5359595474659765 --- acc: 74.2 --- loss: 0.8655728101730347\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08736781477928161\n",
            "epoch: 4 train loss: 0.8423037528991699 validation: auc: 0.568248865881829 --- acc: 77.9 --- loss: 0.7826301455497742\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07921461462974548\n",
            "epoch: 5 train loss: 0.7462104558944702 validation: auc: 0.594690170048148 --- acc: 78.10000000000001 --- loss: 0.711323082447052\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07215020656585694\n",
            "epoch: 6 train loss: 0.6972823143005371 validation: auc: 0.6096076507750968 --- acc: 76.9 --- loss: 0.6661116480827332\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06738783121109009\n",
            "epoch: 7 train loss: 0.7116531133651733 validation: auc: 0.6156783139907045 --- acc: 75.35 --- loss: 0.654102087020874\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06706970334053039\n",
            "epoch: 8 train loss: 0.680428147315979 validation: auc: 0.6140867217722857 --- acc: 75.7 --- loss: 0.6371316909790039\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.065704607963562\n",
            "epoch: 9 train loss: 0.6391727328300476 validation: auc: 0.6077064373382316 --- acc: 76.55 --- loss: 0.6258140802383423\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06379850506782532\n",
            "epoch: 10 train loss: 0.6449302434921265 validation: auc: 0.60150879463416 --- acc: 77.3 --- loss: 0.6244701743125916\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06328613162040711\n",
            "epoch: 11 train loss: 0.6432449817657471 validation: auc: 0.5983256101973227 --- acc: 77.2 --- loss: 0.620438277721405\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0627278983592987\n",
            "epoch: 12 train loss: 0.6380132436752319 validation: auc: 0.5978490022543208 --- acc: 76.8 --- loss: 0.6137092113494873\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06312620639801025\n",
            "epoch: 13 train loss: 0.5872014164924622 validation: auc: 0.5993014388689433 --- acc: 76.55 --- loss: 0.6071720123291016\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06162137389183044\n",
            "epoch: 14 train loss: 0.6130794286727905 validation: auc: 0.6013505051348418 --- acc: 76.6 --- loss: 0.6009336113929749\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06113041043281555\n",
            "epoch: 15 train loss: 0.5987711548805237 validation: auc: 0.6024846233057806 --- acc: 77.3 --- loss: 0.595207154750824\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.060519236326217654\n",
            "epoch: 16 train loss: 0.5903337597846985 validation: auc: 0.6024324399543569 --- acc: 77.64999999999999 --- loss: 0.5912567973136902\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.060224539041519164\n",
            "epoch: 17 train loss: 0.5779889225959778 validation: auc: 0.6014931396287329 --- acc: 77.8 --- loss: 0.5879170894622803\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05964233279228211\n",
            "epoch: 18 train loss: 0.5791438817977905 validation: auc: 0.6008565027413655 --- acc: 77.55 --- loss: 0.5837347507476807\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05898465514183045\n",
            "epoch: 19 train loss: 0.5822481513023376 validation: auc: 0.6003381481172247 --- acc: 77.10000000000001 --- loss: 0.5798614621162415\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05820139050483704\n",
            "epoch: 20 train loss: 0.5949102640151978 validation: auc: 0.6002650914252318 --- acc: 77.0 --- loss: 0.5759134292602539\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05854009985923767\n",
            "epoch: 21 train loss: 0.5608237981796265 validation: auc: 0.601426170994406 --- acc: 77.25 --- loss: 0.5707285404205322\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05755179524421692\n",
            "epoch: 22 train loss: 0.5724120736122131 validation: auc: 0.6028499067657453 --- acc: 77.5 --- loss: 0.5659002661705017\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05753656029701233\n",
            "epoch: 23 train loss: 0.5452739000320435 validation: auc: 0.6038761793437422 --- acc: 78.05 --- loss: 0.5623165965080261\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05636698603630066\n",
            "epoch: 24 train loss: 0.5694195032119751 validation: auc: 0.6050450864156298 --- acc: 78.2 --- loss: 0.5588229298591614\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05617634057998657\n",
            "epoch: 25 train loss: 0.5548723340034485 validation: auc: 0.606641896969191 --- acc: 78.2 --- loss: 0.5544959902763367\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05552236437797546\n",
            "epoch: 26 train loss: 0.5591615438461304 validation: auc: 0.6073324566530294 --- acc: 78.0 --- loss: 0.5509272217750549\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05463208556175232\n",
            "epoch: 27 train loss: 0.5771386027336121 validation: auc: 0.6063200996354122 --- acc: 77.9 --- loss: 0.5482465624809265\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05528899431228638\n",
            "epoch: 28 train loss: 0.5358832478523254 validation: auc: 0.6047963457738443 --- acc: 77.95 --- loss: 0.5458353161811829\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05479599237442016\n",
            "epoch: 29 train loss: 0.5388039946556091 validation: auc: 0.6045771756978654 --- acc: 78.25 --- loss: 0.5430175065994263\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05424595475196838\n",
            "epoch: 30 train loss: 0.5418816804885864 validation: auc: 0.6053964543152153 --- acc: 78.55 --- loss: 0.5397081971168518\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05397396683692932\n",
            "epoch: 31 train loss: 0.5347884893417358 validation: auc: 0.6060635314909132 --- acc: 78.35 --- loss: 0.5363556146621704\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.052912020683288576\n",
            "epoch: 32 train loss: 0.5605957508087158 validation: auc: 0.607184603823996 --- acc: 78.35 --- loss: 0.5327557325363159\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05359126925468445\n",
            "epoch: 33 train loss: 0.5173497200012207 validation: auc: 0.608784893267652 --- acc: 78.3 --- loss: 0.5291687250137329\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.053317022323608396\n",
            "epoch: 34 train loss: 0.5098786950111389 validation: auc: 0.6097111477554201 --- acc: 78.45 --- loss: 0.5258911848068237\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05312687158584595\n",
            "epoch: 35 train loss: 0.4976062476634979 validation: auc: 0.6095919957696696 --- acc: 78.75 --- loss: 0.5235861539840698\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.051845383644104\n",
            "epoch: 36 train loss: 0.5335400700569153 validation: auc: 0.6092788956611283 --- acc: 79.25 --- loss: 0.5215109586715698\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05182953476905823\n",
            "epoch: 37 train loss: 0.5211012363433838 validation: auc: 0.6085831176421475 --- acc: 79.2 --- loss: 0.5189169049263\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05199592709541321\n",
            "epoch: 38 train loss: 0.502485454082489 validation: auc: 0.6082317497425621 --- acc: 78.75 --- loss: 0.5166380405426025\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.051898062229156494\n",
            "epoch: 39 train loss: 0.4943688213825226 validation: auc: 0.6093362973476942 --- acc: 79.0 --- loss: 0.5141234397888184\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05168770551681519\n",
            "epoch: 40 train loss: 0.48774436116218567 validation: auc: 0.6110305168239125 --- acc: 79.25 --- loss: 0.5116010904312134\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05078610181808472\n",
            "epoch: 41 train loss: 0.5082054138183594 validation: auc: 0.6127786590966018 --- acc: 79.65 --- loss: 0.5092655420303345\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05057675242424011\n",
            "epoch: 42 train loss: 0.5037063360214233 validation: auc: 0.6131578581169463 --- acc: 79.7 --- loss: 0.5070494413375854\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05018451809883118\n",
            "epoch: 43 train loss: 0.5085116028785706 validation: auc: 0.612425551751969 --- acc: 79.5 --- loss: 0.5051196813583374\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05028004646301269\n",
            "epoch: 44 train loss: 0.49684062600135803 validation: auc: 0.6118271826556456 --- acc: 79.45 --- loss: 0.5036055445671082\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05109921097755432\n",
            "epoch: 45 train loss: 0.45748329162597656 validation: auc: 0.6119924299351535 --- acc: 79.45 --- loss: 0.5015631914138794\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04989535510540009\n",
            "epoch: 46 train loss: 0.493623286485672 validation: auc: 0.6128778074643066 --- acc: 80.2 --- loss: 0.4994601905345917\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04965481460094452\n",
            "epoch: 47 train loss: 0.4896297752857208 validation: auc: 0.61393365060811 --- acc: 80.35 --- loss: 0.4975332021713257\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.049589133262634276\n",
            "epoch: 48 train loss: 0.48033368587493896 validation: auc: 0.6147755420110769 --- acc: 80.35 --- loss: 0.4954797327518463\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04892655313014984\n",
            "epoch: 49 train loss: 0.4958190619945526 validation: auc: 0.6153286855361666 --- acc: 80.45 --- loss: 0.4933786690235138\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0491135448217392\n",
            "epoch: 50 train loss: 0.4789159893989563 validation: auc: 0.616311471987977 --- acc: 80.35 --- loss: 0.4915024936199188\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04896880984306336\n",
            "epoch: 51 train loss: 0.4755867123603821 validation: auc: 0.6169481088753443 --- acc: 80.60000000000001 --- loss: 0.48962023854255676\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04844075739383698\n",
            "epoch: 52 train loss: 0.4850923418998718 validation: auc: 0.6174021040327293 --- acc: 80.35 --- loss: 0.48800650238990784\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04820696115493774\n",
            "epoch: 53 train loss: 0.4838031828403473 validation: auc: 0.617574309092427 --- acc: 80.45 --- loss: 0.4867445230484009\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04828728437423706\n",
            "epoch: 54 train loss: 0.47167590260505676 validation: auc: 0.6182666082213131 --- acc: 80.45 --- loss: 0.48517000675201416\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04764371514320374\n",
            "epoch: 55 train loss: 0.48803645372390747 validation: auc: 0.6193850713868246 --- acc: 80.65 --- loss: 0.4833710491657257\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04799732863903046\n",
            "epoch: 56 train loss: 0.4653584361076355 validation: auc: 0.6204530906459602 --- acc: 80.7 --- loss: 0.48165932297706604\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04760669767856598\n",
            "epoch: 57 train loss: 0.47264155745506287 validation: auc: 0.6215941665970889 --- acc: 80.65 --- loss: 0.48002973198890686\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04772566854953766\n",
            "epoch: 58 train loss: 0.4588675796985626 validation: auc: 0.6226517491859397 --- acc: 80.75 --- loss: 0.4784293472766876\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04718063473701477\n",
            "epoch: 59 train loss: 0.47193434834480286 validation: auc: 0.6229352787286744 --- acc: 80.85 --- loss: 0.47699064016342163\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.047094157338142394\n",
            "epoch: 60 train loss: 0.46762359142303467 validation: auc: 0.6234466755726252 --- acc: 80.7 --- loss: 0.47564032673835754\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04666483998298645\n",
            "epoch: 61 train loss: 0.47718167304992676 validation: auc: 0.6241598480420807 --- acc: 80.80000000000001 --- loss: 0.4743531048297882\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04681362211704254\n",
            "epoch: 62 train loss: 0.463654488325119 validation: auc: 0.6248243160502074 --- acc: 80.85 --- loss: 0.4730997085571289\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04662086367607117\n",
            "epoch: 63 train loss: 0.46412861347198486 validation: auc: 0.6256435946675573 --- acc: 80.85 --- loss: 0.47181272506713867\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04655136168003082\n",
            "epoch: 64 train loss: 0.459966778755188 validation: auc: 0.6271516935236982 --- acc: 80.9 --- loss: 0.4704437255859375\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04630151391029358\n",
            "epoch: 65 train loss: 0.46243706345558167 validation: auc: 0.6280996910745595 --- acc: 81.05 --- loss: 0.46915313601493835\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04627629816532135\n",
            "epoch: 66 train loss: 0.45607781410217285 validation: auc: 0.6278231193120147 --- acc: 81.15 --- loss: 0.4683052599430084\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04610612392425537\n",
            "epoch: 67 train loss: 0.45736178755760193 validation: auc: 0.6276404775820322 --- acc: 81.05 --- loss: 0.46761661767959595\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04560297131538391\n",
            "epoch: 68 train loss: 0.4724046289920807 validation: auc: 0.6281501349809356 --- acc: 81.0 --- loss: 0.46657228469848633\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04566934108734131\n",
            "epoch: 69 train loss: 0.46450915932655334 validation: auc: 0.628955498037906 --- acc: 81.2 --- loss: 0.465434730052948\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04619961678981781\n",
            "epoch: 70 train loss: 0.4386201500892639 validation: auc: 0.6303070468397762 --- acc: 81.25 --- loss: 0.46427229046821594\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04554645419120788\n",
            "epoch: 71 train loss: 0.45789989829063416 validation: auc: 0.6319421251843812 --- acc: 81.3 --- loss: 0.4631000757217407\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.045207083225250244\n",
            "epoch: 72 train loss: 0.4634784460067749 validation: auc: 0.6332049622888314 --- acc: 81.10000000000001 --- loss: 0.46206656098365784\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04577758014202118\n",
            "epoch: 73 train loss: 0.4341185688972473 validation: auc: 0.6341581781748351 --- acc: 81.15 --- loss: 0.4612126350402832\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.045519351959228516\n",
            "epoch: 74 train loss: 0.438591867685318 validation: auc: 0.6353044724611061 --- acc: 81.10000000000001 --- loss: 0.46037131547927856\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04506099820137024\n",
            "epoch: 75 train loss: 0.451698899269104 validation: auc: 0.6361098355180763 --- acc: 81.3 --- loss: 0.4594971537590027\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0446290522813797\n",
            "epoch: 76 train loss: 0.46468254923820496 validation: auc: 0.6365099078789903 --- acc: 81.39999999999999 --- loss: 0.4587039053440094\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.045387017726898196\n",
            "epoch: 77 train loss: 0.43148308992385864 validation: auc: 0.636409020066238 --- acc: 81.3 --- loss: 0.4581124186515808\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044998711347579955\n",
            "epoch: 78 train loss: 0.4439796507358551 validation: auc: 0.6368908463443823 --- acc: 81.35 --- loss: 0.4574427604675293\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04459145367145538\n",
            "epoch: 79 train loss: 0.45612794160842896 validation: auc: 0.6380858450919819 --- acc: 81.3 --- loss: 0.4565509855747223\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04473126530647278\n",
            "epoch: 80 train loss: 0.44531768560409546 validation: auc: 0.6392738860593916 --- acc: 81.25 --- loss: 0.45565930008888245\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04462586939334869\n",
            "epoch: 81 train loss: 0.44430994987487793 validation: auc: 0.6399192201719964 --- acc: 81.25 --- loss: 0.45487865805625916\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04427086114883423\n",
            "epoch: 82 train loss: 0.45411890745162964 validation: auc: 0.6409872394311319 --- acc: 81.2 --- loss: 0.4539676010608673\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04417179226875305\n",
            "epoch: 83 train loss: 0.4538835883140564 validation: auc: 0.6424588099412764 --- acc: 81.25 --- loss: 0.45297619700431824\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04429694414138794\n",
            "epoch: 84 train loss: 0.4448506236076355 validation: auc: 0.643700773705157 --- acc: 81.10000000000001 --- loss: 0.452143132686615\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04454798102378845\n",
            "epoch: 85 train loss: 0.4314117431640625 validation: auc: 0.6442260861094876 --- acc: 81.2 --- loss: 0.4514683783054352\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043860587477684024\n",
            "epoch: 86 train loss: 0.4549792408943176 validation: auc: 0.6446800812668727 --- acc: 81.25 --- loss: 0.45090532302856445\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04457968175411224\n",
            "epoch: 87 train loss: 0.42175063490867615 validation: auc: 0.6450592802872172 --- acc: 81.35 --- loss: 0.4504360854625702\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04467038810253143\n",
            "epoch: 88 train loss: 0.4143928289413452 validation: auc: 0.6453567253903314 --- acc: 81.35 --- loss: 0.4500923454761505\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043936213850975035\n",
            "epoch: 89 train loss: 0.4406430125236511 validation: auc: 0.6457881077620996 --- acc: 81.3 --- loss: 0.4496941864490509\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04389082789421082\n",
            "epoch: 90 train loss: 0.4398256242275238 validation: auc: 0.6459881439425565 --- acc: 81.3 --- loss: 0.449263334274292\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044277501106262204\n",
            "epoch: 91 train loss: 0.42273303866386414 validation: auc: 0.6462403634744371 --- acc: 81.39999999999999 --- loss: 0.44892752170562744\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043152397871017455\n",
            "epoch: 92 train loss: 0.4659165143966675 validation: auc: 0.6465569424730734 --- acc: 81.39999999999999 --- loss: 0.44849756360054016\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04377738237380981\n",
            "epoch: 93 train loss: 0.4389965534210205 validation: auc: 0.6475414683699313 --- acc: 81.35 --- loss: 0.44794797897338867\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043651926517486575\n",
            "epoch: 94 train loss: 0.4418250024318695 validation: auc: 0.6485329520469789 --- acc: 81.3 --- loss: 0.44741326570510864\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04344407618045807\n",
            "epoch: 95 train loss: 0.4474799633026123 validation: auc: 0.649348751774234 --- acc: 81.39999999999999 --- loss: 0.44693246483802795\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043698403239250186\n",
            "epoch: 96 train loss: 0.4345477819442749 validation: auc: 0.6501541148312043 --- acc: 81.6 --- loss: 0.44648951292037964\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04398426711559296\n",
            "epoch: 97 train loss: 0.4201470911502838 validation: auc: 0.6511195068325402 --- acc: 81.69999999999999 --- loss: 0.44601696729660034\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043514755368232724\n",
            "epoch: 98 train loss: 0.43600234389305115 validation: auc: 0.6523145055801397 --- acc: 81.75 --- loss: 0.44547009468078613\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04402264058589935\n",
            "epoch: 99 train loss: 0.4127301871776581 validation: auc: 0.65348167320698 --- acc: 81.8 --- loss: 0.44490551948547363\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043620312213897706\n",
            "epoch: 100 train loss: 0.42564526200294495 validation: auc: 0.6545531713562105 --- acc: 81.8 --- loss: 0.4444211721420288\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04347982108592987\n",
            "epoch: 101 train loss: 0.4285230338573456 validation: auc: 0.6555429155882108 --- acc: 81.75 --- loss: 0.44394585490226746\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04310286343097687\n",
            "epoch: 102 train loss: 0.4412970244884491 validation: auc: 0.6565691881662075 --- acc: 81.6 --- loss: 0.4434306025505066\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043380099534988406\n",
            "epoch: 103 train loss: 0.4284023642539978 validation: auc: 0.657306712866327 --- acc: 81.6 --- loss: 0.4430240988731384\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04310986399650574\n",
            "epoch: 104 train loss: 0.437374085187912 validation: auc: 0.6578233280454203 --- acc: 81.69999999999999 --- loss: 0.44272714853286743\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04323415756225586\n",
            "epoch: 105 train loss: 0.43036311864852905 validation: auc: 0.6581486042692939 --- acc: 81.85 --- loss: 0.4425176680088043\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042895743250846864\n",
            "epoch: 106 train loss: 0.442124605178833 validation: auc: 0.6580503256241129 --- acc: 81.89999999999999 --- loss: 0.44239845871925354\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04311802387237549\n",
            "epoch: 107 train loss: 0.43214285373687744 validation: auc: 0.6581346887089143 --- acc: 81.89999999999999 --- loss: 0.44220975041389465\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04282265901565552\n",
            "epoch: 108 train loss: 0.44318339228630066 validation: auc: 0.6585747683059197 --- acc: 81.95 --- loss: 0.4419041574001312\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04309627413749695\n",
            "epoch: 109 train loss: 0.43115484714508057 validation: auc: 0.6597662881634242 --- acc: 81.95 --- loss: 0.4414927363395691\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043059620261192325\n",
            "epoch: 110 train loss: 0.4310372471809387 validation: auc: 0.6607751662909466 --- acc: 81.95 --- loss: 0.4410955011844635\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.043008309602737424\n",
            "epoch: 111 train loss: 0.4311498999595642 validation: auc: 0.6618588405555093 --- acc: 81.95 --- loss: 0.44070181250572205\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04312342405319214\n",
            "epoch: 112 train loss: 0.4238395690917969 validation: auc: 0.663316495505274 --- acc: 81.95 --- loss: 0.440303236246109\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04303768575191498\n",
            "epoch: 113 train loss: 0.4242715537548065 validation: auc: 0.6645367162060616 --- acc: 82.0 --- loss: 0.4399953782558441\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04272562265396118\n",
            "epoch: 114 train loss: 0.43425601720809937 validation: auc: 0.6653359912053658 --- acc: 81.89999999999999 --- loss: 0.439718633890152\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04323326349258423\n",
            "epoch: 115 train loss: 0.41243064403533936 validation: auc: 0.6657838983050849 --- acc: 82.0 --- loss: 0.43949517607688904\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042787975072860716\n",
            "epoch: 116 train loss: 0.42933139204978943 validation: auc: 0.6659352300242131 --- acc: 82.0 --- loss: 0.43933695554733276\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042996692657470706\n",
            "epoch: 117 train loss: 0.4203089475631714 validation: auc: 0.6660778645181042 --- acc: 82.0 --- loss: 0.4391952455043793\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042562592029571536\n",
            "epoch: 118 train loss: 0.4370662569999695 validation: auc: 0.6659360997467368 --- acc: 82.05 --- loss: 0.4390600919723511\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04275040328502655\n",
            "epoch: 119 train loss: 0.4291181266307831 validation: auc: 0.666510116612396 --- acc: 82.05 --- loss: 0.4388255178928375\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04290650188922882\n",
            "epoch: 120 train loss: 0.42221805453300476 validation: auc: 0.667418106927166 --- acc: 82.05 --- loss: 0.43846502900123596\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042455852031707764\n",
            "epoch: 121 train loss: 0.438753217458725 validation: auc: 0.6687192118226601 --- acc: 81.95 --- loss: 0.4380953311920166\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042989695072174074\n",
            "epoch: 122 train loss: 0.41538915038108826 validation: auc: 0.669782882469177 --- acc: 81.95 --- loss: 0.43777263164520264\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04280811548233032\n",
            "epoch: 123 train loss: 0.42037665843963623 validation: auc: 0.6707178341821826 --- acc: 82.0 --- loss: 0.43752625584602356\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042619696259498595\n",
            "epoch: 124 train loss: 0.425721675157547 validation: auc: 0.671697141743898 --- acc: 82.0 --- loss: 0.43732476234436035\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04217839539051056\n",
            "epoch: 125 train loss: 0.4414604604244232 validation: auc: 0.6728451754752165 --- acc: 81.95 --- loss: 0.4370228052139282\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04252369701862335\n",
            "epoch: 126 train loss: 0.4260145425796509 validation: auc: 0.673836659152264 --- acc: 81.95 --- loss: 0.43665534257888794\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042808938026428225\n",
            "epoch: 127 train loss: 0.41369324922561646 validation: auc: 0.6746698533299936 --- acc: 82.0 --- loss: 0.43637678027153015\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04245391190052032\n",
            "epoch: 128 train loss: 0.4269505441188812 validation: auc: 0.6752708315938882 --- acc: 81.95 --- loss: 0.4361928403377533\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0422963947057724\n",
            "epoch: 129 train loss: 0.4323580265045166 validation: auc: 0.6754038991400184 --- acc: 81.95 --- loss: 0.43607813119888306\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04254279732704162\n",
            "epoch: 130 train loss: 0.4215756356716156 validation: auc: 0.675583061979906 --- acc: 82.0 --- loss: 0.4360049068927765\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042191505432128906\n",
            "epoch: 131 train loss: 0.4344857335090637 validation: auc: 0.6760509726976706 --- acc: 82.19999999999999 --- loss: 0.43595582246780396\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04225764572620392\n",
            "epoch: 132 train loss: 0.43071693181991577 validation: auc: 0.6765327989758148 --- acc: 82.19999999999999 --- loss: 0.4358873665332794\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042337754368782045\n",
            "epoch: 133 train loss: 0.42666420340538025 validation: auc: 0.6768293743564053 --- acc: 82.19999999999999 --- loss: 0.4357771575450897\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04199816882610321\n",
            "epoch: 134 train loss: 0.4395620822906494 validation: auc: 0.6773442500904511 --- acc: 82.19999999999999 --- loss: 0.4356021583080292\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04205839633941651\n",
            "epoch: 135 train loss: 0.4364693760871887 validation: auc: 0.6778539074893546 --- acc: 82.19999999999999 --- loss: 0.4354107975959778\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04210728406906128\n",
            "epoch: 136 train loss: 0.43390214443206787 validation: auc: 0.6784453188054883 --- acc: 82.15 --- loss: 0.43522748351097107\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042277741432189944\n",
            "epoch: 137 train loss: 0.4266441762447357 validation: auc: 0.6789680220422476 --- acc: 82.0 --- loss: 0.43507060408592224\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042664486169815066\n",
            "epoch: 138 train loss: 0.41057705879211426 validation: auc: 0.6793689641256854 --- acc: 82.05 --- loss: 0.43493935465812683\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042484518885612485\n",
            "epoch: 139 train loss: 0.4166485667228699 validation: auc: 0.679779473156884 --- acc: 82.19999999999999 --- loss: 0.4348284602165222\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04257501363754272\n",
            "epoch: 140 train loss: 0.41152071952819824 validation: auc: 0.6800769182599984 --- acc: 82.19999999999999 --- loss: 0.434747576713562\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04200021624565124\n",
            "epoch: 141 train loss: 0.43328219652175903 validation: auc: 0.680335225849545 --- acc: 82.25 --- loss: 0.43467456102371216\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04200402200222016\n",
            "epoch: 142 train loss: 0.43204596638679504 validation: auc: 0.6804909061812918 --- acc: 82.19999999999999 --- loss: 0.43454134464263916\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04185788631439209\n",
            "epoch: 143 train loss: 0.4371265769004822 validation: auc: 0.6806170159472322 --- acc: 82.25 --- loss: 0.4343523681163788\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04214501678943634\n",
            "epoch: 144 train loss: 0.4252624213695526 validation: auc: 0.6806691992986558 --- acc: 82.25 --- loss: 0.43417859077453613\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04207778573036194\n",
            "epoch: 145 train loss: 0.4277036190032959 validation: auc: 0.6809431618936295 --- acc: 82.19999999999999 --- loss: 0.4340529143810272\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04252189695835114\n",
            "epoch: 146 train loss: 0.40989041328430176 validation: auc: 0.6811640714146558 --- acc: 82.19999999999999 --- loss: 0.4339396059513092\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04243260622024536\n",
            "epoch: 147 train loss: 0.4128962755203247 validation: auc: 0.6814093331663467 --- acc: 82.39999999999999 --- loss: 0.43388861417770386\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042548945546150206\n",
            "epoch: 148 train loss: 0.4072912633419037 validation: auc: 0.681893768612062 --- acc: 82.39999999999999 --- loss: 0.43389081954956055\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042297950387001036\n",
            "epoch: 149 train loss: 0.41639453172683716 validation: auc: 0.6827234838996965 --- acc: 82.5 --- loss: 0.43390586972236633\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04218316972255707\n",
            "epoch: 150 train loss: 0.4203214645385742 validation: auc: 0.6836192980991346 --- acc: 82.5 --- loss: 0.4338744580745697\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042308655381202695\n",
            "epoch: 151 train loss: 0.41466957330703735 validation: auc: 0.684490760067908 --- acc: 82.5 --- loss: 0.43373221158981323\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04209448993206024\n",
            "epoch: 152 train loss: 0.4226090908050537 validation: auc: 0.6852282847680277 --- acc: 82.5 --- loss: 0.4334471821784973\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0422741711139679\n",
            "epoch: 153 train loss: 0.414712518453598 validation: auc: 0.6859901616988117 --- acc: 82.5 --- loss: 0.4330872893333435\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04245443642139435\n",
            "epoch: 154 train loss: 0.40702712535858154 validation: auc: 0.6868268347666361 --- acc: 82.5 --- loss: 0.4327746033668518\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041856443881988524\n",
            "epoch: 155 train loss: 0.4305225610733032 validation: auc: 0.6873112702123514 --- acc: 82.5 --- loss: 0.4325339198112488\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042588382959365845\n",
            "epoch: 156 train loss: 0.40081241726875305 validation: auc: 0.6877609167571178 --- acc: 82.39999999999999 --- loss: 0.432393342256546\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0420470654964447\n",
            "epoch: 157 train loss: 0.42160674929618835 validation: auc: 0.687969650162812 --- acc: 82.39999999999999 --- loss: 0.4323609471321106\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04206691980361939\n",
            "epoch: 158 train loss: 0.4198930263519287 validation: auc: 0.6880801049233252 --- acc: 82.45 --- loss: 0.4323911964893341\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04213463962078094\n",
            "epoch: 159 train loss: 0.4163840413093567 validation: auc: 0.6880470554674236 --- acc: 82.5 --- loss: 0.43244993686676025\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042159080505371094\n",
            "epoch: 160 train loss: 0.4147437810897827 validation: auc: 0.6880722774206117 --- acc: 82.55 --- loss: 0.4324837028980255\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041958349943161014\n",
            "epoch: 161 train loss: 0.4221823513507843 validation: auc: 0.6882749227686399 --- acc: 82.5 --- loss: 0.4324168860912323\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042062437534332274\n",
            "epoch: 162 train loss: 0.41736289858818054 validation: auc: 0.6885193147978069 --- acc: 82.45 --- loss: 0.4322143495082855\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04189975261688232\n",
            "epoch: 163 train loss: 0.42326441407203674 validation: auc: 0.6885227936879018 --- acc: 82.45 --- loss: 0.43199533224105835\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04199247658252716\n",
            "epoch: 164 train loss: 0.4193061590194702 validation: auc: 0.6888237176811111 --- acc: 82.39999999999999 --- loss: 0.43180906772613525\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04176835119724274\n",
            "epoch: 165 train loss: 0.42802610993385315 validation: auc: 0.6890350602543763 --- acc: 82.39999999999999 --- loss: 0.43166735768318176\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0422875314950943\n",
            "epoch: 166 train loss: 0.4069344997406006 validation: auc: 0.6892272689321198 --- acc: 82.45 --- loss: 0.4315836727619171\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04138097763061523\n",
            "epoch: 167 train loss: 0.4426216185092926 validation: auc: 0.689639517408366 --- acc: 82.55 --- loss: 0.4315379559993744\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041853141784667966\n",
            "epoch: 168 train loss: 0.4230107069015503 validation: auc: 0.6902048370487879 --- acc: 82.55 --- loss: 0.43150240182876587\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041882219910621646\n",
            "epoch: 169 train loss: 0.4212435185909271 validation: auc: 0.6907753750243523 --- acc: 82.55 --- loss: 0.4314650595188141\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04165030717849731\n",
            "epoch: 170 train loss: 0.4298211634159088 validation: auc: 0.6912954690935404 --- acc: 82.55 --- loss: 0.43142104148864746\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041406449675559995\n",
            "epoch: 171 train loss: 0.4390960931777954 validation: auc: 0.6917198936851188 --- acc: 82.55 --- loss: 0.43134790658950806\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04176674783229828\n",
            "epoch: 172 train loss: 0.42431408166885376 validation: auc: 0.6921356210514598 --- acc: 82.55 --- loss: 0.4312436878681183\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041717177629470824\n",
            "epoch: 173 train loss: 0.4259853661060333 validation: auc: 0.6922539033146865 --- acc: 82.55 --- loss: 0.4311652183532715\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04171452820301056\n",
            "epoch: 174 train loss: 0.42583614587783813 validation: auc: 0.6924261083743842 --- acc: 82.55 --- loss: 0.43110668659210205\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04189291894435883\n",
            "epoch: 175 train loss: 0.41833043098449707 validation: auc: 0.6923687066878184 --- acc: 82.55 --- loss: 0.4310928285121918\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04184811115264893\n",
            "epoch: 176 train loss: 0.41985031962394714 validation: auc: 0.692233029974117 --- acc: 82.55 --- loss: 0.4310969114303589\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04206260442733765\n",
            "epoch: 177 train loss: 0.4109121561050415 validation: auc: 0.6922504244245915 --- acc: 82.55 --- loss: 0.43111270666122437\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04138709306716919\n",
            "epoch: 178 train loss: 0.43764275312423706 validation: auc: 0.6922817344354457 --- acc: 82.55 --- loss: 0.43112921714782715\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042032545804977416\n",
            "epoch: 179 train loss: 0.41136547923088074 validation: auc: 0.6921843255127884 --- acc: 82.55 --- loss: 0.4310685992240906\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04173698127269745\n",
            "epoch: 180 train loss: 0.42287665605545044 validation: auc: 0.692182586067741 --- acc: 82.55 --- loss: 0.43099740147590637\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041661232709884644\n",
            "epoch: 181 train loss: 0.4256455898284912 validation: auc: 0.6922034594083103 --- acc: 82.55 --- loss: 0.4309007227420807\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041963806748390196\n",
            "epoch: 182 train loss: 0.4132126271724701 validation: auc: 0.6923721855779132 --- acc: 82.6 --- loss: 0.4307919442653656\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04200433492660523\n",
            "epoch: 183 train loss: 0.41117608547210693 validation: auc: 0.692509601736662 --- acc: 82.6 --- loss: 0.4307434856891632\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042076575756073\n",
            "epoch: 184 train loss: 0.4078768789768219 validation: auc: 0.692706159027024 --- acc: 82.6 --- loss: 0.43072307109832764\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04211658537387848\n",
            "epoch: 185 train loss: 0.40581879019737244 validation: auc: 0.6928748851966269 --- acc: 82.65 --- loss: 0.4307454824447632\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04172489941120148\n",
            "epoch: 186 train loss: 0.4210163950920105 validation: auc: 0.6930766608221313 --- acc: 82.65 --- loss: 0.43082138895988464\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04169445037841797\n",
            "epoch: 187 train loss: 0.4217422902584076 validation: auc: 0.693429768166764 --- acc: 82.65 --- loss: 0.43079084157943726\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.042055755853652954\n",
            "epoch: 188 train loss: 0.4068164825439453 validation: auc: 0.6937724388411122 --- acc: 82.65 --- loss: 0.430700421333313\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04205656051635742\n",
            "epoch: 189 train loss: 0.40630191564559937 validation: auc: 0.6941325039659346 --- acc: 82.65 --- loss: 0.4306371212005615\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04166754186153412\n",
            "epoch: 190 train loss: 0.42147040367126465 validation: auc: 0.6943342795914391 --- acc: 82.65 --- loss: 0.4306192100048065\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04163004755973816\n",
            "epoch: 191 train loss: 0.4227086901664734 validation: auc: 0.6947013024964515 --- acc: 82.65 --- loss: 0.4305680990219116\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041534644365310666\n",
            "epoch: 192 train loss: 0.42627501487731934 validation: auc: 0.6949517825832846 --- acc: 82.65 --- loss: 0.4304710626602173\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04177416563034057\n",
            "epoch: 193 train loss: 0.4164198637008667 validation: auc: 0.6951222481979349 --- acc: 82.55 --- loss: 0.4303515553474426\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04170413911342621\n",
            "epoch: 194 train loss: 0.41900721192359924 validation: auc: 0.6953361999387715 --- acc: 82.6 --- loss: 0.4302824139595032\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04191979169845581\n",
            "epoch: 195 train loss: 0.4102238416671753 validation: auc: 0.6954770949876151 --- acc: 82.65 --- loss: 0.4302837550640106\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0418448269367218\n",
            "epoch: 196 train loss: 0.41307854652404785 validation: auc: 0.6957101806239737 --- acc: 82.65 --- loss: 0.430276483297348\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04180367290973663\n",
            "epoch: 197 train loss: 0.4144822359085083 validation: auc: 0.6958528151178649 --- acc: 82.65 --- loss: 0.430229127407074\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041644501686096194\n",
            "epoch: 198 train loss: 0.4206616282463074 validation: auc: 0.6961519996660265 --- acc: 82.65 --- loss: 0.43019530177116394\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041759705543518065\n",
            "epoch: 199 train loss: 0.41578787565231323 validation: auc: 0.6965138042358965 --- acc: 82.65 --- loss: 0.4301486611366272\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04169432818889618\n",
            "epoch: 200 train loss: 0.4181210398674011 validation: auc: 0.6969208343770004 --- acc: 82.65 --- loss: 0.43012183904647827\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0415965735912323\n",
            "epoch: 201 train loss: 0.4216993749141693 validation: auc: 0.6971260888925999 --- acc: 82.65 --- loss: 0.43011119961738586\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04165464043617249\n",
            "epoch: 202 train loss: 0.41909024119377136 validation: auc: 0.6973000333973449 --- acc: 82.65 --- loss: 0.4300912022590637\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04186145663261413\n",
            "epoch: 203 train loss: 0.4105626046657562 validation: auc: 0.6974983301327544 --- acc: 82.65 --- loss: 0.4300365149974823\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04201230108737945\n",
            "epoch: 204 train loss: 0.404291570186615 validation: auc: 0.6976270490662659 --- acc: 82.65 --- loss: 0.43001583218574524\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04146255552768707\n",
            "epoch: 205 train loss: 0.42611488699913025 validation: auc: 0.6978183880214857 --- acc: 82.65 --- loss: 0.4299781024456024\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04157003164291382\n",
            "epoch: 206 train loss: 0.4214997887611389 validation: auc: 0.6979297125045225 --- acc: 82.65 --- loss: 0.42987531423568726\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041647490859031674\n",
            "epoch: 207 train loss: 0.41817203164100647 validation: auc: 0.6981401853552642 --- acc: 82.65 --- loss: 0.4297562539577484\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04201945960521698\n",
            "epoch: 208 train loss: 0.4031415283679962 validation: auc: 0.6982288970526843 --- acc: 82.69999999999999 --- loss: 0.42970678210258484\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041492569446563723\n",
            "epoch: 209 train loss: 0.4239177703857422 validation: auc: 0.6981123542345051 --- acc: 82.69999999999999 --- loss: 0.42972686886787415\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041750168800354\n",
            "epoch: 210 train loss: 0.4134589433670044 validation: auc: 0.697827085246723 --- acc: 82.69999999999999 --- loss: 0.4297092854976654\n",
            "<Timer(Thread-8811, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04196626543998718\n",
            "epoch: 211 train loss: 0.40454715490341187 validation: auc: 0.697712281873591 --- acc: 82.69999999999999 --- loss: 0.4296632409095764\n",
            "time up, break\n",
            "test auc: 0.6839711310608823 test acc: 84.35000000000001 test loss 0.40878257155418396\n",
            "0.01\n",
            "0.6839711310608823\n",
            "84.35000000000001\n",
            "0.40878257155418396\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachineModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FeaturesLinear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachine. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，无法输出最后 5000 行内容。\u001b[0m\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014061750471591949\n",
            "epoch: 143 train loss: 0.13161994516849518 validation: auc: 0.6417247339707233 --- acc: 79.4 --- loss: 0.5084497332572937\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013692307472229003\n",
            "epoch: 144 train loss: 0.13542157411575317 validation: auc: 0.6419215068947689 --- acc: 79.45 --- loss: 0.5081696510314941\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013409647345542907\n",
            "epoch: 145 train loss: 0.13613632321357727 validation: auc: 0.6420951300630446 --- acc: 79.4 --- loss: 0.5079262852668762\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013223403692245483\n",
            "epoch: 146 train loss: 0.1331547200679779 validation: auc: 0.6423632814007145 --- acc: 79.5 --- loss: 0.50771564245224\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013140745460987091\n",
            "epoch: 147 train loss: 0.1263444572687149 validation: auc: 0.6424732427406225 --- acc: 79.4 --- loss: 0.507501482963562\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012940055131912232\n",
            "epoch: 148 train loss: 0.12439120560884476 validation: auc: 0.6425832040805303 --- acc: 79.35 --- loss: 0.5072957873344421\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012706005573272705\n",
            "epoch: 149 train loss: 0.12404226511716843 validation: auc: 0.6428436388329437 --- acc: 79.4 --- loss: 0.5071262717247009\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012438522279262542\n",
            "epoch: 150 train loss: 0.12519541382789612 validation: auc: 0.6431175776095562 --- acc: 79.35 --- loss: 0.5069634914398193\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012505210936069489\n",
            "epoch: 151 train loss: 0.11325813084840775 validation: auc: 0.6433394294356861 --- acc: 79.4 --- loss: 0.5067148804664612\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012085147947072983\n",
            "epoch: 152 train loss: 0.12095996737480164 validation: auc: 0.6435825018712719 --- acc: 79.35 --- loss: 0.5064266920089722\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011841858178377152\n",
            "epoch: 153 train loss: 0.12181787192821503 validation: auc: 0.6438255743068577 --- acc: 79.35 --- loss: 0.5061639547348022\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0116982564330101\n",
            "epoch: 154 train loss: 0.11889722943305969 validation: auc: 0.6439355356467655 --- acc: 79.4 --- loss: 0.5059600472450256\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011701518297195434\n",
            "epoch: 155 train loss: 0.11030912399291992 validation: auc: 0.64403006381616 --- acc: 79.45 --- loss: 0.5058107972145081\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011456874012947083\n",
            "epoch: 156 train loss: 0.11178789287805557 validation: auc: 0.644195970399179 --- acc: 79.4 --- loss: 0.5056095123291016\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011316291987895966\n",
            "epoch: 157 train loss: 0.10932040214538574 validation: auc: 0.6444178222253089 --- acc: 79.35 --- loss: 0.5053994655609131\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011119857430458069\n",
            "epoch: 158 train loss: 0.10928536206483841 validation: auc: 0.644508492102075 --- acc: 79.4 --- loss: 0.5052065849304199\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01099255159497261\n",
            "epoch: 159 train loss: 0.10664942115545273 validation: auc: 0.6446512489293238 --- acc: 79.3 --- loss: 0.504963219165802\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010849952697753906\n",
            "epoch: 160 train loss: 0.10481652617454529 validation: auc: 0.6447978640492009 --- acc: 79.35 --- loss: 0.5047783851623535\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01056554913520813\n",
            "epoch: 161 train loss: 0.10875561088323593 validation: auc: 0.6449753455101048 --- acc: 79.45 --- loss: 0.5047428011894226\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010443845391273498\n",
            "epoch: 162 train loss: 0.10644306242465973 validation: auc: 0.6451817641657214 --- acc: 79.45 --- loss: 0.5047536492347717\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010289248824119569\n",
            "epoch: 163 train loss: 0.10556894540786743 validation: auc: 0.6452454259940891 --- acc: 79.5 --- loss: 0.504768967628479\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01019751951098442\n",
            "epoch: 164 train loss: 0.10229971259832382 validation: auc: 0.6453206627003419 --- acc: 79.4 --- loss: 0.5048055648803711\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009958753734827042\n",
            "epoch: 165 train loss: 0.10502683371305466 validation: auc: 0.6455039316001884 --- acc: 79.35 --- loss: 0.5048372149467468\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009924063831567765\n",
            "epoch: 166 train loss: 0.09985408186912537 validation: auc: 0.6455135773317591 --- acc: 79.3 --- loss: 0.5048500299453735\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009978485852479934\n",
            "epoch: 167 train loss: 0.09118761122226715 validation: auc: 0.6455425145264718 --- acc: 79.35 --- loss: 0.5047602653503418\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009683184325695038\n",
            "epoch: 168 train loss: 0.09659747034311295 validation: auc: 0.645702633670548 --- acc: 79.4 --- loss: 0.5045650005340576\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00949522778391838\n",
            "epoch: 169 train loss: 0.09788859635591507 validation: auc: 0.6458318864735977 --- acc: 79.55 --- loss: 0.5043880939483643\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009431789070367813\n",
            "epoch: 170 train loss: 0.09433899819850922 validation: auc: 0.6460440925681568 --- acc: 79.60000000000001 --- loss: 0.5042784214019775\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009198234230279923\n",
            "epoch: 171 train loss: 0.09773237258195877 validation: auc: 0.6462042117122331 --- acc: 79.55 --- loss: 0.5042206645011902\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009155628085136414\n",
            "epoch: 172 train loss: 0.0936424508690834 validation: auc: 0.6463083856131984 --- acc: 79.55 --- loss: 0.5041258335113525\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00896214172244072\n",
            "epoch: 173 train loss: 0.09564276039600372 validation: auc: 0.6463913389047078 --- acc: 79.80000000000001 --- loss: 0.5039591789245605\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008928623795509339\n",
            "epoch: 174 train loss: 0.09137064963579178 validation: auc: 0.6463990554899645 --- acc: 79.80000000000001 --- loss: 0.5037988424301147\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008829963952302932\n",
            "epoch: 175 train loss: 0.08988510072231293 validation: auc: 0.6464453550015047 --- acc: 79.75 --- loss: 0.503624439239502\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00878625214099884\n",
            "epoch: 176 train loss: 0.08624785393476486 validation: auc: 0.6464877962204165 --- acc: 79.75 --- loss: 0.5034967660903931\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008624698966741562\n",
            "epoch: 177 train loss: 0.08747588098049164 validation: auc: 0.6466093324382094 --- acc: 79.80000000000001 --- loss: 0.5034163594245911\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008451380580663682\n",
            "epoch: 178 train loss: 0.08924099802970886 validation: auc: 0.6466691359739488 --- acc: 79.75 --- loss: 0.503413200378418\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008512871712446213\n",
            "epoch: 179 train loss: 0.0817989632487297 validation: auc: 0.6467308686560023 --- acc: 79.75 --- loss: 0.50348299741745\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00831993892788887\n",
            "epoch: 180 train loss: 0.08458201587200165 validation: auc: 0.6467790973138566 --- acc: 79.75 --- loss: 0.5035532116889954\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00822644904255867\n",
            "epoch: 181 train loss: 0.08350049704313278 validation: auc: 0.6467983887769985 --- acc: 79.80000000000001 --- loss: 0.5035955905914307\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00822480395436287\n",
            "epoch: 182 train loss: 0.0788906067609787 validation: auc: 0.6467675224359717 --- acc: 79.95 --- loss: 0.5035647749900818\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008196523785591126\n",
            "epoch: 183 train loss: 0.0753990188241005 validation: auc: 0.6467598058507149 --- acc: 79.9 --- loss: 0.5034782290458679\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008068381249904633\n",
            "epoch: 184 train loss: 0.07602885365486145 validation: auc: 0.6468196093864543 --- acc: 79.85 --- loss: 0.5033788084983826\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007878030091524124\n",
            "epoch: 185 train loss: 0.07922676205635071 validation: auc: 0.6468852003611361 --- acc: 79.85 --- loss: 0.5032981038093567\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007705146074295044\n",
            "epoch: 186 train loss: 0.0818239152431488 validation: auc: 0.646972011945274 --- acc: 79.85 --- loss: 0.5032311677932739\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007818237692117692\n",
            "epoch: 187 train loss: 0.07312541455030441 validation: auc: 0.647093548163067 --- acc: 79.85 --- loss: 0.5031976103782654\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007630701363086701\n",
            "epoch: 188 train loss: 0.07645825296640396 validation: auc: 0.6471880763324613 --- acc: 79.9 --- loss: 0.5031384229660034\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007610028982162476\n",
            "epoch: 189 train loss: 0.07325146347284317 validation: auc: 0.6472845336481701 --- acc: 79.9 --- loss: 0.5030488967895508\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007564724236726761\n",
            "epoch: 190 train loss: 0.07110410183668137 validation: auc: 0.6473481954765377 --- acc: 79.95 --- loss: 0.502972424030304\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007501760125160217\n",
            "epoch: 191 train loss: 0.06975327432155609 validation: auc: 0.6474099281585913 --- acc: 79.95 --- loss: 0.5028798580169678\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007174257934093475\n",
            "epoch: 192 train loss: 0.07903043925762177 validation: auc: 0.6474504402311889 --- acc: 79.95 --- loss: 0.5028263926506042\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007283525168895721\n",
            "epoch: 193 train loss: 0.07094930857419968 validation: auc: 0.6476394965699778 --- acc: 79.95 --- loss: 0.5028652548789978\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007147038727998734\n",
            "epoch: 194 train loss: 0.07274377346038818 validation: auc: 0.6477706785193417 --- acc: 79.95 --- loss: 0.5029107928276062\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007038640230894089\n",
            "epoch: 195 train loss: 0.07348761707544327 validation: auc: 0.6479790263212724 --- acc: 80.05 --- loss: 0.5029863715171814\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0070101916790008545\n",
            "epoch: 196 train loss: 0.07113604247570038 validation: auc: 0.6480561921738393 --- acc: 80.05 --- loss: 0.503026008605957\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006943999230861664\n",
            "epoch: 197 train loss: 0.07030034065246582 validation: auc: 0.6480909168074944 --- acc: 80.05 --- loss: 0.5029861927032471\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007059232890605926\n",
            "epoch: 198 train loss: 0.06233450770378113 validation: auc: 0.6481719409526896 --- acc: 80.05 --- loss: 0.5028672218322754\n",
            "<Timer(Thread-9237, started 140343223252736)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006856589019298554\n",
            "epoch: 199 train loss: 0.06711682677268982 validation: auc: 0.6482433193663141 --- acc: 80.05 --- loss: 0.5026828646659851\n",
            "time up, break\n",
            "test auc: 0.6269294668223817 test acc: 77.5 test loss 0.5720809102058411\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3232703924179077\n",
            "epoch: 0 train loss: 3.1428825855255127 validation: auc: 0.5043155003048052 --- acc: 27.700000000000003 --- loss: 3.2328431606292725\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3194150686264038\n",
            "epoch: 1 train loss: 3.126809597015381 validation: auc: 0.5043531186579315 --- acc: 27.800000000000004 --- loss: 3.199026107788086\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.3134665250778198\n",
            "epoch: 2 train loss: 3.1933434009552 validation: auc: 0.5044148513399851 --- acc: 28.050000000000004 --- loss: 3.1641311645507812\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.31160945892333985\n",
            "epoch: 3 train loss: 3.0923752784729004 validation: auc: 0.504492981765709 --- acc: 28.1 --- loss: 3.128237247467041\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.30846681594848635\n",
            "epoch: 4 train loss: 3.0365097522735596 validation: auc: 0.5045518207282913 --- acc: 28.4 --- loss: 3.090890884399414\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.30376856327056884\n",
            "epoch: 5 train loss: 3.034878730773926 validation: auc: 0.5045932973740461 --- acc: 28.449999999999996 --- loss: 3.051868200302124\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.29843697547912595\n",
            "epoch: 6 train loss: 3.0497186183929443 validation: auc: 0.5046405614587433 --- acc: 28.95 --- loss: 3.011016368865967\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.29500226974487304\n",
            "epoch: 7 train loss: 2.979233980178833 validation: auc: 0.504748593652337 --- acc: 29.25 --- loss: 2.968177556991577\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2909470796585083\n",
            "epoch: 8 train loss: 2.923130512237549 validation: auc: 0.5048392635291031 --- acc: 29.4 --- loss: 2.9230806827545166\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2867203950881958\n",
            "epoch: 9 train loss: 2.8631696701049805 validation: auc: 0.5049067836500992 --- acc: 29.9 --- loss: 2.8759071826934814\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.28295438289642333\n",
            "epoch: 10 train loss: 2.7733371257781982 validation: auc: 0.5050234970021067 --- acc: 30.599999999999998 --- loss: 2.8264482021331787\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.279280948638916\n",
            "epoch: 11 train loss: 2.6679108142852783 validation: auc: 0.5050370010263059 --- acc: 30.95 --- loss: 2.7745468616485596\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2718112707138062\n",
            "epoch: 12 train loss: 2.701749801635742 validation: auc: 0.5051585372440988 --- acc: 31.5 --- loss: 2.720367193222046\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2659163236618042\n",
            "epoch: 13 train loss: 2.661004066467285 validation: auc: 0.5052742860229491 --- acc: 32.2 --- loss: 2.6636879444122314\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2612516164779663\n",
            "epoch: 14 train loss: 2.5583748817443848 validation: auc: 0.5053919639481137 --- acc: 33.25 --- loss: 2.6047215461730957\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.25583078861236574\n",
            "epoch: 15 train loss: 2.4734842777252197 validation: auc: 0.5055241104706345 --- acc: 33.75 --- loss: 2.5433361530303955\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2480501651763916\n",
            "epoch: 16 train loss: 2.4705231189727783 validation: auc: 0.5056379301031707 --- acc: 34.300000000000004 --- loss: 2.4796547889709473\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.24336740970611573\n",
            "epoch: 17 train loss: 2.3318936824798584 validation: auc: 0.5057874389425192 --- acc: 34.849999999999994 --- loss: 2.413735866546631\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2364368200302124\n",
            "epoch: 18 train loss: 2.271667242050171 validation: auc: 0.5059311603429251 --- acc: 35.6 --- loss: 2.3456850051879883\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.22961325645446778\n",
            "epoch: 19 train loss: 2.197571277618408 validation: auc: 0.5061501184495837 --- acc: 36.5 --- loss: 2.2756946086883545\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.22127408981323243\n",
            "epoch: 20 train loss: 2.1724865436553955 validation: auc: 0.5064144114946254 --- acc: 37.25 --- loss: 2.203965187072754\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2137850284576416\n",
            "epoch: 21 train loss: 2.105522394180298 validation: auc: 0.50670088972228 --- acc: 38.35 --- loss: 2.13077712059021\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.20756833553314208\n",
            "epoch: 22 train loss: 1.9790492057800293 validation: auc: 0.5070597109367163 --- acc: 39.15 --- loss: 2.0562756061553955\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.20013027191162108\n",
            "epoch: 23 train loss: 1.8957771062850952 validation: auc: 0.5074397527606084 --- acc: 40.1 --- loss: 1.9808109998703003\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1914968252182007\n",
            "epoch: 24 train loss: 1.8558555841445923 validation: auc: 0.5078795981202399 --- acc: 41.349999999999994 --- loss: 1.9047132730484009\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1828474998474121\n",
            "epoch: 25 train loss: 1.8115779161453247 validation: auc: 0.5083995030519095 --- acc: 42.55 --- loss: 1.828321099281311\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1756243586540222\n",
            "epoch: 26 train loss: 1.71134614944458 validation: auc: 0.5089251954225216 --- acc: 43.9 --- loss: 1.7519810199737549\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1671138286590576\n",
            "epoch: 27 train loss: 1.660645604133606 validation: auc: 0.5093631116358389 --- acc: 45.2 --- loss: 1.676154613494873\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1596234917640686\n",
            "epoch: 28 train loss: 1.5738447904586792 validation: auc: 0.5099785093100601 --- acc: 46.35 --- loss: 1.6011979579925537\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1525721311569214\n",
            "epoch: 29 train loss: 1.4739211797714233 validation: auc: 0.5107048328973463 --- acc: 48.199999999999996 --- loss: 1.5276235342025757\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.14450939893722534\n",
            "epoch: 30 train loss: 1.4210237264633179 validation: auc: 0.5113385574615521 --- acc: 49.5 --- loss: 1.4558769464492798\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.13716000318527222\n",
            "epoch: 31 train loss: 1.3490347862243652 validation: auc: 0.5122028150103016 --- acc: 50.74999999999999 --- loss: 1.3863320350646973\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.13086307048797607\n",
            "epoch: 32 train loss: 1.247218370437622 validation: auc: 0.513053568534852 --- acc: 52.65 --- loss: 1.3194233179092407\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1227847695350647\n",
            "epoch: 33 train loss: 1.2284860610961914 validation: auc: 0.5141531819339306 --- acc: 54.35 --- loss: 1.255510687828064\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1172349214553833\n",
            "epoch: 34 train loss: 1.1252195835113525 validation: auc: 0.5152952365519211 --- acc: 56.15 --- loss: 1.1948484182357788\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1114973783493042\n",
            "epoch: 35 train loss: 1.0457274913787842 validation: auc: 0.5166070560455587 --- acc: 57.550000000000004 --- loss: 1.137721300125122\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10505514144897461\n",
            "epoch: 36 train loss: 1.0134886503219604 validation: auc: 0.5179420252949665 --- acc: 59.050000000000004 --- loss: 1.0843796730041504\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09925100803375245\n",
            "epoch: 37 train loss: 0.973752498626709 validation: auc: 0.5194718383221058 --- acc: 60.85 --- loss: 1.034988284111023\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09428257942199707\n",
            "epoch: 38 train loss: 0.9214327931404114 validation: auc: 0.5211656287859496 --- acc: 62.0 --- loss: 0.9895651340484619\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09012745618820191\n",
            "epoch: 39 train loss: 0.8556714057922363 validation: auc: 0.5230677670517243 --- acc: 63.55 --- loss: 0.948111355304718\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0859691321849823\n",
            "epoch: 40 train loss: 0.8125116229057312 validation: auc: 0.5251223078763185 --- acc: 65.0 --- loss: 0.9105774760246277\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08223118782043456\n",
            "epoch: 41 train loss: 0.769747793674469 validation: auc: 0.5272038567493114 --- acc: 66.60000000000001 --- loss: 0.8769162893295288\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07808483242988587\n",
            "epoch: 42 train loss: 0.7639029026031494 validation: auc: 0.5292024523307945 --- acc: 67.65 --- loss: 0.8468821048736572\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07486011981964111\n",
            "epoch: 43 train loss: 0.7389314770698547 validation: auc: 0.5317566420507597 --- acc: 69.19999999999999 --- loss: 0.820268452167511\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07213356494903564\n",
            "epoch: 44 train loss: 0.7109971642494202 validation: auc: 0.5343262649412381 --- acc: 69.95 --- loss: 0.7968468070030212\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06972437500953674\n",
            "epoch: 45 train loss: 0.6847970485687256 validation: auc: 0.5366354530793034 --- acc: 70.6 --- loss: 0.776330828666687\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06757806539535523\n",
            "epoch: 46 train loss: 0.6611931324005127 validation: auc: 0.5391221226782724 --- acc: 71.8 --- loss: 0.7583667635917664\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06593478322029114\n",
            "epoch: 47 train loss: 0.6299176216125488 validation: auc: 0.5415470595951879 --- acc: 72.89999999999999 --- loss: 0.7426028847694397\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06318625807762146\n",
            "epoch: 48 train loss: 0.6532052755355835 validation: auc: 0.5442980222391987 --- acc: 73.8 --- loss: 0.7287629246711731\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06225472092628479\n",
            "epoch: 49 train loss: 0.6114857792854309 validation: auc: 0.5471396547599756 --- acc: 74.15 --- loss: 0.7165913581848145\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.060437464714050294\n",
            "epoch: 50 train loss: 0.6134804487228394 validation: auc: 0.5497247108209675 --- acc: 74.5 --- loss: 0.7058439254760742\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05943444967269897\n",
            "epoch: 51 train loss: 0.5869598984718323 validation: auc: 0.5523078377356452 --- acc: 75.3 --- loss: 0.6962804794311523\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.056990128755569455\n",
            "epoch: 52 train loss: 0.6242151856422424 validation: auc: 0.5550549420870277 --- acc: 75.9 --- loss: 0.6876689195632935\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.056738102436065675\n",
            "epoch: 53 train loss: 0.5771743059158325 validation: auc: 0.5576506084527475 --- acc: 76.44999999999999 --- loss: 0.6798890829086304\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.055947226285934445\n",
            "epoch: 54 train loss: 0.5549517869949341 validation: auc: 0.5600938722596477 --- acc: 76.7 --- loss: 0.6728204488754272\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05565189123153687\n",
            "epoch: 55 train loss: 0.5149306058883667 validation: auc: 0.562484084542908 --- acc: 77.0 --- loss: 0.6663546562194824\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05443577766418457\n",
            "epoch: 56 train loss: 0.5137608051300049 validation: auc: 0.5649514626787354 --- acc: 77.3 --- loss: 0.6603724956512451\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05310104489326477\n",
            "epoch: 57 train loss: 0.5191597938537598 validation: auc: 0.5674284865461336 --- acc: 77.5 --- loss: 0.6548009514808655\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.051549994945526124\n",
            "epoch: 58 train loss: 0.5342810153961182 validation: auc: 0.5697704701715397 --- acc: 77.75 --- loss: 0.6495997309684753\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05075201392173767\n",
            "epoch: 59 train loss: 0.5204582214355469 validation: auc: 0.5720507211148922 --- acc: 77.8 --- loss: 0.6447253227233887\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05062326788902283\n",
            "epoch: 60 train loss: 0.481131911277771 validation: auc: 0.574255735351992 --- acc: 78.0 --- loss: 0.6401415467262268\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04960629343986511\n",
            "epoch: 61 train loss: 0.4777178168296814 validation: auc: 0.5765147656858887 --- acc: 78.10000000000001 --- loss: 0.6358165740966797\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04805331826210022\n",
            "epoch: 62 train loss: 0.4965687692165375 validation: auc: 0.5785770230957397 --- acc: 78.10000000000001 --- loss: 0.6317334175109863\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04700095653533935\n",
            "epoch: 63 train loss: 0.4961928427219391 validation: auc: 0.5806701468466174 --- acc: 78.25 --- loss: 0.627856433391571\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04648876190185547\n",
            "epoch: 64 train loss: 0.47506827116012573 validation: auc: 0.5824623237724842 --- acc: 78.25 --- loss: 0.6241682171821594\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0450962632894516\n",
            "epoch: 65 train loss: 0.48955968022346497 validation: auc: 0.5843721786235155 --- acc: 78.35 --- loss: 0.6206810474395752\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.045309150218963624\n",
            "epoch: 66 train loss: 0.4405710697174072 validation: auc: 0.586349553595543 --- acc: 78.4 --- loss: 0.6173684597015381\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044313228130340575\n",
            "epoch: 67 train loss: 0.4405396282672882 validation: auc: 0.5882362586908042 --- acc: 78.35 --- loss: 0.6142120957374573\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04284093379974365\n",
            "epoch: 68 train loss: 0.4600643217563629 validation: auc: 0.5898856787894221 --- acc: 78.3 --- loss: 0.611185610294342\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04279163777828217\n",
            "epoch: 69 train loss: 0.42336297035217285 validation: auc: 0.5914579330354731 --- acc: 78.25 --- loss: 0.6082478165626526\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04174112379550934\n",
            "epoch: 70 train loss: 0.4273929297924042 validation: auc: 0.5931015656951486 --- acc: 78.0 --- loss: 0.6054010987281799\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.040954369306564334\n",
            "epoch: 71 train loss: 0.4212351143360138 validation: auc: 0.5944963384802957 --- acc: 77.95 --- loss: 0.6026387214660645\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.040846922993659975\n",
            "epoch: 72 train loss: 0.38870471715927124 validation: auc: 0.5959663479716956 --- acc: 77.85 --- loss: 0.5999431014060974\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.040244320034980775\n",
            "epoch: 73 train loss: 0.37644654512405396 validation: auc: 0.5973128920989883 --- acc: 78.05 --- loss: 0.597322940826416\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03899200558662415\n",
            "epoch: 74 train loss: 0.3905009329319 validation: auc: 0.5986169950073693 --- acc: 78.10000000000001 --- loss: 0.5947920083999634\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03843458890914917\n",
            "epoch: 75 train loss: 0.3775586187839508 validation: auc: 0.599861294380011 --- acc: 78.25 --- loss: 0.5923221111297607\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.037926867604255676\n",
            "epoch: 76 train loss: 0.36299940943717957 validation: auc: 0.6013409496029816 --- acc: 78.3 --- loss: 0.5898923873901367\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.037329578399658205\n",
            "epoch: 77 train loss: 0.35251984000205994 validation: auc: 0.6027723761680981 --- acc: 78.4 --- loss: 0.5875141620635986\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0358724981546402\n",
            "epoch: 78 train loss: 0.37669673562049866 validation: auc: 0.6041189202953909 --- acc: 78.55 --- loss: 0.5851954221725464\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03543524146080017\n",
            "epoch: 79 train loss: 0.36080417037010193 validation: auc: 0.60560243381099 --- acc: 78.45 --- loss: 0.5829493999481201\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03515651822090149\n",
            "epoch: 80 train loss: 0.3390396237373352 validation: auc: 0.6067387009900379 --- acc: 78.4 --- loss: 0.5807781219482422\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03463592231273651\n",
            "epoch: 81 train loss: 0.3271750509738922 validation: auc: 0.608112253165729 --- acc: 78.3 --- loss: 0.5786615014076233\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.033748281002044675\n",
            "epoch: 82 train loss: 0.33056122064590454 validation: auc: 0.6093526942457423 --- acc: 78.3 --- loss: 0.576610267162323\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03337127268314362\n",
            "epoch: 83 train loss: 0.31402838230133057 validation: auc: 0.6104503784985068 --- acc: 78.35 --- loss: 0.5746163129806519\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.032029497623443606\n",
            "epoch: 84 train loss: 0.3364454209804535 validation: auc: 0.6116194411648957 --- acc: 78.45 --- loss: 0.5726765990257263\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03204576969146729\n",
            "epoch: 85 train loss: 0.30510151386260986 validation: auc: 0.612714231698189 --- acc: 78.4 --- loss: 0.570797324180603\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.030974382162094118\n",
            "epoch: 86 train loss: 0.31769612431526184 validation: auc: 0.6137569352809995 --- acc: 78.3 --- loss: 0.5689594745635986\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.030891254544258118\n",
            "epoch: 87 train loss: 0.29126518964767456 validation: auc: 0.6147735953885686 --- acc: 78.4 --- loss: 0.5671778321266174\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03056512475013733\n",
            "epoch: 88 train loss: 0.27507954835891724 validation: auc: 0.6159850992738692 --- acc: 78.60000000000001 --- loss: 0.5654201507568359\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.029619961977005005\n",
            "epoch: 89 train loss: 0.28392258286476135 validation: auc: 0.6170191216982661 --- acc: 78.60000000000001 --- loss: 0.563684344291687\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.028843063116073608\n",
            "epoch: 90 train loss: 0.2867071032524109 validation: auc: 0.6181013727805171 --- acc: 78.64999999999999 --- loss: 0.5619733929634094\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02866019308567047\n",
            "epoch: 91 train loss: 0.26622313261032104 validation: auc: 0.6191855530090825 --- acc: 78.7 --- loss: 0.5603014230728149\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027486222982406616\n",
            "epoch: 92 train loss: 0.28568652272224426 validation: auc: 0.6200710311672879 --- acc: 78.64999999999999 --- loss: 0.5586912631988525\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.027303168177604677\n",
            "epoch: 93 train loss: 0.26604166626930237 validation: auc: 0.6210819038359146 --- acc: 78.64999999999999 --- loss: 0.5571426749229431\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02670169472694397\n",
            "epoch: 94 train loss: 0.26347753405570984 validation: auc: 0.6220754141877137 --- acc: 78.75 --- loss: 0.5556236505508423\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.025978699326515198\n",
            "epoch: 95 train loss: 0.26621371507644653 validation: auc: 0.6229068762491223 --- acc: 78.85 --- loss: 0.5541562438011169\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.025428768992424012\n",
            "epoch: 96 train loss: 0.2624739408493042 validation: auc: 0.6236814284942628 --- acc: 78.9 --- loss: 0.5527349710464478\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02516968548297882\n",
            "epoch: 97 train loss: 0.24749301373958588 validation: auc: 0.6245582254940544 --- acc: 78.9 --- loss: 0.5513516664505005\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02467874139547348\n",
            "epoch: 98 train loss: 0.2420833557844162 validation: auc: 0.6253993332870337 --- acc: 78.85 --- loss: 0.5500025749206543\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.024325439333915712\n",
            "epoch: 99 train loss: 0.23190362751483917 validation: auc: 0.6261266214474771 --- acc: 78.85 --- loss: 0.5487069487571716\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02369677126407623\n",
            "epoch: 100 train loss: 0.23301206529140472 validation: auc: 0.6269542252162572 --- acc: 78.8 --- loss: 0.5474678874015808\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02282678484916687\n",
            "epoch: 101 train loss: 0.24412678182125092 validation: auc: 0.6277953330092366 --- acc: 78.85 --- loss: 0.5462751984596252\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.022710032761096954\n",
            "epoch: 102 train loss: 0.22574104368686676 validation: auc: 0.628489825682339 --- acc: 78.95 --- loss: 0.5451041460037231\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02200232744216919\n",
            "epoch: 103 train loss: 0.23139134049415588 validation: auc: 0.6292672716469507 --- acc: 78.85 --- loss: 0.543964684009552\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021905244886875154\n",
            "epoch: 104 train loss: 0.2130378931760788 validation: auc: 0.6300061346852791 --- acc: 78.9 --- loss: 0.5428517460823059\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021214792132377626\n",
            "epoch: 105 train loss: 0.21872971951961517 validation: auc: 0.6306755484562971 --- acc: 78.95 --- loss: 0.5417369604110718\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020884212851524354\n",
            "epoch: 106 train loss: 0.21057212352752686 validation: auc: 0.6313391747883728 --- acc: 79.14999999999999 --- loss: 0.5406597852706909\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02064315676689148\n",
            "epoch: 107 train loss: 0.1992989480495453 validation: auc: 0.6318928397805402 --- acc: 79.10000000000001 --- loss: 0.5396295189857483\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020400819182395936\n",
            "epoch: 108 train loss: 0.18847151100635529 validation: auc: 0.6325101666010756 --- acc: 79.2 --- loss: 0.5386309027671814\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019659771025180815\n",
            "epoch: 109 train loss: 0.1979786455631256 validation: auc: 0.6331506431773811 --- acc: 79.3 --- loss: 0.5376440286636353\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019348084926605225\n",
            "epoch: 110 train loss: 0.1908101588487625 validation: auc: 0.6337120247548055 --- acc: 79.2 --- loss: 0.5366693139076233\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019117675721645355\n",
            "epoch: 111 train loss: 0.180894672870636 validation: auc: 0.6343409264532258 --- acc: 79.14999999999999 --- loss: 0.5357164740562439\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018764446675777435\n",
            "epoch: 112 train loss: 0.17622898519039154 validation: auc: 0.634921599493792 --- acc: 79.2 --- loss: 0.5348117351531982\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.018320207297801972\n",
            "epoch: 113 train loss: 0.17563174664974213 validation: auc: 0.6354501855838754 --- acc: 79.2 --- loss: 0.5339384078979492\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017829178273677825\n",
            "epoch: 114 train loss: 0.17729954421520233 validation: auc: 0.6359556219181887 --- acc: 79.10000000000001 --- loss: 0.533087968826294\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017415618896484374\n",
            "epoch: 115 train loss: 0.176358163356781 validation: auc: 0.6364244044725329 --- acc: 79.05 --- loss: 0.5322573781013489\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017046765983104707\n",
            "epoch: 116 train loss: 0.17384742200374603 validation: auc: 0.6369221242215894 --- acc: 79.10000000000001 --- loss: 0.5314674973487854\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016688518226146698\n",
            "epoch: 117 train loss: 0.17146718502044678 validation: auc: 0.6373889776296193 --- acc: 79.14999999999999 --- loss: 0.5307345390319824\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016571737825870514\n",
            "epoch: 118 train loss: 0.159674733877182 validation: auc: 0.6379175637197028 --- acc: 79.05 --- loss: 0.5300320982933044\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016445463895797728\n",
            "epoch: 119 train loss: 0.14873014390468597 validation: auc: 0.638399850298246 --- acc: 79.0 --- loss: 0.5293390154838562\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015777598321437835\n",
            "epoch: 120 train loss: 0.15962518751621246 validation: auc: 0.6387953252926515 --- acc: 78.95 --- loss: 0.5286346673965454\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015797415375709535\n",
            "epoch: 121 train loss: 0.14366281032562256 validation: auc: 0.6392178083354554 --- acc: 78.95 --- loss: 0.5279248952865601\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01505422443151474\n",
            "epoch: 122 train loss: 0.15831027925014496 validation: auc: 0.6396441496708877 --- acc: 78.95 --- loss: 0.5272029042243958\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014616994559764862\n",
            "epoch: 123 train loss: 0.1610862761735916 validation: auc: 0.6400068291779522 --- acc: 78.95 --- loss: 0.5265018343925476\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014647723734378814\n",
            "epoch: 124 train loss: 0.14571180939674377 validation: auc: 0.6403328549050473 --- acc: 78.95 --- loss: 0.5258083939552307\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014247040450572967\n",
            "epoch: 125 train loss: 0.14763273298740387 validation: auc: 0.6408035666057057 --- acc: 78.95 --- loss: 0.5251181125640869\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014322499930858611\n",
            "epoch: 126 train loss: 0.13107424974441528 validation: auc: 0.6412241205021953 --- acc: 79.10000000000001 --- loss: 0.524448812007904\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01392870545387268\n",
            "epoch: 127 train loss: 0.13349691033363342 validation: auc: 0.6415385713514056 --- acc: 79.10000000000001 --- loss: 0.5237810611724854\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013579817116260528\n",
            "epoch: 128 train loss: 0.1344270557165146 validation: auc: 0.6419687709794661 --- acc: 79.10000000000001 --- loss: 0.5231192708015442\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013373169302940368\n",
            "epoch: 129 train loss: 0.1301654726266861 validation: auc: 0.642279363536048 --- acc: 79.10000000000001 --- loss: 0.5224860310554504\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013061681389808654\n",
            "epoch: 130 train loss: 0.1303151696920395 validation: auc: 0.6426786968230819 --- acc: 79.10000000000001 --- loss: 0.5218979120254517\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013014090061187745\n",
            "epoch: 131 train loss: 0.12029385566711426 validation: auc: 0.6429680687702077 --- acc: 79.25 --- loss: 0.521330714225769\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012502063810825349\n",
            "epoch: 132 train loss: 0.12910054624080658 validation: auc: 0.6433558271793567 --- acc: 79.2 --- loss: 0.5207582712173462\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012557674944400788\n",
            "epoch: 133 train loss: 0.1155945286154747 validation: auc: 0.6437744519295322 --- acc: 79.14999999999999 --- loss: 0.520202100276947\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012182825058698655\n",
            "epoch: 134 train loss: 0.11949287354946136 validation: auc: 0.6441120525345124 --- acc: 79.14999999999999 --- loss: 0.5196539759635925\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011915352195501328\n",
            "epoch: 135 train loss: 0.11944245547056198 validation: auc: 0.6444380782616076 --- acc: 79.14999999999999 --- loss: 0.5191082954406738\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011634428054094315\n",
            "epoch: 136 train loss: 0.12015004456043243 validation: auc: 0.6446464260635384 --- acc: 79.10000000000001 --- loss: 0.5185753107070923\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011433649063110351\n",
            "epoch: 137 train loss: 0.11796318739652634 validation: auc: 0.6449377271569785 --- acc: 79.10000000000001 --- loss: 0.5180697441101074\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011528322100639343\n",
            "epoch: 138 train loss: 0.1042066290974617 validation: auc: 0.6451615081294225 --- acc: 79.05 --- loss: 0.5175831913948059\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011142181605100632\n",
            "epoch: 139 train loss: 0.1099410131573677 validation: auc: 0.6454692069665331 --- acc: 79.0 --- loss: 0.5171109437942505\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011006501317024232\n",
            "epoch: 140 train loss: 0.10592139512300491 validation: auc: 0.645780764096272 --- acc: 79.05 --- loss: 0.5166448950767517\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010591223090887069\n",
            "epoch: 141 train loss: 0.11324559897184372 validation: auc: 0.6461222229938808 --- acc: 79.0 --- loss: 0.5161795616149902\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010862104594707489\n",
            "epoch: 142 train loss: 0.09358479827642441 validation: auc: 0.6463980909168076 --- acc: 79.0 --- loss: 0.5157040357589722\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010417581349611283\n",
            "epoch: 143 train loss: 0.10260972380638123 validation: auc: 0.6466807108518339 --- acc: 79.10000000000001 --- loss: 0.5152163505554199\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010216910392045975\n",
            "epoch: 144 train loss: 0.1020997166633606 validation: auc: 0.6469382518847759 --- acc: 79.10000000000001 --- loss: 0.5147497653961182\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010080583393573761\n",
            "epoch: 145 train loss: 0.09934220463037491 validation: auc: 0.6472083323687602 --- acc: 79.05 --- loss: 0.5143000483512878\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009713655710220337\n",
            "epoch: 146 train loss: 0.10593744367361069 validation: auc: 0.6474475465117178 --- acc: 79.10000000000001 --- loss: 0.5138712525367737\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009780807793140412\n",
            "epoch: 147 train loss: 0.0954354777932167 validation: auc: 0.6475594369979396 --- acc: 79.10000000000001 --- loss: 0.5134397149085999\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009642121195793153\n",
            "epoch: 148 train loss: 0.09334400296211243 validation: auc: 0.6478237300429813 --- acc: 79.10000000000001 --- loss: 0.5129984617233276\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009546871483325958\n",
            "epoch: 149 train loss: 0.08965110778808594 validation: auc: 0.6480127863817703 --- acc: 79.05 --- loss: 0.5125539302825928\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009246230870485306\n",
            "epoch: 150 train loss: 0.09439120441675186 validation: auc: 0.6482462130857853 --- acc: 79.05 --- loss: 0.5121208429336548\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009231890738010406\n",
            "epoch: 151 train loss: 0.08788836747407913 validation: auc: 0.6485490890571105 --- acc: 79.14999999999999 --- loss: 0.5117175579071045\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008883441239595414\n",
            "epoch: 152 train loss: 0.09487079828977585 validation: auc: 0.6488365318579222 --- acc: 79.14999999999999 --- loss: 0.5113419890403748\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008835957199335099\n",
            "epoch: 153 train loss: 0.09000787138938904 validation: auc: 0.6490834625861364 --- acc: 79.14999999999999 --- loss: 0.5109943747520447\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008909375220537186\n",
            "epoch: 154 train loss: 0.08052472770214081 validation: auc: 0.6493246058754081 --- acc: 79.14999999999999 --- loss: 0.5106474161148071\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00853789746761322\n",
            "epoch: 155 train loss: 0.08888107538223267 validation: auc: 0.6495753948962505 --- acc: 79.14999999999999 --- loss: 0.5102857351303101\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008436382561922074\n",
            "epoch: 156 train loss: 0.08673419803380966 validation: auc: 0.6498435462339205 --- acc: 79.2 --- loss: 0.5099339485168457\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00837309956550598\n",
            "epoch: 157 train loss: 0.08316056430339813 validation: auc: 0.6501001226937055 --- acc: 79.3 --- loss: 0.5095924139022827\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008223380893468857\n",
            "epoch: 158 train loss: 0.08325802534818649 validation: auc: 0.650404927811345 --- acc: 79.35 --- loss: 0.5092572569847107\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008122345060110092\n",
            "epoch: 159 train loss: 0.08151131868362427 validation: auc: 0.6506981580510992 --- acc: 79.35 --- loss: 0.5089361071586609\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008076539635658265\n",
            "epoch: 160 train loss: 0.07771696150302887 validation: auc: 0.6510261129245086 --- acc: 79.35 --- loss: 0.5086236000061035\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007944447547197342\n",
            "epoch: 161 train loss: 0.0775301530957222 validation: auc: 0.65119780694647 --- acc: 79.35 --- loss: 0.5083115696907043\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007861699908971786\n",
            "epoch: 162 train loss: 0.07549642026424408 validation: auc: 0.6514350919431133 --- acc: 79.4 --- loss: 0.5079987645149231\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007863765954971314\n",
            "epoch: 163 train loss: 0.07025983929634094 validation: auc: 0.6516820226713274 --- acc: 79.45 --- loss: 0.5076848864555359\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007700765877962113\n",
            "epoch: 164 train loss: 0.07165411114692688 validation: auc: 0.6518749373027448 --- acc: 79.45 --- loss: 0.5073803663253784\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007510195672512055\n",
            "epoch: 165 train loss: 0.07438293099403381 validation: auc: 0.6520466313247063 --- acc: 79.4 --- loss: 0.5070667266845703\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007399200648069382\n",
            "epoch: 166 train loss: 0.07399464398622513 validation: auc: 0.6522646248582078 --- acc: 79.5 --- loss: 0.5067657828330994\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007341508567333221\n",
            "epoch: 167 train loss: 0.07166676968336105 validation: auc: 0.6525038390011652 --- acc: 79.45 --- loss: 0.5064719915390015\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007256931066513062\n",
            "epoch: 168 train loss: 0.0704919621348381 validation: auc: 0.6526784267425978 --- acc: 79.4 --- loss: 0.5061709880828857\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007221651077270508\n",
            "epoch: 169 train loss: 0.06743798404932022 validation: auc: 0.6528028566798619 --- acc: 79.45 --- loss: 0.5058590769767761\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007013342529535294\n",
            "epoch: 170 train loss: 0.07143605500459671 validation: auc: 0.6529417552144825 --- acc: 79.5 --- loss: 0.5055434107780457\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00691201388835907\n",
            "epoch: 171 train loss: 0.07126117497682571 validation: auc: 0.6531404572848423 --- acc: 79.55 --- loss: 0.5052385926246643\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006916137039661407\n",
            "epoch: 172 train loss: 0.06693991273641586 validation: auc: 0.6532851432584054 --- acc: 79.55 --- loss: 0.5049446225166321\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006815216690301895\n",
            "epoch: 173 train loss: 0.06695146858692169 validation: auc: 0.6534761287435084 --- acc: 79.60000000000001 --- loss: 0.5046626925468445\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006633306294679642\n",
            "epoch: 174 train loss: 0.07026681303977966 validation: auc: 0.6536420353265273 --- acc: 79.65 --- loss: 0.5043861269950867\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006702442467212677\n",
            "epoch: 175 train loss: 0.06362518668174744 validation: auc: 0.6537172720327801 --- acc: 79.60000000000001 --- loss: 0.5041065216064453\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006584654748439789\n",
            "epoch: 176 train loss: 0.0645485371351242 validation: auc: 0.6539005409326266 --- acc: 79.60000000000001 --- loss: 0.5038286447525024\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0063537262380123135\n",
            "epoch: 177 train loss: 0.07003294676542282 validation: auc: 0.6541243219050705 --- acc: 79.65 --- loss: 0.5035492777824402\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006388700753450394\n",
            "epoch: 178 train loss: 0.0650586187839508 validation: auc: 0.6543095199512312 --- acc: 79.65 --- loss: 0.5032582879066467\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006310766935348511\n",
            "epoch: 179 train loss: 0.06462190300226212 validation: auc: 0.6545043637289626 --- acc: 79.7 --- loss: 0.5029745697975159\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0063998490571975705\n",
            "epoch: 180 train loss: 0.05760641023516655 validation: auc: 0.654708853238265 --- acc: 79.7 --- loss: 0.5027037262916565\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006249615550041198\n",
            "epoch: 181 train loss: 0.06020013988018036 validation: auc: 0.6549249176254524 --- acc: 79.7 --- loss: 0.5024285912513733\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006224865093827248\n",
            "epoch: 182 train loss: 0.05793164670467377 validation: auc: 0.6551197614031838 --- acc: 79.75 --- loss: 0.5021393299102783\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006202913448214531\n",
            "epoch: 183 train loss: 0.05558330938220024 validation: auc: 0.6552490142062336 --- acc: 79.75 --- loss: 0.5018383860588074\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005996137484908104\n",
            "epoch: 184 train loss: 0.060708023607730865 validation: auc: 0.6554399996913366 --- acc: 79.80000000000001 --- loss: 0.5015283226966858\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005921361967921257\n",
            "epoch: 185 train loss: 0.06065305322408676 validation: auc: 0.6556059062743554 --- acc: 79.9 --- loss: 0.5012155175209045\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00577663891017437\n",
            "epoch: 186 train loss: 0.06343559920787811 validation: auc: 0.6557814585889452 --- acc: 80.0 --- loss: 0.5009073615074158\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006002373993396759\n",
            "epoch: 187 train loss: 0.051504164934158325 validation: auc: 0.655935790294079 --- acc: 80.05 --- loss: 0.5005940198898315\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005888479202985764\n",
            "epoch: 188 train loss: 0.053193043917417526 validation: auc: 0.6561123071818259 --- acc: 80.0 --- loss: 0.5002734661102295\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005909725278615952\n",
            "epoch: 189 train loss: 0.049541059881448746 validation: auc: 0.6562888240695728 --- acc: 80.05 --- loss: 0.49996089935302734\n",
            "<Timer(Thread-9639, started 140341906761472)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005578824877738952\n",
            "epoch: 190 train loss: 0.060015711933374405 validation: auc: 0.6563929979705381 --- acc: 80.10000000000001 --- loss: 0.4996577799320221\n",
            "time up, break\n",
            "test auc: 0.6322451747579382 test acc: 79.14999999999999 test loss 0.5399757623672485\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1095163345336914\n",
            "epoch: 0 train loss: 1.0781183242797852 validation: auc: 0.4537438472985348 --- acc: 59.45 --- loss: 1.1219607591629028\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10827969312667847\n",
            "epoch: 1 train loss: 1.0613391399383545 validation: auc: 0.454189202724359 --- acc: 60.0 --- loss: 1.1102453470230103\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10661919116973877\n",
            "epoch: 2 train loss: 1.0624070167541504 validation: auc: 0.45465959821428575 --- acc: 60.45 --- loss: 1.0983128547668457\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10563678741455078\n",
            "epoch: 3 train loss: 1.0344569683074951 validation: auc: 0.4551639766483517 --- acc: 60.75000000000001 --- loss: 1.0859993696212769\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10338343381881714\n",
            "epoch: 4 train loss: 1.0546784400939941 validation: auc: 0.4557398981227106 --- acc: 61.3 --- loss: 1.0732134580612183\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10231763124465942\n",
            "epoch: 5 train loss: 1.0246453285217285 validation: auc: 0.4563980940934066 --- acc: 62.050000000000004 --- loss: 1.0599570274353027\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10226649045944214\n",
            "epoch: 6 train loss: 0.9494081139564514 validation: auc: 0.4572208390567766 --- acc: 62.64999999999999 --- loss: 1.0462149381637573\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09952845573425292\n",
            "epoch: 7 train loss: 0.978451669216156 validation: auc: 0.458079355540293 --- acc: 63.3 --- loss: 1.0319663286209106\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09716325998306274\n",
            "epoch: 8 train loss: 0.9900577068328857 validation: auc: 0.45917932978479853 --- acc: 64.1 --- loss: 1.0173035860061646\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09581009745597839\n",
            "epoch: 9 train loss: 0.9572091102600098 validation: auc: 0.46014516082875456 --- acc: 64.5 --- loss: 1.0023261308670044\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09435513019561767\n",
            "epoch: 10 train loss: 0.9261684417724609 validation: auc: 0.4614222040979854 --- acc: 65.2 --- loss: 0.9871202707290649\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09135770201683044\n",
            "epoch: 11 train loss: 0.9545964598655701 validation: auc: 0.46292639652014655 --- acc: 66.2 --- loss: 0.9718000292778015\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08946966528892517\n",
            "epoch: 12 train loss: 0.9372278451919556 validation: auc: 0.4645754635989011 --- acc: 66.7 --- loss: 0.9564793109893799\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08947164416313172\n",
            "epoch: 13 train loss: 0.8437309861183167 validation: auc: 0.46662874885531136 --- acc: 67.7 --- loss: 0.9412927031517029\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08646392822265625\n",
            "epoch: 14 train loss: 0.8695052266120911 validation: auc: 0.46882333161630035 --- acc: 68.30000000000001 --- loss: 0.9263640642166138\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08404870629310608\n",
            "epoch: 15 train loss: 0.8731457591056824 validation: auc: 0.4711359603937729 --- acc: 69.3 --- loss: 0.9117841720581055\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08314941525459289\n",
            "epoch: 16 train loss: 0.8160290718078613 validation: auc: 0.47385817307692313 --- acc: 70.05 --- loss: 0.8976671099662781\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08122966885566711\n",
            "epoch: 17 train loss: 0.8019053936004639 validation: auc: 0.4769613524496337 --- acc: 70.5 --- loss: 0.8840635418891907\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07997035384178161\n",
            "epoch: 18 train loss: 0.7622926831245422 validation: auc: 0.480155749198718 --- acc: 71.15 --- loss: 0.8709955215454102\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07727657556533814\n",
            "epoch: 19 train loss: 0.7826981544494629 validation: auc: 0.4835236378205129 --- acc: 72.0 --- loss: 0.8584704995155334\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.075779390335083\n",
            "epoch: 20 train loss: 0.7570931315422058 validation: auc: 0.4872206244276557 --- acc: 72.95 --- loss: 0.8465164303779602\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07434440851211548\n",
            "epoch: 21 train loss: 0.7315437197685242 validation: auc: 0.49118321457188646 --- acc: 74.0 --- loss: 0.8350989818572998\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07202875018119811\n",
            "epoch: 22 train loss: 0.7419103980064392 validation: auc: 0.49525669642857145 --- acc: 74.6 --- loss: 0.8241650462150574\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0711656928062439\n",
            "epoch: 23 train loss: 0.6961825489997864 validation: auc: 0.4997478107829671 --- acc: 75.3 --- loss: 0.8136163353919983\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06960351467132568\n",
            "epoch: 24 train loss: 0.6799585223197937 validation: auc: 0.5043981084020146 --- acc: 75.7 --- loss: 0.8033742308616638\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0681119680404663\n",
            "epoch: 25 train loss: 0.6611670851707458 validation: auc: 0.509046617445055 --- acc: 75.8 --- loss: 0.7934058308601379\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0663414478302002\n",
            "epoch: 26 train loss: 0.6536603569984436 validation: auc: 0.5137630923763736 --- acc: 75.85 --- loss: 0.7836788296699524\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06523276567459106\n",
            "epoch: 27 train loss: 0.6201989054679871 validation: auc: 0.5185779389880952 --- acc: 76.05 --- loss: 0.7741653919219971\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06306694149971008\n",
            "epoch: 28 train loss: 0.6291458606719971 validation: auc: 0.5233355511675825 --- acc: 76.05 --- loss: 0.764854907989502\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06172411441802979\n",
            "epoch: 29 train loss: 0.6053621172904968 validation: auc: 0.5281199919871795 --- acc: 76.1 --- loss: 0.7557595372200012\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06126101016998291\n",
            "epoch: 30 train loss: 0.5470224618911743 validation: auc: 0.5327890696543041 --- acc: 76.25 --- loss: 0.7469300031661987\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05959458947181702\n",
            "epoch: 31 train loss: 0.536482572555542 validation: auc: 0.537273923992674 --- acc: 76.25 --- loss: 0.7384316325187683\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05679989457130432\n",
            "epoch: 32 train loss: 0.5719162821769714 validation: auc: 0.5417006496108058 --- acc: 76.1 --- loss: 0.7302808165550232\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05506024956703186\n",
            "epoch: 33 train loss: 0.5660310387611389 validation: auc: 0.5462042839972527 --- acc: 76.0 --- loss: 0.7224708199501038\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.053086656332015994\n",
            "epoch: 34 train loss: 0.570034921169281 validation: auc: 0.550452152014652 --- acc: 76.05 --- loss: 0.7149595022201538\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05238440036773682\n",
            "epoch: 35 train loss: 0.5247084498405457 validation: auc: 0.5544335222069596 --- acc: 76.1 --- loss: 0.7077545523643494\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05044298768043518\n",
            "epoch: 36 train loss: 0.5292027592658997 validation: auc: 0.5580375028617216 --- acc: 76.2 --- loss: 0.7007707357406616\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04920687079429627\n",
            "epoch: 37 train loss: 0.5066971778869629 validation: auc: 0.5617559523809524 --- acc: 76.25 --- loss: 0.6939798593521118\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04789612591266632\n",
            "epoch: 38 train loss: 0.4879392981529236 validation: auc: 0.5654368418040293 --- acc: 76.5 --- loss: 0.6872991323471069\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04702163338661194\n",
            "epoch: 39 train loss: 0.4520450830459595 validation: auc: 0.5685328668727107 --- acc: 76.4 --- loss: 0.6806337833404541\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044893547892570496\n",
            "epoch: 40 train loss: 0.46643057465553284 validation: auc: 0.5714875944368132 --- acc: 76.3 --- loss: 0.6739130020141602\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044182044267654416\n",
            "epoch: 41 train loss: 0.4251483976840973 validation: auc: 0.574361836080586 --- acc: 76.35 --- loss: 0.6672506332397461\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04270606935024261\n",
            "epoch: 42 train loss: 0.41464439034461975 validation: auc: 0.5769427512591575 --- acc: 76.64999999999999 --- loss: 0.660660982131958\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.041173535585403445\n",
            "epoch: 43 train loss: 0.40739497542381287 validation: auc: 0.5793197687728937 --- acc: 76.44999999999999 --- loss: 0.6542432904243469\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03975221514701843\n",
            "epoch: 44 train loss: 0.39657527208328247 validation: auc: 0.5813462253891942 --- acc: 76.3 --- loss: 0.6480022668838501\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.039176589250564574\n",
            "epoch: 45 train loss: 0.3538721799850464 validation: auc: 0.58349609375 --- acc: 76.44999999999999 --- loss: 0.6419059634208679\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03725910186767578\n",
            "epoch: 46 train loss: 0.3651532232761383 validation: auc: 0.5857640081272893 --- acc: 76.44999999999999 --- loss: 0.6360491514205933\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.036210402846336365\n",
            "epoch: 47 train loss: 0.343621164560318 validation: auc: 0.5879585908882784 --- acc: 76.7 --- loss: 0.6305093765258789\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03448150753974914\n",
            "epoch: 48 train loss: 0.35085731744766235 validation: auc: 0.5899009844322345 --- acc: 76.85 --- loss: 0.62528395652771\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03320684731006622\n",
            "epoch: 49 train loss: 0.3412211239337921 validation: auc: 0.5920186584249084 --- acc: 76.9 --- loss: 0.6203948855400085\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03264394402503967\n",
            "epoch: 50 train loss: 0.30448657274246216 validation: auc: 0.5942203954899268 --- acc: 76.9 --- loss: 0.615788459777832\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0313256174325943\n",
            "epoch: 51 train loss: 0.29934656620025635 validation: auc: 0.5963899381868132 --- acc: 76.95 --- loss: 0.611458420753479\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.030563005805015565\n",
            "epoch: 52 train loss: 0.27393218874931335 validation: auc: 0.598335908882784 --- acc: 76.8 --- loss: 0.6073912382125854\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.029225647449493408\n",
            "epoch: 53 train loss: 0.27311021089553833 validation: auc: 0.600503663003663 --- acc: 76.7 --- loss: 0.6036037802696228\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02801832854747772\n",
            "epoch: 54 train loss: 0.26854756474494934 validation: auc: 0.6028145032051281 --- acc: 76.4 --- loss: 0.6001404523849487\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.026970550417900085\n",
            "epoch: 55 train loss: 0.260011225938797 validation: auc: 0.604733645260989 --- acc: 76.35 --- loss: 0.5969107151031494\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.025766268372535706\n",
            "epoch: 56 train loss: 0.2591334581375122 validation: auc: 0.6066313244047619 --- acc: 76.35 --- loss: 0.5938360691070557\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02485036700963974\n",
            "epoch: 57 train loss: 0.24855294823646545 validation: auc: 0.6083233173076923 --- acc: 76.35 --- loss: 0.5908439755439758\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02415786385536194\n",
            "epoch: 58 train loss: 0.23104137182235718 validation: auc: 0.6102084764194139 --- acc: 76.44999999999999 --- loss: 0.5879473686218262\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.023180225491523744\n",
            "epoch: 59 train loss: 0.2263597995042801 validation: auc: 0.6118414463141025 --- acc: 76.2 --- loss: 0.5851753354072571\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.022278855741024017\n",
            "epoch: 60 train loss: 0.22016587853431702 validation: auc: 0.6132597870879121 --- acc: 76.25 --- loss: 0.5824989080429077\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021773220598697664\n",
            "epoch: 61 train loss: 0.19997188448905945 validation: auc: 0.6147836538461539 --- acc: 76.3 --- loss: 0.5799584984779358\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020964783430099488\n",
            "epoch: 62 train loss: 0.19358178973197937 validation: auc: 0.6161698002518315 --- acc: 76.3 --- loss: 0.577569842338562\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02003466784954071\n",
            "epoch: 63 train loss: 0.19367341697216034 validation: auc: 0.6176131810897436 --- acc: 76.25 --- loss: 0.5753571391105652\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019405321776866914\n",
            "epoch: 64 train loss: 0.18339495360851288 validation: auc: 0.6188508756868132 --- acc: 76.3 --- loss: 0.5732922554016113\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01834130138158798\n",
            "epoch: 65 train loss: 0.19178317487239838 validation: auc: 0.6201261303800367 --- acc: 76.4 --- loss: 0.5713891983032227\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017805622518062593\n",
            "epoch: 66 train loss: 0.18091616034507751 validation: auc: 0.6215820312500001 --- acc: 76.35 --- loss: 0.5696795582771301\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017233337461948394\n",
            "epoch: 67 train loss: 0.17268288135528564 validation: auc: 0.6228894803113553 --- acc: 76.3 --- loss: 0.5681481957435608\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016782838106155395\n",
            "epoch: 68 train loss: 0.16094841063022614 validation: auc: 0.6241379063644689 --- acc: 76.3 --- loss: 0.5667372941970825\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.016288059949874877\n",
            "epoch: 69 train loss: 0.15214547514915466 validation: auc: 0.6253559266254578 --- acc: 76.25 --- loss: 0.5653570294380188\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015725953876972197\n",
            "epoch: 70 train loss: 0.1473727822303772 validation: auc: 0.6264863066620878 --- acc: 76.44999999999999 --- loss: 0.5639461874961853\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014918400347232819\n",
            "epoch: 71 train loss: 0.15354782342910767 validation: auc: 0.6274753891941393 --- acc: 76.44999999999999 --- loss: 0.5627192258834839\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.014671501517295838\n",
            "epoch: 72 train loss: 0.1385120302438736 validation: auc: 0.6281943967490842 --- acc: 76.4 --- loss: 0.5616777539253235\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013869455456733704\n",
            "epoch: 73 train loss: 0.14674586057662964 validation: auc: 0.6291673820970696 --- acc: 76.35 --- loss: 0.5606760382652283\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013652601838111877\n",
            "epoch: 74 train loss: 0.1327057033777237 validation: auc: 0.6300044356684982 --- acc: 76.4 --- loss: 0.559697151184082\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013237524032592773\n",
            "epoch: 75 train loss: 0.127492293715477 validation: auc: 0.6307288089514652 --- acc: 76.35 --- loss: 0.5586603283882141\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01280810683965683\n",
            "epoch: 76 train loss: 0.12372249364852905 validation: auc: 0.6315032623626374 --- acc: 76.44999999999999 --- loss: 0.5576152801513672\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01261812299489975\n",
            "epoch: 77 train loss: 0.11141818016767502 validation: auc: 0.6322365785256411 --- acc: 76.5 --- loss: 0.5565980672836304\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011975499987602233\n",
            "epoch: 78 train loss: 0.11804340034723282 validation: auc: 0.6329645289606227 --- acc: 76.44999999999999 --- loss: 0.5555993318557739\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011642701923847198\n",
            "epoch: 79 train loss: 0.11324556171894073 validation: auc: 0.6335869534111722 --- acc: 76.44999999999999 --- loss: 0.5546104907989502\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011316715180873871\n",
            "epoch: 80 train loss: 0.10887999832630157 validation: auc: 0.6343399439102564 --- acc: 76.7 --- loss: 0.553738534450531\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010830987989902497\n",
            "epoch: 81 train loss: 0.11181299388408661 validation: auc: 0.6350356999771063 --- acc: 76.6 --- loss: 0.5529332756996155\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010800892114639282\n",
            "epoch: 82 train loss: 0.09722024202346802 validation: auc: 0.6355221926510989 --- acc: 76.55 --- loss: 0.5521048307418823\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010265160351991653\n",
            "epoch: 83 train loss: 0.1035080999135971 validation: auc: 0.6360068967490842 --- acc: 76.7 --- loss: 0.551268458366394\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010046207159757615\n",
            "epoch: 84 train loss: 0.09792681038379669 validation: auc: 0.6365416809752747 --- acc: 76.75 --- loss: 0.5505220293998718\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009641987830400467\n",
            "epoch: 85 train loss: 0.10030379891395569 validation: auc: 0.6371104481456045 --- acc: 76.8 --- loss: 0.5499744415283203\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009554468095302582\n",
            "epoch: 86 train loss: 0.09070240706205368 validation: auc: 0.6376810038919415 --- acc: 76.6 --- loss: 0.5495492219924927\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00921403467655182\n",
            "epoch: 87 train loss: 0.09171482175588608 validation: auc: 0.6381388793498168 --- acc: 76.64999999999999 --- loss: 0.5491418242454529\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009012407064437867\n",
            "epoch: 88 train loss: 0.08773592859506607 validation: auc: 0.6386271605998168 --- acc: 76.75 --- loss: 0.5486598610877991\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008741988986730575\n",
            "epoch: 89 train loss: 0.0869549959897995 validation: auc: 0.6390492645375458 --- acc: 76.7 --- loss: 0.5481042861938477\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008542871475219727\n",
            "epoch: 90 train loss: 0.08395744115114212 validation: auc: 0.6395357572115385 --- acc: 76.64999999999999 --- loss: 0.5475450158119202\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008176721632480621\n",
            "epoch: 91 train loss: 0.08799724280834198 validation: auc: 0.6399578611492673 --- acc: 76.7 --- loss: 0.5470737218856812\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008009979128837585\n",
            "epoch: 92 train loss: 0.08453185856342316 validation: auc: 0.6402941134386447 --- acc: 76.8 --- loss: 0.5466635823249817\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007922003418207169\n",
            "epoch: 93 train loss: 0.07840101420879364 validation: auc: 0.6408306862408425 --- acc: 76.9 --- loss: 0.5462907552719116\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007721789181232452\n",
            "epoch: 94 train loss: 0.0771171823143959 validation: auc: 0.6412581559065934 --- acc: 76.9 --- loss: 0.5459548830986023\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0074659988284111025\n",
            "epoch: 95 train loss: 0.07844719290733337 validation: auc: 0.6417464371565934 --- acc: 76.9 --- loss: 0.5456099510192871\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00745403915643692\n",
            "epoch: 96 train loss: 0.07050575315952301 validation: auc: 0.6421256152701466 --- acc: 77.0 --- loss: 0.5452326536178589\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0072663046419620516\n",
            "epoch: 97 train loss: 0.0698634460568428 validation: auc: 0.642309838598901 --- acc: 77.10000000000001 --- loss: 0.5449522137641907\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0072346307337284085\n",
            "epoch: 98 train loss: 0.0633283481001854 validation: auc: 0.6425620278159341 --- acc: 77.25 --- loss: 0.5447572469711304\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006891124695539474\n",
            "epoch: 99 train loss: 0.06954680383205414 validation: auc: 0.6429930746336997 --- acc: 77.3 --- loss: 0.5445340871810913\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006773222237825394\n",
            "epoch: 100 train loss: 0.06710878014564514 validation: auc: 0.6432881896749085 --- acc: 77.35 --- loss: 0.5443038940429688\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006581422686576843\n",
            "epoch: 101 train loss: 0.0678885281085968 validation: auc: 0.6435421674679487 --- acc: 77.4 --- loss: 0.5441422462463379\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006469725072383881\n",
            "epoch: 102 train loss: 0.06570718437433243 validation: auc: 0.6438569568452381 --- acc: 77.5 --- loss: 0.5439527034759521\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0063713714480400085\n",
            "epoch: 103 train loss: 0.0632769986987114 validation: auc: 0.6441127232142857 --- acc: 77.60000000000001 --- loss: 0.5437198877334595\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006210089847445488\n",
            "epoch: 104 train loss: 0.06355009973049164 validation: auc: 0.6444060496794872 --- acc: 77.60000000000001 --- loss: 0.5434131622314453\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0061710424721241\n",
            "epoch: 105 train loss: 0.05921152979135513 validation: auc: 0.6446188902243589 --- acc: 77.64999999999999 --- loss: 0.5431072115898132\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005987232550978661\n",
            "epoch: 106 train loss: 0.0608774870634079 validation: auc: 0.6448585594093407 --- acc: 77.60000000000001 --- loss: 0.5427876710891724\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005826815590262413\n",
            "epoch: 107 train loss: 0.06181970611214638 validation: auc: 0.6451751373626373 --- acc: 77.64999999999999 --- loss: 0.542466402053833\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005886153131723404\n",
            "epoch: 108 train loss: 0.05425388365983963 validation: auc: 0.645337897779304 --- acc: 77.60000000000001 --- loss: 0.5422672033309937\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00569891594350338\n",
            "epoch: 109 train loss: 0.05668588727712631 validation: auc: 0.6455865098443223 --- acc: 77.60000000000001 --- loss: 0.5421146154403687\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00564425103366375\n",
            "epoch: 110 train loss: 0.053994905203580856 validation: auc: 0.6458351219093408 --- acc: 77.7 --- loss: 0.5419661998748779\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005449707061052323\n",
            "epoch: 111 train loss: 0.05712132900953293 validation: auc: 0.6459621108058609 --- acc: 77.75 --- loss: 0.5418785214424133\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005336121469736099\n",
            "epoch: 112 train loss: 0.05710940435528755 validation: auc: 0.6462250314789377 --- acc: 77.7 --- loss: 0.5418527722358704\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0053024526685476305\n",
            "epoch: 113 train loss: 0.054128240793943405 validation: auc: 0.646418197687729 --- acc: 77.8 --- loss: 0.5418068766593933\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005286221578717232\n",
            "epoch: 114 train loss: 0.05057322606444359 validation: auc: 0.6466310382326007 --- acc: 77.8 --- loss: 0.5416578650474548\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005222047865390778\n",
            "epoch: 115 train loss: 0.04902547970414162 validation: auc: 0.6467884329212454 --- acc: 77.75 --- loss: 0.5414535999298096\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005177026242017746\n",
            "epoch: 116 train loss: 0.04690265655517578 validation: auc: 0.6469583476419414 --- acc: 77.75 --- loss: 0.5413153767585754\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004925591498613357\n",
            "epoch: 117 train loss: 0.053118932992219925 validation: auc: 0.6472981770833334 --- acc: 77.8 --- loss: 0.5412904024124146\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0048978254199028015\n",
            "epoch: 118 train loss: 0.050575703382492065 validation: auc: 0.647691663804945 --- acc: 77.8 --- loss: 0.5412431359291077\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004815036803483963\n",
            "epoch: 119 train loss: 0.05031269043684006 validation: auc: 0.6480386475503663 --- acc: 77.8 --- loss: 0.5410953164100647\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0048270680010318754\n",
            "epoch: 120 train loss: 0.04637806490063667 validation: auc: 0.6483748998397436 --- acc: 77.95 --- loss: 0.5409246683120728\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004719311743974686\n",
            "epoch: 121 train loss: 0.04737262800335884 validation: auc: 0.6487737522893773 --- acc: 77.9 --- loss: 0.540838360786438\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004665310680866242\n",
            "epoch: 122 train loss: 0.04631132632493973 validation: auc: 0.6491117931547619 --- acc: 77.9 --- loss: 0.5408242344856262\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004527045786380768\n",
            "epoch: 123 train loss: 0.04868787154555321 validation: auc: 0.6494873941163003 --- acc: 77.9 --- loss: 0.5408161878585815\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004573247954249382\n",
            "epoch: 124 train loss: 0.04378001391887665 validation: auc: 0.6497306404532968 --- acc: 77.9 --- loss: 0.5408337116241455\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004481659457087517\n",
            "epoch: 125 train loss: 0.04450482502579689 validation: auc: 0.6499864068223442 --- acc: 77.9 --- loss: 0.5408353209495544\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004427395761013031\n",
            "epoch: 126 train loss: 0.043850455433130264 validation: auc: 0.6502511160714286 --- acc: 77.9 --- loss: 0.5408177971839905\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004354608431458473\n",
            "epoch: 127 train loss: 0.043985515832901 validation: auc: 0.6504514365842491 --- acc: 77.85 --- loss: 0.5407456159591675\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0042965423315763475\n",
            "epoch: 128 train loss: 0.04365169629454613 validation: auc: 0.6506678542811355 --- acc: 77.75 --- loss: 0.5406518578529358\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004303263500332832\n",
            "epoch: 129 train loss: 0.04073472321033478 validation: auc: 0.6509343521062271 --- acc: 77.8 --- loss: 0.5404713749885559\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0042614594101905824\n",
            "epoch: 130 train loss: 0.03986078128218651 validation: auc: 0.6511561355311355 --- acc: 77.75 --- loss: 0.5402224659919739\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004108494892716408\n",
            "epoch: 131 train loss: 0.04348548501729965 validation: auc: 0.6514387305402931 --- acc: 77.8 --- loss: 0.5399788618087769\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004058603197336197\n",
            "epoch: 132 train loss: 0.0430389866232872 validation: auc: 0.6517284798534799 --- acc: 77.75 --- loss: 0.5397950410842896\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0040855899453163145\n",
            "epoch: 133 train loss: 0.039603423327207565 validation: auc: 0.6519699376144688 --- acc: 77.7 --- loss: 0.5396811962127686\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00399685725569725\n",
            "epoch: 134 train loss: 0.040816158056259155 validation: auc: 0.652272206959707 --- acc: 77.7 --- loss: 0.5395329594612122\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003967026621103287\n",
            "epoch: 135 train loss: 0.039760056883096695 validation: auc: 0.6525369162087913 --- acc: 77.75 --- loss: 0.539385974407196\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0039302479475736615\n",
            "epoch: 136 train loss: 0.03902808576822281 validation: auc: 0.6528982085622711 --- acc: 77.8 --- loss: 0.5392776727676392\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003944059461355209\n",
            "epoch: 137 train loss: 0.03637529909610748 validation: auc: 0.6531629178113554 --- acc: 77.8 --- loss: 0.5391735434532166\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003845074772834778\n",
            "epoch: 138 train loss: 0.03823242709040642 validation: auc: 0.6533686040521978 --- acc: 77.85 --- loss: 0.539114236831665\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0038108255714178084\n",
            "epoch: 139 train loss: 0.0375937856733799 validation: auc: 0.6535134787087913 --- acc: 77.9 --- loss: 0.5391460061073303\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003845781460404396\n",
            "epoch: 140 train loss: 0.034231677651405334 validation: auc: 0.6537531478937729 --- acc: 77.85 --- loss: 0.5391784906387329\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0037176933139562605\n",
            "epoch: 141 train loss: 0.03743289038538933 validation: auc: 0.6538550967261905 --- acc: 77.9 --- loss: 0.5392013788223267\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00370418056845665\n",
            "epoch: 142 train loss: 0.03611060231924057 validation: auc: 0.6540196457188645 --- acc: 77.95 --- loss: 0.5392540693283081\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0036230627447366714\n",
            "epoch: 143 train loss: 0.03750411421060562 validation: auc: 0.6542646806318682 --- acc: 77.9 --- loss: 0.5393263101577759\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0036006346344947815\n",
            "epoch: 144 train loss: 0.036605354398489 validation: auc: 0.6544220753205129 --- acc: 77.9 --- loss: 0.5394107699394226\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003551814705133438\n",
            "epoch: 145 train loss: 0.03679227456450462 validation: auc: 0.6546152415293041 --- acc: 77.85 --- loss: 0.5395146012306213\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003550964966416359\n",
            "epoch: 146 train loss: 0.035106681287288666 validation: auc: 0.6548459678342491 --- acc: 77.9 --- loss: 0.5396032929420471\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034229397773742677\n",
            "epoch: 147 train loss: 0.03851814195513725 validation: auc: 0.6550516540750916 --- acc: 78.0 --- loss: 0.5396624207496643\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003491077572107315\n",
            "epoch: 148 train loss: 0.03414957597851753 validation: auc: 0.6552573403159341 --- acc: 78.0 --- loss: 0.5396921634674072\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003454511985182762\n",
            "epoch: 149 train loss: 0.03396002575755119 validation: auc: 0.6554433522206959 --- acc: 78.0 --- loss: 0.5396555662155151\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034000247716903685\n",
            "epoch: 150 train loss: 0.03455367684364319 validation: auc: 0.6556132669413919 --- acc: 78.0 --- loss: 0.5396069288253784\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033733155578374863\n",
            "epoch: 151 train loss: 0.034040555357933044 validation: auc: 0.6558153760302198 --- acc: 78.05 --- loss: 0.5395853519439697\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033714905381202697\n",
            "epoch: 152 train loss: 0.03259320184588432 validation: auc: 0.6559584621108059 --- acc: 78.05 --- loss: 0.5395662188529968\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032724525779485704\n",
            "epoch: 153 train loss: 0.035038646310567856 validation: auc: 0.6561426854395604 --- acc: 78.05 --- loss: 0.5396014451980591\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003276378661394119\n",
            "epoch: 154 train loss: 0.033401187509298325 validation: auc: 0.6562804057921245 --- acc: 78.05 --- loss: 0.5396419763565063\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003147647529840469\n",
            "epoch: 155 train loss: 0.03710514307022095 validation: auc: 0.656469994848901 --- acc: 78.14999999999999 --- loss: 0.5396955013275146\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032105583697557448\n",
            "epoch: 156 train loss: 0.03317957744002342 validation: auc: 0.6566488524496338 --- acc: 78.14999999999999 --- loss: 0.5397710204124451\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032454300671815872\n",
            "epoch: 157 train loss: 0.03041338175535202 validation: auc: 0.6568312872023808 --- acc: 78.14999999999999 --- loss: 0.539810061454773\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003174838051199913\n",
            "epoch: 158 train loss: 0.03187775984406471 validation: auc: 0.657056647779304 --- acc: 78.2 --- loss: 0.5398136973381042\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003199953958392143\n",
            "epoch: 159 train loss: 0.02955756150186062 validation: auc: 0.6573320884844323 --- acc: 78.25 --- loss: 0.5397531390190125\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030919892713427545\n",
            "epoch: 160 train loss: 0.03255604952573776 validation: auc: 0.6574876945970696 --- acc: 78.2 --- loss: 0.5395941138267517\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031051481142640113\n",
            "epoch: 161 train loss: 0.03074529394507408 validation: auc: 0.6577023237179487 --- acc: 78.2 --- loss: 0.5393989086151123\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003059217520058155\n",
            "epoch: 162 train loss: 0.03132759779691696 validation: auc: 0.6579008556547619 --- acc: 78.2 --- loss: 0.5392410755157471\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030955439433455467\n",
            "epoch: 163 train loss: 0.028635330498218536 validation: auc: 0.6580707703754578 --- acc: 78.3 --- loss: 0.539161741733551\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030005427077412607\n",
            "epoch: 164 train loss: 0.031234361231327057 validation: auc: 0.6582156450320512 --- acc: 78.45 --- loss: 0.5391503572463989\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030304651707410814\n",
            "epoch: 165 train loss: 0.028865275904536247 validation: auc: 0.658346211080586 --- acc: 78.5 --- loss: 0.5391875505447388\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002982345037162304\n",
            "epoch: 166 train loss: 0.02959977649152279 validation: auc: 0.6584088112408426 --- acc: 78.5 --- loss: 0.539247989654541\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00292462520301342\n",
            "epoch: 167 train loss: 0.030778145417571068 validation: auc: 0.6585697830815018 --- acc: 78.55 --- loss: 0.5393463373184204\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029064927250146864\n",
            "epoch: 168 train loss: 0.03038036823272705 validation: auc: 0.6587540064102563 --- acc: 78.60000000000001 --- loss: 0.5394824743270874\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002893579006195068\n",
            "epoch: 169 train loss: 0.02978360839188099 validation: auc: 0.6588774181547619 --- acc: 78.64999999999999 --- loss: 0.5396354794502258\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002891431376338005\n",
            "epoch: 170 train loss: 0.028768837451934814 validation: auc: 0.6590026184752746 --- acc: 78.64999999999999 --- loss: 0.5397812724113464\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028525948524475097\n",
            "epoch: 171 train loss: 0.029230747371912003 validation: auc: 0.6591152987637363 --- acc: 78.75 --- loss: 0.5399601459503174\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00287237074226141\n",
            "epoch: 172 train loss: 0.027397727593779564 validation: auc: 0.6592619619963369 --- acc: 78.7 --- loss: 0.5401800274848938\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002820620127022266\n",
            "epoch: 173 train loss: 0.028403496369719505 validation: auc: 0.6593424479166667 --- acc: 78.75 --- loss: 0.5403133630752563\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028432896360754967\n",
            "epoch: 174 train loss: 0.026476187631487846 validation: auc: 0.659415779532967 --- acc: 78.8 --- loss: 0.5403868556022644\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027703072875738142\n",
            "epoch: 175 train loss: 0.028365056961774826 validation: auc: 0.6595356141254579 --- acc: 78.8 --- loss: 0.5404094457626343\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027748791500926016\n",
            "epoch: 176 train loss: 0.027195025235414505 validation: auc: 0.6596125228937728 --- acc: 78.9 --- loss: 0.5403648018836975\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00275961272418499\n",
            "epoch: 177 train loss: 0.026804747059941292 validation: auc: 0.6598289405906593 --- acc: 78.85 --- loss: 0.5403715968132019\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002754557318985462\n",
            "epoch: 178 train loss: 0.02603588066995144 validation: auc: 0.6599791809752746 --- acc: 78.85 --- loss: 0.5404356122016907\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027464313432574274\n",
            "epoch: 179 train loss: 0.025394782423973083 validation: auc: 0.6601365756639195 --- acc: 78.8 --- loss: 0.5405104160308838\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00264741200953722\n",
            "epoch: 180 train loss: 0.028391895815730095 validation: auc: 0.6603619362408425 --- acc: 78.85 --- loss: 0.5405747294425964\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002607349306344986\n",
            "epoch: 181 train loss: 0.029048623517155647 validation: auc: 0.6605890853937729 --- acc: 78.95 --- loss: 0.5406368374824524\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0026305388659238816\n",
            "epoch: 182 train loss: 0.027194242924451828 validation: auc: 0.6609199719551282 --- acc: 79.0 --- loss: 0.5406953692436218\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025138771161437035\n",
            "epoch: 183 train loss: 0.030947245657444 validation: auc: 0.6611435439560439 --- acc: 78.95 --- loss: 0.5407458543777466\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002560589276254177\n",
            "epoch: 184 train loss: 0.028178555890917778 validation: auc: 0.661385001717033 --- acc: 78.95 --- loss: 0.540771484375\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025847755372524262\n",
            "epoch: 185 train loss: 0.02632736600935459 validation: auc: 0.6614958934294871 --- acc: 78.95 --- loss: 0.5408115386962891\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025882139801979066\n",
            "epoch: 186 train loss: 0.025340111926198006 validation: auc: 0.6617069453983516 --- acc: 78.9 --- loss: 0.5409237146377563\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025649590417742727\n",
            "epoch: 187 train loss: 0.02542235143482685 validation: auc: 0.6618267799908425 --- acc: 78.95 --- loss: 0.5410485863685608\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002486804500222206\n",
            "epoch: 188 train loss: 0.027700137346982956 validation: auc: 0.6620199461996337 --- acc: 78.95 --- loss: 0.5411967635154724\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025469698011875153\n",
            "epoch: 189 train loss: 0.024481646716594696 validation: auc: 0.6622184781364469 --- acc: 79.0 --- loss: 0.5413369536399841\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025403179228305818\n",
            "epoch: 190 train loss: 0.023915622383356094 validation: auc: 0.66241701007326 --- acc: 79.0 --- loss: 0.5414360761642456\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024553075432777404\n",
            "epoch: 191 train loss: 0.026496263220906258 validation: auc: 0.6626674107142857 --- acc: 79.0 --- loss: 0.5414915680885315\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025101985782384874\n",
            "epoch: 192 train loss: 0.023472819477319717 validation: auc: 0.6628087082188645 --- acc: 79.0 --- loss: 0.5415253043174744\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024820337072014808\n",
            "epoch: 193 train loss: 0.023770621046423912 validation: auc: 0.6629875658195972 --- acc: 79.14999999999999 --- loss: 0.5414963960647583\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00239541195333004\n",
            "epoch: 194 train loss: 0.026439214125275612 validation: auc: 0.6631288633241758 --- acc: 79.10000000000001 --- loss: 0.5414272546768188\n",
            "<Timer(Thread-10023, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024185938760638236\n",
            "epoch: 195 train loss: 0.024711735546588898 validation: auc: 0.6632093492445055 --- acc: 79.3 --- loss: 0.5413737893104553\n",
            "time up, break\n",
            "test auc: 0.6683611450798892 test acc: 80.10000000000001 test loss 0.5308102965354919\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.21537578105926514\n",
            "epoch: 0 train loss: 2.1064717769622803 validation: auc: 0.4867089003924188 --- acc: 38.35 --- loss: 2.1409318447113037\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.21139211654663087\n",
            "epoch: 1 train loss: 2.0623111724853516 validation: auc: 0.48689154212240127 --- acc: 38.95 --- loss: 2.102163791656494\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.2044306755065918\n",
            "epoch: 2 train loss: 2.1303043365478516 validation: auc: 0.4870515710667669 --- acc: 39.6 --- loss: 2.0608723163604736\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.20316190719604493\n",
            "epoch: 3 train loss: 1.9580185413360596 validation: auc: 0.48734379783473875 --- acc: 40.1 --- loss: 2.016119956970215\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.19733119010925293\n",
            "epoch: 4 train loss: 1.948041558265686 validation: auc: 0.48766211627842254 --- acc: 40.699999999999996 --- loss: 1.9673129320144653\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.19261443614959717\n",
            "epoch: 5 train loss: 1.8707401752471924 validation: auc: 0.48809001976009575 --- acc: 41.199999999999996 --- loss: 1.914066195487976\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.18571364879608154\n",
            "epoch: 6 train loss: 1.8566452264785767 validation: auc: 0.4885474938075757 --- acc: 42.05 --- loss: 1.8562012910842896\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1807043194770813\n",
            "epoch: 7 train loss: 1.7419835329055786 validation: auc: 0.48920674348056 --- acc: 42.8 --- loss: 1.7937288284301758\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.17371032238006592\n",
            "epoch: 8 train loss: 1.6832104921340942 validation: auc: 0.4899790570816286 --- acc: 43.7 --- loss: 1.7268682718276978\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.16609877347946167\n",
            "epoch: 9 train loss: 1.6232272386550903 validation: auc: 0.49088878684144605 --- acc: 45.25 --- loss: 1.6559901237487793\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1592765212059021\n",
            "epoch: 10 train loss: 1.5110080242156982 validation: auc: 0.49203856001781193 --- acc: 46.550000000000004 --- loss: 1.5815659761428833\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.15065912008285523\n",
            "epoch: 11 train loss: 1.4539859294891357 validation: auc: 0.49334314380340094 --- acc: 48.199999999999996 --- loss: 1.5043648481369019\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.14134628772735597\n",
            "epoch: 12 train loss: 1.4106634855270386 validation: auc: 0.49494691213715175 --- acc: 49.55 --- loss: 1.425432562828064\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.13302016258239746\n",
            "epoch: 13 train loss: 1.317203402519226 validation: auc: 0.4966620049539395 --- acc: 51.5 --- loss: 1.3458956480026245\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1258474111557007\n",
            "epoch: 14 train loss: 1.1765190362930298 validation: auc: 0.4988206562578275 --- acc: 52.949999999999996 --- loss: 1.2669086456298828\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.11616523265838623\n",
            "epoch: 15 train loss: 1.1405396461486816 validation: auc: 0.5012941471153043 --- acc: 54.900000000000006 --- loss: 1.1898819208145142\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.1083906888961792\n",
            "epoch: 16 train loss: 1.039137363433838 validation: auc: 0.5038354763296318 --- acc: 57.9 --- loss: 1.1163007020950317\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.10067028999328613\n",
            "epoch: 17 train loss: 0.9556818604469299 validation: auc: 0.5067681806796359 --- acc: 60.199999999999996 --- loss: 1.0475994348526\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.09376211762428284\n",
            "epoch: 18 train loss: 0.8672271966934204 validation: auc: 0.5102575074448248 --- acc: 62.150000000000006 --- loss: 0.9851320385932922\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08630433678627014\n",
            "epoch: 19 train loss: 0.8338049054145813 validation: auc: 0.5144165205532827 --- acc: 64.55 --- loss: 0.9299408197402954\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.08091882467269898\n",
            "epoch: 20 train loss: 0.7557770609855652 validation: auc: 0.5188294926386685 --- acc: 66.4 --- loss: 0.8826033473014832\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0759378731250763\n",
            "epoch: 21 train loss: 0.7008960247039795 validation: auc: 0.5238043054743815 --- acc: 68.65 --- loss: 0.8432518243789673\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.07100732326507568\n",
            "epoch: 22 train loss: 0.6861771941184998 validation: auc: 0.529100915643873 --- acc: 70.35 --- loss: 0.8114625811576843\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06674635410308838\n",
            "epoch: 23 train loss: 0.6786245107650757 validation: auc: 0.5347923798391362 --- acc: 71.7 --- loss: 0.7863238453865051\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06530897617340088\n",
            "epoch: 24 train loss: 0.5933676958084106 validation: auc: 0.5405186329353483 --- acc: 73.5 --- loss: 0.7665854096412659\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.06225927472114563\n",
            "epoch: 25 train loss: 0.5928859114646912 validation: auc: 0.546554507250007 --- acc: 74.75 --- loss: 0.7509669065475464\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.059627360105514525\n",
            "epoch: 26 train loss: 0.5968328714370728 validation: auc: 0.5524581837410593 --- acc: 75.14999999999999 --- loss: 0.7381592392921448\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05813313126564026\n",
            "epoch: 27 train loss: 0.5651569962501526 validation: auc: 0.55850623417105 --- acc: 75.75 --- loss: 0.7270477414131165\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05652852058410644\n",
            "epoch: 28 train loss: 0.5429607629776001 validation: auc: 0.5639211266037684 --- acc: 76.14999999999999 --- loss: 0.7168827056884766\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05465521812438965\n",
            "epoch: 29 train loss: 0.5343482494354248 validation: auc: 0.5693481951518188 --- acc: 76.4 --- loss: 0.7072011828422546\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05239164233207703\n",
            "epoch: 30 train loss: 0.5408976674079895 validation: auc: 0.5747978764854861 --- acc: 76.7 --- loss: 0.6977202892303467\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.05129976868629456\n",
            "epoch: 31 train loss: 0.49965131282806396 validation: auc: 0.5795430825749353 --- acc: 76.9 --- loss: 0.6883769631385803\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04977443814277649\n",
            "epoch: 32 train loss: 0.47384098172187805 validation: auc: 0.5841960980768696 --- acc: 77.05 --- loss: 0.6792495846748352\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.047373372316360476\n",
            "epoch: 33 train loss: 0.4827573001384735 validation: auc: 0.5886473379532994 --- acc: 77.0 --- loss: 0.6704518795013428\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.045925503969192503\n",
            "epoch: 34 train loss: 0.4548541307449341 validation: auc: 0.5929611616709805 --- acc: 76.85 --- loss: 0.6621523499488831\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.044059920310974124\n",
            "epoch: 35 train loss: 0.44346752762794495 validation: auc: 0.5966887924076703 --- acc: 76.44999999999999 --- loss: 0.6544626355171204\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04288151264190674\n",
            "epoch: 36 train loss: 0.408203661441803 validation: auc: 0.6002250841891403 --- acc: 76.44999999999999 --- loss: 0.6473959684371948\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04092307388782501\n",
            "epoch: 37 train loss: 0.40543556213378906 validation: auc: 0.6035491636748211 --- acc: 76.1 --- loss: 0.640975296497345\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.04041686356067657\n",
            "epoch: 38 train loss: 0.3484967052936554 validation: auc: 0.6064088113328323 --- acc: 76.2 --- loss: 0.635161280632019\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.037039855122566225\n",
            "epoch: 39 train loss: 0.4067353904247284 validation: auc: 0.6090875567059084 --- acc: 76.4 --- loss: 0.62986159324646\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0357848197221756\n",
            "epoch: 40 train loss: 0.38387611508369446 validation: auc: 0.6116236675850936 --- acc: 76.2 --- loss: 0.6249813437461853\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03555326759815216\n",
            "epoch: 41 train loss: 0.3234870135784149 validation: auc: 0.6139092983774457 --- acc: 76.0 --- loss: 0.6204594969749451\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03398958444595337\n",
            "epoch: 42 train loss: 0.31651732325553894 validation: auc: 0.6160244635551473 --- acc: 76.14999999999999 --- loss: 0.6161598563194275\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.03227424621582031\n",
            "epoch: 43 train loss: 0.3176690936088562 validation: auc: 0.617958726447914 --- acc: 76.1 --- loss: 0.6120561361312866\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.031051024794578552\n",
            "epoch: 44 train loss: 0.30159321427345276 validation: auc: 0.619859939884779 --- acc: 75.94999999999999 --- loss: 0.6081146597862244\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02979838252067566\n",
            "epoch: 45 train loss: 0.2880185842514038 validation: auc: 0.6215332860204279 --- acc: 75.6 --- loss: 0.6042912602424622\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02852921485900879\n",
            "epoch: 46 train loss: 0.27688759565353394 validation: auc: 0.6235005983690963 --- acc: 75.5 --- loss: 0.6005080938339233\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.026829558610916137\n",
            "epoch: 47 train loss: 0.28449639678001404 validation: auc: 0.6252922267679719 --- acc: 75.55 --- loss: 0.5968099236488342\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.025994521379470826\n",
            "epoch: 48 train loss: 0.2599871754646301 validation: auc: 0.6270055801397122 --- acc: 75.5 --- loss: 0.5932955741882324\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.025082722306251526\n",
            "epoch: 49 train loss: 0.24015672504901886 validation: auc: 0.6287502435223066 --- acc: 75.5 --- loss: 0.5899233222007751\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.02399199903011322\n",
            "epoch: 50 train loss: 0.22909444570541382 validation: auc: 0.6302653001586374 --- acc: 75.6 --- loss: 0.586645245552063\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.022920742630958557\n",
            "epoch: 51 train loss: 0.21972976624965668 validation: auc: 0.6318447162617239 --- acc: 75.9 --- loss: 0.5834935307502747\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.021969257295131682\n",
            "epoch: 52 train loss: 0.20749343931674957 validation: auc: 0.6333354206673902 --- acc: 75.75 --- loss: 0.5805127024650574\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.020958714187145233\n",
            "epoch: 53 train loss: 0.19940128922462463 validation: auc: 0.6350331190337034 --- acc: 75.75 --- loss: 0.5777105689048767\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01990949660539627\n",
            "epoch: 54 train loss: 0.19515886902809143 validation: auc: 0.6365864434610782 --- acc: 76.1 --- loss: 0.5751162767410278\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.019266705214977264\n",
            "epoch: 55 train loss: 0.17679931223392487 validation: auc: 0.6378858089115249 --- acc: 76.4 --- loss: 0.5727181434631348\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017861256003379823\n",
            "epoch: 56 train loss: 0.1902676373720169 validation: auc: 0.6394652250146113 --- acc: 76.55 --- loss: 0.5704697966575623\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.017535041272640228\n",
            "epoch: 57 train loss: 0.1634313464164734 validation: auc: 0.6406323926414517 --- acc: 76.6 --- loss: 0.5683614611625671\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01672731637954712\n",
            "epoch: 58 train loss: 0.15722882747650146 validation: auc: 0.6421161392669283 --- acc: 76.64999999999999 --- loss: 0.5663928389549255\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015922468900680543\n",
            "epoch: 59 train loss: 0.15254196524620056 validation: auc: 0.6433180957947177 --- acc: 76.75 --- loss: 0.5645871758460999\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.015210515260696411\n",
            "epoch: 60 train loss: 0.14652621746063232 validation: auc: 0.6443478472628092 --- acc: 76.55 --- loss: 0.562970757484436\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.01417524814605713\n",
            "epoch: 61 train loss: 0.15464214980602264 validation: auc: 0.645304542038908 --- acc: 76.55 --- loss: 0.5615168809890747\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013840016722679139\n",
            "epoch: 62 train loss: 0.13702364265918732 validation: auc: 0.6463082018312878 --- acc: 76.5 --- loss: 0.5601957440376282\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.013276311755180358\n",
            "epoch: 63 train loss: 0.1298859864473343 validation: auc: 0.6472666360524338 --- acc: 76.7 --- loss: 0.5589765906333923\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012413978576660156\n",
            "epoch: 64 train loss: 0.13612616062164307 validation: auc: 0.6481850630374885 --- acc: 76.64999999999999 --- loss: 0.5578504204750061\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.012216699868440628\n",
            "epoch: 65 train loss: 0.11785608530044556 validation: auc: 0.6492409061812919 --- acc: 76.5 --- loss: 0.5567967295646667\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011724727600812912\n",
            "epoch: 66 train loss: 0.11253681033849716 validation: auc: 0.6499488603156048 --- acc: 76.5 --- loss: 0.5558328628540039\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.011330968141555786\n",
            "epoch: 67 train loss: 0.10471627116203308 validation: auc: 0.6507559628176227 --- acc: 76.44999999999999 --- loss: 0.554949939250946\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010824166238307953\n",
            "epoch: 68 train loss: 0.10291855782270432 validation: auc: 0.6514012969302273 --- acc: 76.55 --- loss: 0.5541077852249146\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.010458259284496308\n",
            "epoch: 69 train loss: 0.09674599766731262 validation: auc: 0.652069243828449 --- acc: 76.64999999999999 --- loss: 0.5533286333084106\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009904582053422928\n",
            "epoch: 70 train loss: 0.09913323074579239 validation: auc: 0.6527006623806741 --- acc: 76.6 --- loss: 0.5526014566421509\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.009613423049449921\n",
            "epoch: 71 train loss: 0.09252044558525085 validation: auc: 0.6533303414878517 --- acc: 76.64999999999999 --- loss: 0.5518987774848938\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00930427834391594\n",
            "epoch: 72 train loss: 0.08751370757818222 validation: auc: 0.6538452172218976 --- acc: 76.64999999999999 --- loss: 0.5512118339538574\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00889008715748787\n",
            "epoch: 73 train loss: 0.08768533915281296 validation: auc: 0.6544922907795496 --- acc: 76.64999999999999 --- loss: 0.5505395531654358\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008614547550678253\n",
            "epoch: 74 train loss: 0.08333496004343033 validation: auc: 0.6551289276669171 --- acc: 76.8 --- loss: 0.5498858094215393\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008282575756311417\n",
            "epoch: 75 train loss: 0.08213318139314651 validation: auc: 0.6557151206479085 --- acc: 76.85 --- loss: 0.5492550134658813\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.008080761879682541\n",
            "epoch: 76 train loss: 0.07651077210903168 validation: auc: 0.6562212991567171 --- acc: 76.95 --- loss: 0.5486827492713928\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007837019115686416\n",
            "epoch: 77 train loss: 0.07337009161710739 validation: auc: 0.656767484901617 --- acc: 77.05 --- loss: 0.5481784343719482\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007488636672496796\n",
            "epoch: 78 train loss: 0.07513383030891418 validation: auc: 0.6573014945311848 --- acc: 77.2 --- loss: 0.5477378964424133\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007349492609500885\n",
            "epoch: 79 train loss: 0.06930684298276901 validation: auc: 0.6576772146614345 --- acc: 77.2 --- loss: 0.5473491549491882\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.007113101333379746\n",
            "epoch: 80 train loss: 0.06795816868543625 validation: auc: 0.6578842086220812 --- acc: 77.10000000000001 --- loss: 0.5470321178436279\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006948485970497131\n",
            "epoch: 81 train loss: 0.0643559917807579 validation: auc: 0.6580772870223485 --- acc: 77.10000000000001 --- loss: 0.5467672348022461\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006667640805244446\n",
            "epoch: 82 train loss: 0.06593305617570877 validation: auc: 0.6583190698839443 --- acc: 77.2 --- loss: 0.546531081199646\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006502564251422882\n",
            "epoch: 83 train loss: 0.06348409503698349 validation: auc: 0.6585347610698282 --- acc: 77.25 --- loss: 0.5463038086891174\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0062062665820121765\n",
            "epoch: 84 train loss: 0.06668411940336227 validation: auc: 0.658602599426679 --- acc: 77.25 --- loss: 0.5460949540138245\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.006064537912607193\n",
            "epoch: 85 train loss: 0.06424453109502792 validation: auc: 0.6587695861512343 --- acc: 77.2 --- loss: 0.5459359288215637\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00603070966899395\n",
            "epoch: 86 train loss: 0.05785703286528587 validation: auc: 0.6588843895243661 --- acc: 77.25 --- loss: 0.5457856059074402\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0057958349585533146\n",
            "epoch: 87 train loss: 0.059945497661828995 validation: auc: 0.6590826862597756 --- acc: 77.10000000000001 --- loss: 0.5456481575965881\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005633090063929558\n",
            "epoch: 88 train loss: 0.05945703014731407 validation: auc: 0.6592775041050902 --- acc: 77.10000000000001 --- loss: 0.5455490946769714\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005569685623049736\n",
            "epoch: 89 train loss: 0.05538380518555641 validation: auc: 0.6594653641702152 --- acc: 77.14999999999999 --- loss: 0.5454819798469543\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0055132627487182615\n",
            "epoch: 90 train loss: 0.05136628448963165 validation: auc: 0.6595732097631573 --- acc: 77.14999999999999 --- loss: 0.5454279780387878\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005321146920323372\n",
            "epoch: 91 train loss: 0.05307414010167122 validation: auc: 0.6597210625921905 --- acc: 77.14999999999999 --- loss: 0.5453810691833496\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005179391056299209\n",
            "epoch: 92 train loss: 0.0530288890004158 validation: auc: 0.6599193593276002 --- acc: 77.3 --- loss: 0.5453237295150757\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.005103740841150284\n",
            "epoch: 93 train loss: 0.05061272159218788 validation: auc: 0.6600219865853998 --- acc: 77.3 --- loss: 0.5452759265899658\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00487397275865078\n",
            "epoch: 94 train loss: 0.05463322624564171 validation: auc: 0.6601750577495755 --- acc: 77.2 --- loss: 0.545242428779602\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00486123189330101\n",
            "epoch: 95 train loss: 0.050178200006484985 validation: auc: 0.6602620300019482 --- acc: 77.3 --- loss: 0.5452049374580383\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004749921336770058\n",
            "epoch: 96 train loss: 0.0499001182615757 validation: auc: 0.6604098828309816 --- acc: 77.3 --- loss: 0.5451597571372986\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004731319099664688\n",
            "epoch: 97 train loss: 0.04611106961965561 validation: auc: 0.6606029612312488 --- acc: 77.35 --- loss: 0.5451042652130127\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004492802545428276\n",
            "epoch: 98 train loss: 0.05124375969171524 validation: auc: 0.6607595112855195 --- acc: 77.35 --- loss: 0.545015811920166\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0046144906431436535\n",
            "epoch: 99 train loss: 0.04225676506757736 validation: auc: 0.66092301911998 --- acc: 77.45 --- loss: 0.5449161529541016\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004472781717777252\n",
            "epoch: 100 train loss: 0.04390965774655342 validation: auc: 0.6611178369652946 --- acc: 77.4 --- loss: 0.5448076725006104\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004384660348296166\n",
            "epoch: 101 train loss: 0.04359782114624977 validation: auc: 0.6611891542122401 --- acc: 77.4 --- loss: 0.5446975827217102\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00429331511259079\n",
            "epoch: 102 train loss: 0.04358019679784775 validation: auc: 0.6614657259747851 --- acc: 77.55 --- loss: 0.5446045994758606\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004209065064787865\n",
            "epoch: 103 train loss: 0.04343809559941292 validation: auc: 0.6615561771172525 --- acc: 77.64999999999999 --- loss: 0.544524610042572\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004161370173096657\n",
            "epoch: 104 train loss: 0.041968923062086105 validation: auc: 0.6616935932760012 --- acc: 77.8 --- loss: 0.5444561839103699\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004103823006153107\n",
            "epoch: 105 train loss: 0.041033510118722916 validation: auc: 0.6618501433302719 --- acc: 77.95 --- loss: 0.5443857312202454\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.004072722420096398\n",
            "epoch: 106 train loss: 0.03911420330405235 validation: auc: 0.6620797500765356 --- acc: 78.0 --- loss: 0.5443081259727478\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003943921998143196\n",
            "epoch: 107 train loss: 0.041288211941719055 validation: auc: 0.6622502156911858 --- acc: 78.0 --- loss: 0.5442503690719604\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003931155800819397\n",
            "epoch: 108 train loss: 0.0388769805431366 validation: auc: 0.6624833013275445 --- acc: 78.0 --- loss: 0.5441965460777283\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0038622096180915833\n",
            "epoch: 109 train loss: 0.03881879895925522 validation: auc: 0.6626207174862931 --- acc: 78.05 --- loss: 0.5441498160362244\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003812119737267494\n",
            "epoch: 110 train loss: 0.03810594975948334 validation: auc: 0.6628833736884585 --- acc: 78.0 --- loss: 0.5441358089447021\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0037810295820236207\n",
            "epoch: 111 train loss: 0.036728162318468094 validation: auc: 0.6630712337535833 --- acc: 78.0 --- loss: 0.5441247224807739\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00377604179084301\n",
            "epoch: 112 train loss: 0.03440936654806137 validation: auc: 0.6631408115554813 --- acc: 78.0 --- loss: 0.5441207885742188\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0036880113184452057\n",
            "epoch: 113 train loss: 0.03548566997051239 validation: auc: 0.663332150510701 --- acc: 78.0 --- loss: 0.5441296100616455\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003563721105456352\n",
            "epoch: 114 train loss: 0.038116347044706345 validation: auc: 0.6634678272244023 --- acc: 78.05 --- loss: 0.5441707372665405\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0035165522247552873\n",
            "epoch: 115 train loss: 0.03767084330320358 validation: auc: 0.6635582783668698 --- acc: 78.14999999999999 --- loss: 0.5442396998405457\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003575713559985161\n",
            "epoch: 116 train loss: 0.03310699388384819 validation: auc: 0.6636104617182934 --- acc: 78.2 --- loss: 0.544325053691864\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034678250551223753\n",
            "epoch: 117 train loss: 0.03524046391248703 validation: auc: 0.6636122011633407 --- acc: 78.14999999999999 --- loss: 0.5444190502166748\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0034741628915071487\n",
            "epoch: 118 train loss: 0.0328928604722023 validation: auc: 0.6636122011633409 --- acc: 78.14999999999999 --- loss: 0.5445189476013184\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033234674483537674\n",
            "epoch: 119 train loss: 0.0368218831717968 validation: auc: 0.6637026523058083 --- acc: 78.14999999999999 --- loss: 0.5446125268936157\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003285098448395729\n",
            "epoch: 120 train loss: 0.036379922181367874 validation: auc: 0.6637530962121845 --- acc: 78.2 --- loss: 0.5446889400482178\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0033902470022439957\n",
            "epoch: 121 train loss: 0.030210664495825768 validation: auc: 0.6638418079096046 --- acc: 78.2 --- loss: 0.5447471737861633\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0032140787690877913\n",
            "epoch: 122 train loss: 0.03533394634723663 validation: auc: 0.6639113857115027 --- acc: 78.2 --- loss: 0.544809877872467\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003321019560098648\n",
            "epoch: 123 train loss: 0.029207874089479446 validation: auc: 0.6639705268431161 --- acc: 78.25 --- loss: 0.544882595539093\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031533822417259215\n",
            "epoch: 124 train loss: 0.0340692438185215 validation: auc: 0.663989660738638 --- acc: 78.25 --- loss: 0.544974684715271\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031681966036558153\n",
            "epoch: 125 train loss: 0.031706977635622025 validation: auc: 0.6639879212935905 --- acc: 78.3 --- loss: 0.5450610518455505\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031072311103343964\n",
            "epoch: 126 train loss: 0.03240310773253441 validation: auc: 0.6641949152542372 --- acc: 78.3 --- loss: 0.5451340079307556\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003085688874125481\n",
            "epoch: 127 train loss: 0.03161543607711792 validation: auc: 0.6642436197155659 --- acc: 78.25 --- loss: 0.5452132821083069\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0031657807528972627\n",
            "epoch: 128 train loss: 0.026768488809466362 validation: auc: 0.6643305919679385 --- acc: 78.3 --- loss: 0.5453222990036011\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.003026815503835678\n",
            "epoch: 129 train loss: 0.030712014064192772 validation: auc: 0.6644471347861178 --- acc: 78.35 --- loss: 0.5454216599464417\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030442975461483003\n",
            "epoch: 130 train loss: 0.028459293767809868 validation: auc: 0.6645010575825888 --- acc: 78.4 --- loss: 0.545484185218811\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0029582314193248747\n",
            "epoch: 131 train loss: 0.03038058988749981 validation: auc: 0.6646628259720019 --- acc: 78.45 --- loss: 0.5455546975135803\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002923037111759186\n",
            "epoch: 132 train loss: 0.03027050569653511 validation: auc: 0.6647497982243744 --- acc: 78.5 --- loss: 0.5456448197364807\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0030012434348464013\n",
            "epoch: 133 train loss: 0.025695452466607094 validation: auc: 0.664878517157886 --- acc: 78.45 --- loss: 0.5457409024238586\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002873966284096241\n",
            "epoch: 134 train loss: 0.0293599721044302 validation: auc: 0.6650385461022515 --- acc: 78.45 --- loss: 0.5458419322967529\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028505027294158936\n",
            "epoch: 135 train loss: 0.02888183295726776 validation: auc: 0.6651759622610002 --- acc: 78.5 --- loss: 0.5459444522857666\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028768545016646385\n",
            "epoch: 136 train loss: 0.026456523686647415 validation: auc: 0.6652524978430882 --- acc: 78.55 --- loss: 0.5460400581359863\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0028259271755814554\n",
            "epoch: 137 train loss: 0.027155712246894836 validation: auc: 0.6653951323369792 --- acc: 78.55 --- loss: 0.5461173057556152\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002782464027404785\n",
            "epoch: 138 train loss: 0.027552835643291473 validation: auc: 0.665516893490301 --- acc: 78.55 --- loss: 0.5461865067481995\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002784466743469238\n",
            "epoch: 139 train loss: 0.026193834841251373 validation: auc: 0.6657465002365646 --- acc: 78.60000000000001 --- loss: 0.5462585091590881\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027221797034144403\n",
            "epoch: 140 train loss: 0.027401842176914215 validation: auc: 0.6658995714007403 --- acc: 78.60000000000001 --- loss: 0.5463417172431946\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002744968421757221\n",
            "epoch: 141 train loss: 0.0252375528216362 validation: auc: 0.6660074169936824 --- acc: 78.64999999999999 --- loss: 0.5464578866958618\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0027222990989685058\n",
            "epoch: 142 train loss: 0.02494793012738228 validation: auc: 0.6660387270045365 --- acc: 78.7 --- loss: 0.5465788841247559\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002666323632001877\n",
            "epoch: 143 train loss: 0.025974776595830917 validation: auc: 0.6661239598118617 --- acc: 78.7 --- loss: 0.5466973185539246\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.00264369398355484\n",
            "epoch: 144 train loss: 0.025715436786413193 validation: auc: 0.6663118198769865 --- acc: 78.75 --- loss: 0.5467915534973145\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002626851014792919\n",
            "epoch: 145 train loss: 0.025227395817637444 validation: auc: 0.6663918343491693 --- acc: 78.75 --- loss: 0.5468774437904358\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025727406144142153\n",
            "epoch: 146 train loss: 0.026263393461704254 validation: auc: 0.6665240321727757 --- acc: 78.85 --- loss: 0.5469575524330139\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002575375512242317\n",
            "epoch: 147 train loss: 0.025058677420020103 validation: auc: 0.6666840611171412 --- acc: 78.95 --- loss: 0.5470389127731323\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025058962404727937\n",
            "epoch: 148 train loss: 0.026726610958576202 validation: auc: 0.6668927945228355 --- acc: 79.0 --- loss: 0.5471042394638062\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025543088093400003\n",
            "epoch: 149 train loss: 0.023746313527226448 validation: auc: 0.6670789151429128 --- acc: 78.95 --- loss: 0.5471581816673279\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002514870651066303\n",
            "epoch: 150 train loss: 0.024257339537143707 validation: auc: 0.6671850212908075 --- acc: 78.95 --- loss: 0.5472322106361389\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0025163302198052406\n",
            "epoch: 151 train loss: 0.023181108757853508 validation: auc: 0.6673241768946035 --- acc: 78.9 --- loss: 0.5473108887672424\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002468351274728775\n",
            "epoch: 152 train loss: 0.024089394137263298 validation: auc: 0.667475508613732 --- acc: 78.9 --- loss: 0.5474109053611755\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0024152196943759917\n",
            "epoch: 153 train loss: 0.025197915732860565 validation: auc: 0.6676094458823856 --- acc: 78.9 --- loss: 0.5475134253501892\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023953482508659364\n",
            "epoch: 154 train loss: 0.02502189576625824 validation: auc: 0.6677120731401853 --- acc: 78.95 --- loss: 0.5476158857345581\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002410723641514778\n",
            "epoch: 155 train loss: 0.023430729284882545 validation: auc: 0.6678268765133172 --- acc: 78.9 --- loss: 0.5477261543273926\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002329356595873833\n",
            "epoch: 156 train loss: 0.025733277201652527 validation: auc: 0.6678929754251204 --- acc: 78.9 --- loss: 0.5478648543357849\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023363746702671053\n",
            "epoch: 157 train loss: 0.02448834851384163 validation: auc: 0.6679973421279675 --- acc: 78.8 --- loss: 0.5480188727378845\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023029470816254617\n",
            "epoch: 158 train loss: 0.024887653067708015 validation: auc: 0.6680843143803401 --- acc: 78.8 --- loss: 0.5481809973716736\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002373082377016544\n",
            "epoch: 159 train loss: 0.021177880465984344 validation: auc: 0.6681625894074754 --- acc: 78.75 --- loss: 0.5483537912368774\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023151494562625883\n",
            "epoch: 160 train loss: 0.022564269602298737 validation: auc: 0.6682739138905123 --- acc: 78.8 --- loss: 0.5485141277313232\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0023342879489064217\n",
            "epoch: 161 train loss: 0.020899685099720955 validation: auc: 0.6683852383735494 --- acc: 78.8 --- loss: 0.5486391186714172\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022584548220038413\n",
            "epoch: 162 train loss: 0.02302636206150055 validation: auc: 0.6684774289610642 --- acc: 78.85 --- loss: 0.5487709641456604\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022366831079125406\n",
            "epoch: 163 train loss: 0.02304012142121792 validation: auc: 0.6686113662297182 --- acc: 79.0 --- loss: 0.5489086508750916\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022688336670398713\n",
            "epoch: 164 train loss: 0.020891254767775536 validation: auc: 0.6685991901143858 --- acc: 79.05 --- loss: 0.5490362048149109\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022245390340685845\n",
            "epoch: 165 train loss: 0.02183600515127182 validation: auc: 0.6686879018118059 --- acc: 79.05 --- loss: 0.5491985082626343\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0022087350487709047\n",
            "epoch: 166 train loss: 0.02164372056722641 validation: auc: 0.6687522612785617 --- acc: 79.05 --- loss: 0.5493906736373901\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002197805605828762\n",
            "epoch: 167 train loss: 0.0212662722915411 validation: auc: 0.6688270574156021 --- acc: 79.05 --- loss: 0.5495690703392029\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021304843947291376\n",
            "epoch: 168 train loss: 0.02316906861960888 validation: auc: 0.6688235785255072 --- acc: 79.0 --- loss: 0.5497279167175293\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002198476158082485\n",
            "epoch: 169 train loss: 0.019662540405988693 validation: auc: 0.6688531490913139 --- acc: 79.0 --- loss: 0.5498794913291931\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0021226471289992332\n",
            "epoch: 170 train loss: 0.021920597180724144 validation: auc: 0.6688514096462664 --- acc: 79.0 --- loss: 0.5500259399414062\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002181212417781353\n",
            "epoch: 171 train loss: 0.01884087733924389 validation: auc: 0.6689044627202139 --- acc: 79.05 --- loss: 0.5501740574836731\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002102407068014145\n",
            "epoch: 172 train loss: 0.021228009834885597 validation: auc: 0.6689923046951101 --- acc: 79.10000000000001 --- loss: 0.5503107905387878\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0020554183050990106\n",
            "epoch: 173 train loss: 0.022375183179974556 validation: auc: 0.6690740586123404 --- acc: 79.14999999999999 --- loss: 0.5504252314567566\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002082919888198376\n",
            "epoch: 174 train loss: 0.02054155059158802 validation: auc: 0.6691627703097605 --- acc: 79.2 --- loss: 0.5505348443984985\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002077673189342022\n",
            "epoch: 175 train loss: 0.020042918622493744 validation: auc: 0.6693227992541261 --- acc: 79.2 --- loss: 0.5506414175033569\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002075386792421341\n",
            "epoch: 176 train loss: 0.01942826248705387 validation: auc: 0.6693541092649801 --- acc: 79.3 --- loss: 0.550769567489624\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019870977848768234\n",
            "epoch: 177 train loss: 0.022268226370215416 validation: auc: 0.669446299852495 --- acc: 79.3 --- loss: 0.5508956909179688\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019884485751390456\n",
            "epoch: 178 train loss: 0.021509673446416855 validation: auc: 0.6695837160112439 --- acc: 79.35 --- loss: 0.5510032176971436\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001989586278796196\n",
            "epoch: 179 train loss: 0.020788365975022316 validation: auc: 0.6696776460438063 --- acc: 79.4 --- loss: 0.5511013865470886\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.002007700689136982\n",
            "epoch: 180 train loss: 0.01937737688422203 validation: auc: 0.6697576605159891 --- acc: 79.45 --- loss: 0.5512292385101318\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019761573523283003\n",
            "epoch: 181 train loss: 0.019969133660197258 validation: auc: 0.6698220199827447 --- acc: 79.45 --- loss: 0.5513864755630493\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019706115126609802\n",
            "epoch: 182 train loss: 0.019543111324310303 validation: auc: 0.6699403022459713 --- acc: 79.45 --- loss: 0.5515468120574951\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019409071654081344\n",
            "epoch: 183 train loss: 0.020056521520018578 validation: auc: 0.6700777184047201 --- acc: 79.45 --- loss: 0.5517101287841797\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018943751230835915\n",
            "epoch: 184 train loss: 0.0212570670992136 validation: auc: 0.6701803456625198 --- acc: 79.45 --- loss: 0.5518934726715088\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0019599353894591332\n",
            "epoch: 185 train loss: 0.017985858023166656 validation: auc: 0.6702812334752719 --- acc: 79.45 --- loss: 0.5521218776702881\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018979530781507493\n",
            "epoch: 186 train loss: 0.01983417570590973 validation: auc: 0.670354290167265 --- acc: 79.45 --- loss: 0.5523472428321838\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001930052973330021\n",
            "epoch: 187 train loss: 0.017905892804265022 validation: auc: 0.6704743118755393 --- acc: 79.45 --- loss: 0.5525308847427368\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018695071339607239\n",
            "epoch: 188 train loss: 0.019702624529600143 validation: auc: 0.6706065096991456 --- acc: 79.4 --- loss: 0.5527213215827942\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001883842423558235\n",
            "epoch: 189 train loss: 0.018483882769942284 validation: auc: 0.6707004397317079 --- acc: 79.35 --- loss: 0.5529184937477112\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018933217972517014\n",
            "epoch: 190 train loss: 0.017496244981884956 validation: auc: 0.6707630597534163 --- acc: 79.3 --- loss: 0.5531153678894043\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018943825736641884\n",
            "epoch: 191 train loss: 0.016836965456604958 validation: auc: 0.6708204614399822 --- acc: 79.3 --- loss: 0.5533040165901184\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0018211141228675841\n",
            "epoch: 192 train loss: 0.01917913556098938 validation: auc: 0.6708865603517854 --- acc: 79.3 --- loss: 0.5534704327583313\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001874127797782421\n",
            "epoch: 193 train loss: 0.01647910289466381 validation: auc: 0.6710309342907239 --- acc: 79.3 --- loss: 0.5536285638809204\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017769835889339448\n",
            "epoch: 194 train loss: 0.01977819949388504 validation: auc: 0.6711213854331914 --- acc: 79.3 --- loss: 0.5537682771682739\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017211945727467538\n",
            "epoch: 195 train loss: 0.02141290158033371 validation: auc: 0.6711700898945201 --- acc: 79.25 --- loss: 0.5538970828056335\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017877517268061638\n",
            "epoch: 196 train loss: 0.01816781982779503 validation: auc: 0.671208357685564 --- acc: 79.3 --- loss: 0.5540174245834351\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017847580835223197\n",
            "epoch: 197 train loss: 0.01771708019077778 validation: auc: 0.6713301188388856 --- acc: 79.3 --- loss: 0.5541388988494873\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017099082469940186\n",
            "epoch: 198 train loss: 0.020162351429462433 validation: auc: 0.671424048871448 --- acc: 79.35 --- loss: 0.5542893409729004\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017914200201630593\n",
            "epoch: 199 train loss: 0.016329223290085793 validation: auc: 0.6714779716679191 --- acc: 79.35 --- loss: 0.5544864535331726\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016931541264057159\n",
            "epoch: 200 train loss: 0.01970871537923813 validation: auc: 0.6715492889148647 --- acc: 79.4 --- loss: 0.554716944694519\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0017454391345381737\n",
            "epoch: 201 train loss: 0.017061371356248856 validation: auc: 0.6715684228103866 --- acc: 79.4 --- loss: 0.5549684166908264\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001707017607986927\n",
            "epoch: 202 train loss: 0.018091391772031784 validation: auc: 0.6716240850519051 --- acc: 79.4 --- loss: 0.5552452206611633\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016747366636991501\n",
            "epoch: 203 train loss: 0.018838033080101013 validation: auc: 0.6716884445186607 --- acc: 79.5 --- loss: 0.5555265545845032\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001695590279996395\n",
            "epoch: 204 train loss: 0.017457671463489532 validation: auc: 0.6717615012106538 --- acc: 79.45 --- loss: 0.5557667016983032\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001693054474890232\n",
            "epoch: 205 train loss: 0.01703561283648014 validation: auc: 0.6717962901116028 --- acc: 79.4 --- loss: 0.5559840798377991\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.001634003408253193\n",
            "epoch: 206 train loss: 0.018903642892837524 validation: auc: 0.6719215301550192 --- acc: 79.35 --- loss: 0.5562040209770203\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016273340210318566\n",
            "epoch: 207 train loss: 0.018632037565112114 validation: auc: 0.6720154601875817 --- acc: 79.4 --- loss: 0.5564095377922058\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016809003427624702\n",
            "epoch: 208 train loss: 0.015982910990715027 validation: auc: 0.6721093902201443 --- acc: 79.4 --- loss: 0.5566449761390686\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016214735805988312\n",
            "epoch: 209 train loss: 0.01783445104956627 validation: auc: 0.672116348000334 --- acc: 79.45 --- loss: 0.5568841695785522\n",
            "<Timer(Thread-10417, started 140340975630080)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.0016833541914820672\n",
            "epoch: 210 train loss: 0.01489759236574173 validation: auc: 0.6721528763463304 --- acc: 79.5 --- loss: 0.5570985674858093\n",
            "time up, break\n",
            "test auc: 0.6787650932942682 test acc: 79.85 test loss 0.545834481716156\n",
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n",
            "one hot encoding: feature hour\n",
            "one hot encoding: feature C1\n",
            "one hot encoding: feature banner_pos\n",
            "one hot encoding: feature site_id\n",
            "one hot encoding: feature site_domain\n",
            "one hot encoding: feature site_category\n",
            "one hot encoding: feature app_id\n",
            "one hot encoding: feature app_domain\n",
            "one hot encoding: feature app_category\n",
            "one hot encoding: feature device_id\n",
            "one hot encoding: feature device_ip\n",
            "one hot encoding: feature device_model\n",
            "one hot encoding: feature device_type\n",
            "one hot encoding: feature device_conn_type\n",
            "one hot encoding: feature C14\n",
            "one hot encoding: feature C15\n",
            "one hot encoding: feature C16\n",
            "one hot encoding: feature C17\n",
            "one hot encoding: feature C18\n",
            "one hot encoding: feature C19\n",
            "one hot encoding: feature C20\n",
            "one hot encoding: feature C21\n",
            "Data set initiated from /content/drive/My Drive/train20k.csv.\n",
            "Memory Usage:\n",
            "Allocated: 0.2 GB\n",
            "Cached:    1.4 GB\n",
            "    - loss: 0.19083179235458375\n",
            "epoch: 0 train loss: 1.7966684103012085 validation: auc: 0.51215274976469 --- acc: 42.25 --- loss: 1.8134381771087646\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.1835455298423767\n",
            "epoch: 1 train loss: 1.8134794235229492 validation: auc: 0.5126421944332392 --- acc: 42.9 --- loss: 1.7620099782943726\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.17885656356811525\n",
            "epoch: 2 train loss: 1.7187658548355103 validation: auc: 0.5132571377347497 --- acc: 43.6 --- loss: 1.7065614461898804\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.17246493101119995\n",
            "epoch: 3 train loss: 1.667366623878479 validation: auc: 0.5140836358746806 --- acc: 44.5 --- loss: 1.645681381225586\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.16522839069366455\n",
            "epoch: 4 train loss: 1.6171011924743652 validation: auc: 0.5150212899466631 --- acc: 45.9 --- loss: 1.5786247253417969\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.15711578130722045\n",
            "epoch: 5 train loss: 1.5680714845657349 validation: auc: 0.5162942046524135 --- acc: 47.199999999999996 --- loss: 1.5053797960281372\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.14991172552108764\n",
            "epoch: 6 train loss: 1.4473965167999268 validation: auc: 0.5177679171709022 --- acc: 49.7 --- loss: 1.426501989364624\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.14036126136779786\n",
            "epoch: 7 train loss: 1.3883662223815918 validation: auc: 0.5194065707498543 --- acc: 51.349999999999994 --- loss: 1.3430932760238647\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.1312396287918091\n",
            "epoch: 8 train loss: 1.2890334129333496 validation: auc: 0.5214414414414414 --- acc: 54.05 --- loss: 1.2568610906600952\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.12171528339385987\n",
            "epoch: 9 train loss: 1.1883735656738281 validation: auc: 0.5241145623235176 --- acc: 55.95 --- loss: 1.1699988842010498\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.11373732089996338\n",
            "epoch: 10 train loss: 1.0250108242034912 validation: auc: 0.5271086011384518 --- acc: 58.85 --- loss: 1.0850765705108643\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.10336952209472657\n",
            "epoch: 11 train loss: 0.9685665369033813 validation: auc: 0.5306404912375062 --- acc: 61.45 --- loss: 1.0050290822982788\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.09438958168029785\n",
            "epoch: 12 train loss: 0.8798809051513672 validation: auc: 0.5351046568957016 --- acc: 64.25 --- loss: 0.9325435757637024\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0852487862110138\n",
            "epoch: 13 train loss: 0.8417249321937561 validation: auc: 0.540006274931648 --- acc: 67.35 --- loss: 0.8699042797088623\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0786512017250061\n",
            "epoch: 14 train loss: 0.7538907527923584 validation: auc: 0.5456949486800233 --- acc: 70.05 --- loss: 0.818702757358551\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.07202028632164001\n",
            "epoch: 15 train loss: 0.7263321876525879 validation: auc: 0.551656133745686 --- acc: 73.3 --- loss: 0.7792697548866272\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.06758093237876892\n",
            "epoch: 16 train loss: 0.6737320423126221 validation: auc: 0.558696607054816 --- acc: 75.64999999999999 --- loss: 0.750676155090332\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0651451051235199\n",
            "epoch: 17 train loss: 0.5930932760238647 validation: auc: 0.5660167630316885 --- acc: 76.95 --- loss: 0.7308685183525085\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.060991936922073366\n",
            "epoch: 18 train loss: 0.621632993221283 validation: auc: 0.5732383129398055 --- acc: 78.3 --- loss: 0.7170432806015015\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.05947024822235107\n",
            "epoch: 19 train loss: 0.5718693733215332 validation: auc: 0.5805172336515619 --- acc: 78.60000000000001 --- loss: 0.7064073085784912\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.05723828077316284\n",
            "epoch: 20 train loss: 0.5627561211585999 validation: auc: 0.5872618887544261 --- acc: 79.2 --- loss: 0.6968823075294495\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.05561032295227051\n",
            "epoch: 21 train loss: 0.5294597744941711 validation: auc: 0.5936219801891444 --- acc: 79.55 --- loss: 0.6871287226676941\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.053400087356567386\n",
            "epoch: 22 train loss: 0.5137858390808105 validation: auc: 0.5991717090224552 --- acc: 79.65 --- loss: 0.6767634153366089\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.05082034468650818\n",
            "epoch: 23 train loss: 0.5074300765991211 validation: auc: 0.6043494240509165 --- acc: 79.9 --- loss: 0.6658936738967896\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.04845747351646423\n",
            "epoch: 24 train loss: 0.4852225184440613 validation: auc: 0.6087866971449061 --- acc: 79.7 --- loss: 0.6549128890037537\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.04556262195110321\n",
            "epoch: 25 train loss: 0.48012974858283997 validation: auc: 0.6125767558603379 --- acc: 79.55 --- loss: 0.6444442272186279\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.04314165413379669\n",
            "epoch: 26 train loss: 0.4566013216972351 validation: auc: 0.6160996817713236 --- acc: 79.25 --- loss: 0.6349917650222778\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.040974995493888854\n",
            "epoch: 27 train loss: 0.4255712926387787 validation: auc: 0.619179776791717 --- acc: 78.9 --- loss: 0.6269195675849915\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.03906255066394806\n",
            "epoch: 28 train loss: 0.3892737627029419 validation: auc: 0.6217417417417417 --- acc: 78.85 --- loss: 0.6203745603561401\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.03722447156906128\n",
            "epoch: 29 train loss: 0.35664260387420654 validation: auc: 0.6238214333736722 --- acc: 78.64999999999999 --- loss: 0.6152768731117249\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0351282924413681\n",
            "epoch: 30 train loss: 0.33965203166007996 validation: auc: 0.6255676572094483 --- acc: 78.35 --- loss: 0.6113094091415405\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.03303232192993164\n",
            "epoch: 31 train loss: 0.3290511965751648 validation: auc: 0.626819057863834 --- acc: 77.95 --- loss: 0.6080747246742249\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.031198334693908692\n",
            "epoch: 32 train loss: 0.3119308054447174 validation: auc: 0.6278033257137734 --- acc: 77.55 --- loss: 0.605166494846344\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.02983529269695282\n",
            "epoch: 33 train loss: 0.2806646525859833 validation: auc: 0.6284021334767604 --- acc: 77.05 --- loss: 0.6023460626602173\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.027934342622756958\n",
            "epoch: 34 train loss: 0.2731309235095978 validation: auc: 0.6290529335305455 --- acc: 77.14999999999999 --- loss: 0.5994855165481567\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.026470956206321717\n",
            "epoch: 35 train loss: 0.2510063946247101 validation: auc: 0.6292913809331719 --- acc: 76.8 --- loss: 0.5964989066123962\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.025117680430412292\n",
            "epoch: 36 train loss: 0.2275795191526413 validation: auc: 0.6294473578055668 --- acc: 77.10000000000001 --- loss: 0.5934098362922668\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.02354104220867157\n",
            "epoch: 37 train loss: 0.2156156599521637 validation: auc: 0.6296051275155753 --- acc: 77.35 --- loss: 0.5903153419494629\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.021648490428924562\n",
            "epoch: 38 train loss: 0.21957500278949738 validation: auc: 0.6295836134642104 --- acc: 77.8 --- loss: 0.587310254573822\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.020932728052139284\n",
            "epoch: 39 train loss: 0.18109185993671417 validation: auc: 0.6293236520102192 --- acc: 77.85 --- loss: 0.584472119808197\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.019278813898563386\n",
            "epoch: 40 train loss: 0.18309997022151947 validation: auc: 0.6293684729505625 --- acc: 77.85 --- loss: 0.5818706750869751\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.01803249716758728\n",
            "epoch: 41 train loss: 0.17336517572402954 validation: auc: 0.6292725561382277 --- acc: 78.0 --- loss: 0.5795863270759583\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.017018288373947144\n",
            "epoch: 42 train loss: 0.1576305329799652 validation: auc: 0.6290923759580476 --- acc: 77.95 --- loss: 0.5776458382606506\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.01571059077978134\n",
            "epoch: 43 train loss: 0.15743209421634674 validation: auc: 0.6291318183855498 --- acc: 77.95 --- loss: 0.5760093927383423\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0145994171500206\n",
            "epoch: 44 train loss: 0.1531011462211609 validation: auc: 0.6291533324369145 --- acc: 77.95 --- loss: 0.5746880769729614\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.013810972869396209\n",
            "epoch: 45 train loss: 0.13902521133422852 validation: auc: 0.6290852046075927 --- acc: 77.95 --- loss: 0.5736599564552307\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.013218849897384644\n",
            "epoch: 46 train loss: 0.12078738212585449 validation: auc: 0.6289238492223567 --- acc: 77.95 --- loss: 0.5729238390922546\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.012426737695932388\n",
            "epoch: 47 train loss: 0.11318141967058182 validation: auc: 0.6288234503159876 --- acc: 78.0 --- loss: 0.572433590888977\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.01161704957485199\n",
            "epoch: 48 train loss: 0.10935908555984497 validation: auc: 0.6288933709829232 --- acc: 77.8 --- loss: 0.5721240043640137\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.010983825474977494\n",
            "epoch: 49 train loss: 0.10181137174367905 validation: auc: 0.6289184707095156 --- acc: 77.9 --- loss: 0.5719377398490906\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.010188821703195572\n",
            "epoch: 50 train loss: 0.10288816690444946 validation: auc: 0.6288162789655327 --- acc: 77.9 --- loss: 0.571823000907898\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.009727983176708222\n",
            "epoch: 51 train loss: 0.09386209398508072 validation: auc: 0.6287320155976873 --- acc: 78.0 --- loss: 0.571743905544281\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.009250856190919875\n",
            "epoch: 52 train loss: 0.0873262882232666 validation: auc: 0.6288790282820134 --- acc: 78.2 --- loss: 0.5716373324394226\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.008861606568098068\n",
            "epoch: 53 train loss: 0.07948288321495056 validation: auc: 0.6288377930168975 --- acc: 78.14999999999999 --- loss: 0.5714944005012512\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.008297644555568695\n",
            "epoch: 54 train loss: 0.08059767633676529 validation: auc: 0.6287320155976873 --- acc: 78.10000000000001 --- loss: 0.5713467001914978\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.00796368271112442\n",
            "epoch: 55 train loss: 0.07430829852819443 validation: auc: 0.6287409797857559 --- acc: 78.05 --- loss: 0.5711281895637512\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.007623367011547089\n",
            "epoch: 56 train loss: 0.06984318792819977 validation: auc: 0.6284971538702881 --- acc: 78.2 --- loss: 0.570835530757904\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0073306173086166385\n",
            "epoch: 57 train loss: 0.06514625251293182 validation: auc: 0.6284146833400566 --- acc: 78.3 --- loss: 0.5704819560050964\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.007006575912237167\n",
            "epoch: 58 train loss: 0.06304129958152771 validation: auc: 0.628256913630048 --- acc: 78.35 --- loss: 0.5701488256454468\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.006526345014572143\n",
            "epoch: 59 train loss: 0.06826916337013245 validation: auc: 0.6282676706557304 --- acc: 78.45 --- loss: 0.5698400735855103\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.006260626763105392\n",
            "epoch: 60 train loss: 0.06611595302820206 validation: auc: 0.6282533279548205 --- acc: 78.35 --- loss: 0.5695989727973938\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.006043460965156555\n",
            "epoch: 61 train loss: 0.06311340630054474 validation: auc: 0.6283644838868719 --- acc: 78.3 --- loss: 0.5693933367729187\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.005919346958398819\n",
            "epoch: 62 train loss: 0.05723061412572861 validation: auc: 0.6284935681950606 --- acc: 78.4 --- loss: 0.5692183971405029\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.005616981536149979\n",
            "epoch: 63 train loss: 0.05928666889667511 validation: auc: 0.6285491461610865 --- acc: 78.55 --- loss: 0.569074809551239\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.005590707436203957\n",
            "epoch: 64 train loss: 0.051174506545066833 validation: auc: 0.6286818161445027 --- acc: 78.45 --- loss: 0.5689483284950256\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.005445615947246551\n",
            "epoch: 65 train loss: 0.04838940501213074 validation: auc: 0.6288413786921249 --- acc: 78.45 --- loss: 0.5687955617904663\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.005163863673806191\n",
            "epoch: 66 train loss: 0.05165539309382439 validation: auc: 0.6290260409663394 --- acc: 78.4 --- loss: 0.5686019659042358\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004937328398227692\n",
            "epoch: 67 train loss: 0.05336045101284981 validation: auc: 0.6292268387790776 --- acc: 78.4 --- loss: 0.5683794021606445\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004864895343780517\n",
            "epoch: 68 train loss: 0.04938457906246185 validation: auc: 0.6293756443010174 --- acc: 78.5 --- loss: 0.5682304501533508\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004770905524492264\n",
            "epoch: 69 train loss: 0.04674641788005829 validation: auc: 0.6295110035408542 --- acc: 78.55 --- loss: 0.5681490302085876\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004629130288958549\n",
            "epoch: 70 train loss: 0.04641294479370117 validation: auc: 0.6297557258751288 --- acc: 78.55 --- loss: 0.5680841207504272\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004507995769381523\n",
            "epoch: 71 train loss: 0.045575495809316635 validation: auc: 0.6299475594997983 --- acc: 78.64999999999999 --- loss: 0.568006694316864\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004465557262301445\n",
            "epoch: 72 train loss: 0.04196054860949516 validation: auc: 0.6301429787996952 --- acc: 78.60000000000001 --- loss: 0.567972719669342\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004401950910687446\n",
            "epoch: 73 train loss: 0.039594851434230804 validation: auc: 0.6302666845950428 --- acc: 78.75 --- loss: 0.5679602026939392\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004279113188385963\n",
            "epoch: 74 train loss: 0.03974725306034088 validation: auc: 0.6304459683564161 --- acc: 78.75 --- loss: 0.5679472088813782\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004140475764870644\n",
            "epoch: 75 train loss: 0.04083045944571495 validation: auc: 0.6306162879297207 --- acc: 78.7 --- loss: 0.5679382085800171\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.004056704416871071\n",
            "epoch: 76 train loss: 0.039980314671993256 validation: auc: 0.6308385997938236 --- acc: 78.60000000000001 --- loss: 0.5679455399513245\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0038918647915124893\n",
            "epoch: 77 train loss: 0.04252905398607254 validation: auc: 0.6310429832817892 --- acc: 78.60000000000001 --- loss: 0.5679681301116943\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0038383468985557557\n",
            "epoch: 78 train loss: 0.040936652570962906 validation: auc: 0.6311899959661154 --- acc: 78.64999999999999 --- loss: 0.5679821968078613\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.00379297249019146\n",
            "epoch: 79 train loss: 0.039140235632658005 validation: auc: 0.6313513513513512 --- acc: 78.60000000000001 --- loss: 0.5679866671562195\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0037627514451742173\n",
            "epoch: 80 train loss: 0.03690476715564728 validation: auc: 0.6315790417282954 --- acc: 78.7 --- loss: 0.5680148601531982\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0037593495100736616\n",
            "epoch: 81 train loss: 0.03374245762825012 validation: auc: 0.6317672896777375 --- acc: 79.0 --- loss: 0.5680938959121704\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.003664221614599228\n",
            "epoch: 82 train loss: 0.03438778221607208 validation: auc: 0.631937609251042 --- acc: 79.05 --- loss: 0.5681775808334351\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0035286661237478255\n",
            "epoch: 83 train loss: 0.036799781024456024 validation: auc: 0.6320057370803639 --- acc: 79.10000000000001 --- loss: 0.568248450756073\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0034221552312374117\n",
            "epoch: 84 train loss: 0.038192495703697205 validation: auc: 0.6320774505849133 --- acc: 79.14999999999999 --- loss: 0.5683454275131226\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0034174744039773943\n",
            "epoch: 85 train loss: 0.035630982369184494 validation: auc: 0.6322065348931021 --- acc: 79.14999999999999 --- loss: 0.5684597492218018\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.003395581617951393\n",
            "epoch: 86 train loss: 0.033839013427495956 validation: auc: 0.6324001613553852 --- acc: 79.25 --- loss: 0.568602442741394\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0033769723027944564\n",
            "epoch: 87 train loss: 0.0320136733353138 validation: auc: 0.6325812379543724 --- acc: 79.2 --- loss: 0.5687842965126038\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0032705016434192657\n",
            "epoch: 88 train loss: 0.033778637647628784 validation: auc: 0.6328035498184752 --- acc: 79.2 --- loss: 0.5689902305603027\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0032120324671268464\n",
            "epoch: 89 train loss: 0.03364923223853111 validation: auc: 0.6330079333064407 --- acc: 79.2 --- loss: 0.5692042708396912\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.003190561011433601\n",
            "epoch: 90 train loss: 0.03216157853603363 validation: auc: 0.6331567388283805 --- acc: 79.2 --- loss: 0.5694114565849304\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0031323209404945374\n",
            "epoch: 91 train loss: 0.03222239017486572 validation: auc: 0.6334041504190759 --- acc: 79.14999999999999 --- loss: 0.569618284702301\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.003119024820625782\n",
            "epoch: 92 train loss: 0.03061678819358349 validation: auc: 0.6335762628299942 --- acc: 79.14999999999999 --- loss: 0.569807231426239\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0030945435166358946\n",
            "epoch: 93 train loss: 0.029458479955792427 validation: auc: 0.6337358253776164 --- acc: 79.2 --- loss: 0.5700251460075378\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0030012907460331916\n",
            "epoch: 94 train loss: 0.031161582097411156 validation: auc: 0.6338362242839854 --- acc: 79.2 --- loss: 0.5702185034751892\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0030297566205263137\n",
            "epoch: 95 train loss: 0.028032837435603142 validation: auc: 0.634019990139393 --- acc: 79.2 --- loss: 0.5703268051147461\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0028313064947724344\n",
            "epoch: 96 train loss: 0.0340534970164299 validation: auc: 0.6341786562682085 --- acc: 79.3 --- loss: 0.5704328417778015\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0030022136867046355\n",
            "epoch: 97 train loss: 0.025340188294649124 validation: auc: 0.6343238761149208 --- acc: 79.2 --- loss: 0.5705781579017639\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0028991339728236198\n",
            "epoch: 98 train loss: 0.02764289081096649 validation: auc: 0.6345264667652728 --- acc: 79.2 --- loss: 0.5707160830497742\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0028290461748838426\n",
            "epoch: 99 train loss: 0.028685366734862328 validation: auc: 0.6347308502532384 --- acc: 79.25 --- loss: 0.5708473920822144\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002816573157906532\n",
            "epoch: 100 train loss: 0.0274793840944767 validation: auc: 0.6349495764421138 --- acc: 79.25 --- loss: 0.5709912180900574\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0027950070798397064\n",
            "epoch: 101 train loss: 0.026664042845368385 validation: auc: 0.6352274662722424 --- acc: 79.3 --- loss: 0.5711339116096497\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0028452387079596518\n",
            "epoch: 102 train loss: 0.023007690906524658 validation: auc: 0.6354085428712295 --- acc: 79.35 --- loss: 0.5712537169456482\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002658878639340401\n",
            "epoch: 103 train loss: 0.028861667960882187 validation: auc: 0.6356236833848774 --- acc: 79.45 --- loss: 0.5713773369789124\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002685478702187538\n",
            "epoch: 104 train loss: 0.026233147829771042 validation: auc: 0.6358459952489803 --- acc: 79.45 --- loss: 0.5715107321739197\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0026515047997236253\n",
            "epoch: 105 train loss: 0.026059016585350037 validation: auc: 0.6361328492671777 --- acc: 79.5 --- loss: 0.5716606974601746\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0026167163625359533\n",
            "epoch: 106 train loss: 0.025954756885766983 validation: auc: 0.6363712966698042 --- acc: 79.60000000000001 --- loss: 0.5717992782592773\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002659264951944351\n",
            "epoch: 107 train loss: 0.022776447236537933 validation: auc: 0.6365685088073147 --- acc: 79.55 --- loss: 0.571948766708374\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0025215735659003258\n",
            "epoch: 108 train loss: 0.026850976049900055 validation: auc: 0.6367979920218726 --- acc: 79.55 --- loss: 0.5721026062965393\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002520762011408806\n",
            "epoch: 109 train loss: 0.025479085743427277 validation: auc: 0.6369790686208597 --- acc: 79.7 --- loss: 0.5722867846488953\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002484501898288727\n",
            "epoch: 110 train loss: 0.025574469938874245 validation: auc: 0.6370687105015463 --- acc: 79.65 --- loss: 0.5724937915802002\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0024945368990302087\n",
            "epoch: 111 train loss: 0.023829646408557892 validation: auc: 0.6371906234592802 --- acc: 79.7 --- loss: 0.5727114081382751\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0024625323712825775\n",
            "epoch: 112 train loss: 0.023786291480064392 validation: auc: 0.6372157231858725 --- acc: 79.7 --- loss: 0.5729295611381531\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.00239681676030159\n",
            "epoch: 113 train loss: 0.025157522410154343 validation: auc: 0.6372569584509883 --- acc: 79.85 --- loss: 0.5731261372566223\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0023072319105267524\n",
            "epoch: 114 train loss: 0.02755402959883213 validation: auc: 0.6373824570839497 --- acc: 79.85 --- loss: 0.5732879638671875\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0024177860468626022\n",
            "epoch: 115 train loss: 0.02190578170120716 validation: auc: 0.6375527766572543 --- acc: 79.85 --- loss: 0.5734381079673767\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002315692976117134\n",
            "epoch: 116 train loss: 0.02478426694869995 validation: auc: 0.6377338532562413 --- acc: 79.85 --- loss: 0.5735961198806763\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0023101808503270147\n",
            "epoch: 117 train loss: 0.023833610117435455 validation: auc: 0.637897001479091 --- acc: 79.85 --- loss: 0.5737598538398743\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002324969694018364\n",
            "epoch: 118 train loss: 0.02204619161784649 validation: auc: 0.6381013849670567 --- acc: 79.9 --- loss: 0.5739433765411377\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0023049317300319672\n",
            "epoch: 119 train loss: 0.02166169323027134 validation: auc: 0.6382932185917259 --- acc: 79.95 --- loss: 0.5741299986839294\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002244272455573082\n",
            "epoch: 120 train loss: 0.022962234914302826 validation: auc: 0.6385298731567388 --- acc: 79.9 --- loss: 0.5743698477745056\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002249968983232975\n",
            "epoch: 121 train loss: 0.021613197401165962 validation: auc: 0.6386482004392452 --- acc: 79.95 --- loss: 0.5746396780014038\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002211349457502365\n",
            "epoch: 122 train loss: 0.022027568891644478 validation: auc: 0.6387450136703868 --- acc: 79.95 --- loss: 0.5749008059501648\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002230390161275864\n",
            "epoch: 123 train loss: 0.020201751962304115 validation: auc: 0.6388131414997087 --- acc: 80.05 --- loss: 0.5751876831054688\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0021273281425237657\n",
            "epoch: 124 train loss: 0.02325942926108837 validation: auc: 0.6388884406794855 --- acc: 80.05 --- loss: 0.575480043888092\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0021431587636470796\n",
            "epoch: 125 train loss: 0.021587073802947998 validation: auc: 0.638945811483125 --- acc: 80.0 --- loss: 0.5757481455802917\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0021156104281544684\n",
            "epoch: 126 train loss: 0.02162882313132286 validation: auc: 0.6390408318766528 --- acc: 80.0 --- loss: 0.5759896039962769\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002147291600704193\n",
            "epoch: 127 train loss: 0.019385093823075294 validation: auc: 0.6392165299627987 --- acc: 80.0 --- loss: 0.5762050747871399\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002126327343285084\n",
            "epoch: 128 train loss: 0.019289210438728333 validation: auc: 0.6393725068351934 --- acc: 80.05 --- loss: 0.5763906240463257\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0021223945543169974\n",
            "epoch: 129 train loss: 0.018469268456101418 validation: auc: 0.6396360539644121 --- acc: 80.10000000000001 --- loss: 0.5765523314476013\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.002024318277835846\n",
            "epoch: 130 train loss: 0.02141762711107731 validation: auc: 0.6397776881358971 --- acc: 80.25 --- loss: 0.5767432451248169\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0020412418991327284\n",
            "epoch: 131 train loss: 0.019823694601655006 validation: auc: 0.6399838644614764 --- acc: 80.25 --- loss: 0.5769526362419128\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0020880766212940218\n",
            "epoch: 132 train loss: 0.017014486715197563 validation: auc: 0.6401918336246695 --- acc: 80.25 --- loss: 0.5771278738975525\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019958246499300002\n",
            "epoch: 133 train loss: 0.019815128296613693 validation: auc: 0.6403280892833131 --- acc: 80.35 --- loss: 0.5772801041603088\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019832901656627656\n",
            "epoch: 134 train loss: 0.01941533014178276 validation: auc: 0.6405396441217336 --- acc: 80.4 --- loss: 0.5774399042129517\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001922943815588951\n",
            "epoch: 135 train loss: 0.020904958248138428 validation: auc: 0.6406974138317422 --- acc: 80.35 --- loss: 0.5776392221450806\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019259920343756675\n",
            "epoch: 136 train loss: 0.019940335303544998 validation: auc: 0.640892833131639 --- acc: 80.35 --- loss: 0.5778771638870239\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019383933395147324\n",
            "epoch: 137 train loss: 0.01858055777847767 validation: auc: 0.6412047868764287 --- acc: 80.35 --- loss: 0.5781306624412537\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019402626901865005\n",
            "epoch: 138 train loss: 0.017657563090324402 validation: auc: 0.6413858634754158 --- acc: 80.30000000000001 --- loss: 0.5783628821372986\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0019352365285158156\n",
            "epoch: 139 train loss: 0.0170125849545002 validation: auc: 0.6415902469633814 --- acc: 80.30000000000001 --- loss: 0.5785894989967346\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001855968125164509\n",
            "epoch: 140 train loss: 0.01933889463543892 validation: auc: 0.6417677378871408 --- acc: 80.30000000000001 --- loss: 0.5788315534591675\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.00187278613448143\n",
            "epoch: 141 train loss: 0.017850879579782486 validation: auc: 0.6419506073237417 --- acc: 80.30000000000001 --- loss: 0.5790751576423645\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0018048964440822602\n",
            "epoch: 142 train loss: 0.019757065922021866 validation: auc: 0.642170229931424 --- acc: 80.4 --- loss: 0.5792768597602844\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001863580197095871\n",
            "epoch: 143 train loss: 0.01660839468240738 validation: auc: 0.6423055891712608 --- acc: 80.4 --- loss: 0.5794724225997925\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0018764734268188477\n",
            "epoch: 144 train loss: 0.015326499938964844 validation: auc: 0.6425691363004795 --- acc: 80.5 --- loss: 0.5796533226966858\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0018094906583428383\n",
            "epoch: 145 train loss: 0.017221588641405106 validation: auc: 0.642829097754471 --- acc: 80.55 --- loss: 0.5798179507255554\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0017593374475836753\n",
            "epoch: 146 train loss: 0.01844821870326996 validation: auc: 0.6430496167809601 --- acc: 80.55 --- loss: 0.5800014138221741\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0017920134589076043\n",
            "epoch: 147 train loss: 0.016398491337895393 validation: auc: 0.6432396575680157 --- acc: 80.65 --- loss: 0.5802083015441895\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0017850972712039948\n",
            "epoch: 148 train loss: 0.015914758667349815 validation: auc: 0.6434404553807539 --- acc: 80.80000000000001 --- loss: 0.5804235935211182\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0017460662871599197\n",
            "epoch: 149 train loss: 0.01675429940223694 validation: auc: 0.6435856752274662 --- acc: 80.80000000000001 --- loss: 0.580630362033844\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001700448989868164\n",
            "epoch: 150 train loss: 0.017854338511824608 validation: auc: 0.6437990229035006 --- acc: 80.80000000000001 --- loss: 0.5808219313621521\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001696375757455826\n",
            "epoch: 151 train loss: 0.01730567030608654 validation: auc: 0.644051813007037 --- acc: 80.80000000000001 --- loss: 0.5809929370880127\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0016688290983438493\n",
            "epoch: 152 train loss: 0.017695695161819458 validation: auc: 0.6442830890592085 --- acc: 80.85 --- loss: 0.5811901688575745\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.001647307351231575\n",
            "epoch: 153 train loss: 0.01784476451575756 validation: auc: 0.6444390659316033 --- acc: 80.9 --- loss: 0.5814027190208435\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0016558794304728508\n",
            "epoch: 154 train loss: 0.016774971038103104 validation: auc: 0.6445896642911568 --- acc: 80.85 --- loss: 0.5815849900245667\n",
            "<Timer(Thread-10841, started 140341898368768)>\n",
            "Memory Usage:\n",
            "Allocated: 0.3 GB\n",
            "Cached:    1.5 GB\n",
            "    - loss: 0.0016018141061067582\n",
            "epoch: 155 train loss: 0.018225615844130516 validation: auc: 0.644763569539689 --- acc: 80.7 --- loss: 0.5817633271217346\n",
            "time up, break\n",
            "test auc: 0.6797437709010409 test acc: 81.15 test loss 0.5238107442855835\n",
            "0.01\n",
            "0.6839711310608823\n",
            "84.35000000000001\n",
            "0.40878257155418396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachineModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FeaturesLinear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FieldAwareFactorizationMachine. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YV9Nw507VsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e105a6fa-2e97-42d5-aec2-38366f72257b"
      },
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import log10\n",
        "\n",
        "\n",
        "def plot_learning_rate():\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/learning_rate.json\",\"r\") as dump_f:\n",
        "        load_dict = json.load(dump_f)\n",
        "\n",
        "    for i,(k,v) in enumerate(load_dict.items()):\n",
        "        print((k,v))\n",
        "    keys = list(load_dict.keys())\n",
        "    keys.reverse()\n",
        "\n",
        "    # markers = 'v^8sph'\n",
        "    i = 0\n",
        "    for key in keys:\n",
        "        plt.plot(load_dict[key]['EPOCH'],[log10(i) for i in load_dict[key]['val_loss_list']])\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('log(Logloss)')\n",
        "\n",
        "    my_x_ticks = np.arange(0, len(load_dict[keys[0]]['EPOCH']), 50)\n",
        "    plt.xticks(my_x_ticks)\n",
        "\n",
        "    plt.legend(keys, loc='upper right')\n",
        "    # plt.figure()\n",
        "    plt.savefig(\"/content/drive/My Drive/Colab Notebooks/lr_plot.png\",dpi=300)\n",
        "    plt.show()\n",
        "    \n",
        "def plot_weight_decay():\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/Weight_decay.json\",\"r\") as dump_f:\n",
        "        load_dict = json.load(dump_f)\n",
        "\n",
        "    for i,(k,v) in enumerate(load_dict.items()):\n",
        "        print((k,v))\n",
        "    keys = list(load_dict.keys())\n",
        "    keys.reverse()\n",
        "\n",
        "    # markers = 'v^8sph1234Dd'\n",
        "    i = 0\n",
        "    for key in keys:\n",
        "        plt.plot(load_dict[key]['EPOCH'],[log10(i) for i in load_dict[key]['val_loss_list']])\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('log(Logloss)')\n",
        "\n",
        "    my_x_ticks = np.arange(0, len(load_dict[keys[0]]['EPOCH']), 50)\n",
        "    plt.xticks(my_x_ticks)\n",
        "\n",
        "    plt.legend(keys, loc='upper right')\n",
        "    # plt.figure()\n",
        "    plt.savefig(\"/content/drive/My Drive/Colab Notebooks/wd_plot.png\",dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_embed_dim():\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/EMBED_DIM.json\",\"r\") as dump_f:\n",
        "        load_dict = json.load(dump_f)\n",
        "\n",
        "    for i,(k,v) in enumerate(load_dict.items()):\n",
        "        print((k,v))\n",
        "    keys = list(load_dict.keys())\n",
        "    keys.reverse()\n",
        "\n",
        "    # markers = 'v^8sph1234Dd'\n",
        "    i = 0\n",
        "    for key in keys:\n",
        "        plt.plot(load_dict[key]['EPOCH'],[log10(i) for i in load_dict[key]['val_loss_list']])\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('log(Logloss)')\n",
        "\n",
        "    my_x_ticks = np.arange(0, len(load_dict[keys[0]]['EPOCH']), 50)\n",
        "    plt.xticks(my_x_ticks)\n",
        "\n",
        "    plt.legend(keys, loc='upper right')\n",
        "    # plt.figure()\n",
        "    plt.savefig(\"/content/drive/My Drive/Colab Notebooks/ed_plot.png\",dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def plot_one_learning_rate():\n",
        "    with open(\"/content/drive/My Drive/Colab Notebooks/learning_rate.json\",\"r\") as dump_f:\n",
        "        load_dict = json.load(dump_f)\n",
        "\n",
        "    for i,(k,v) in enumerate(load_dict.items()):\n",
        "        print((k,v))\n",
        "    keys = list(load_dict.keys())\n",
        "\n",
        "    key = keys[0]\n",
        "    epoch = load_dict[key]['EPOCH']\n",
        "    train_loss = load_dict[key]['train_loss_list']\n",
        "    val_loss = load_dict[key]['val_loss_list']\n",
        "    val_auc = [i for i in load_dict[key]['val_auc_list']]\n",
        "    val_acc = [i/100 for i in load_dict[key]['val_acc_list']]\n",
        "\n",
        "\n",
        "    plt.plot(epoch,train_loss)\n",
        "    plt.plot(epoch, val_loss)\n",
        "    plt.plot(epoch, val_auc)\n",
        "    plt.plot(epoch, val_acc)\n",
        "\n",
        "    plt.legend(['train_loss', 'val_loss','val_auc','val_acc'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_rate()\n",
        "plot_weight_decay()\n",
        "plot_embed_dim()\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('0.001', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186], 'train_loss_list': [1.836904764175415, 1.8859509229660034, 1.7714141607284546, 1.7496421337127686, 1.724552869796753, 1.6540541648864746, 1.579086422920227, 1.4885574579238892, 1.4972021579742432, 1.386804461479187, 1.3390752077102661, 1.3099610805511475, 1.221330165863037, 1.1706433296203613, 1.069929599761963, 0.9798257350921631, 0.896984875202179, 0.8956053853034973, 0.8471810221672058, 0.801831066608429, 0.7911431789398193, 0.7272192239761353, 0.7293829321861267, 0.6600069403648376, 0.6377235651016235, 0.6323726773262024, 0.6622471809387207, 0.5777216553688049, 0.5457560420036316, 0.5253539085388184, 0.5449205636978149, 0.5020970702171326, 0.49554958939552307, 0.4444139897823334, 0.46393027901649475, 0.4316258132457733, 0.4379468858242035, 0.4040288031101227, 0.38960427045822144, 0.35513609647750854, 0.34552520513534546, 0.3580416440963745, 0.3140948414802551, 0.29239821434020996, 0.2962917983531952, 0.2842364013195038, 0.26704469323158264, 0.25268107652664185, 0.24104875326156616, 0.2327938973903656, 0.21603244543075562, 0.21334244310855865, 0.1949964463710785, 0.1947210729122162, 0.19280704855918884, 0.17171335220336914, 0.1756700724363327, 0.15422403812408447, 0.14969119429588318, 0.146799698472023, 0.1356203407049179, 0.13759338855743408, 0.12059956789016724, 0.12007898092269897, 0.12436150759458542, 0.11066044867038727, 0.10411068052053452, 0.1085272803902626, 0.09863101691007614, 0.09782684594392776, 0.09221462905406952, 0.08909060806035995, 0.08644938468933105, 0.08423889428377151, 0.07870535552501678, 0.07731341570615768, 0.07835149019956589, 0.07487541437149048, 0.07315053790807724, 0.06766156107187271, 0.06944028288125992, 0.0675673857331276, 0.07355883717536926, 0.06553339213132858, 0.05889824032783508, 0.05981285870075226, 0.05164843425154686, 0.05452520400285721, 0.05714884400367737, 0.05599728971719742, 0.05882599204778671, 0.05125144124031067, 0.05387184023857117, 0.04992411658167839, 0.046894680708646774, 0.04942421615123749, 0.04788792133331299, 0.048507727682590485, 0.04735086113214493, 0.04571329057216644, 0.04377397894859314, 0.04434399679303169, 0.04050949960947037, 0.04629034921526909, 0.042485561221838, 0.04125501215457916, 0.040400080382823944, 0.039157282561063766, 0.03928789123892784, 0.04078161343932152, 0.03508525714278221, 0.0397983156144619, 0.03534146770834923, 0.036002904176712036, 0.03546987473964691, 0.03372111916542053, 0.03572005778551102, 0.035715483129024506, 0.03939384967088699, 0.032721444964408875, 0.0353223979473114, 0.03526543080806732, 0.033359356224536896, 0.03283682465553284, 0.032837990671396255, 0.03158807009458542, 0.03274017199873924, 0.030874907970428467, 0.03232261538505554, 0.02932228334248066, 0.033676911145448685, 0.03139417991042137, 0.030260786414146423, 0.03173588588833809, 0.026690704748034477, 0.027681119740009308, 0.028755977749824524, 0.027712754905223846, 0.026710372418165207, 0.02881099469959736, 0.02697073295712471, 0.028402453288435936, 0.02678266353905201, 0.02820270135998726, 0.029078679159283638, 0.028593316674232483, 0.028162742033600807, 0.026341432705521584, 0.02463689260184765, 0.023627178743481636, 0.026899028569459915, 0.02282065898180008, 0.025343189015984535, 0.027366172522306442, 0.023862488567829132, 0.023709280416369438, 0.024434149265289307, 0.02333868481218815, 0.025167914107441902, 0.025477590039372444, 0.023575257509946823, 0.02292451076209545, 0.021072914823889732, 0.021230095997452736, 0.023157598450779915, 0.026262277737259865, 0.024686666205525398, 0.0225429255515337, 0.020738115534186363, 0.023733895272016525, 0.020306291058659554, 0.023200834169983864, 0.022680265828967094, 0.021589966490864754, 0.02109515108168125, 0.022140584886074066, 0.02202509716153145, 0.021364746615290642, 0.02102416753768921, 0.018635343760252, 0.02062991075217724, 0.021453218534588814, 0.0183489378541708, 0.02188941277563572, 0.021633150056004524, 0.020490653812885284, 0.022497380152344704], 'val_auc_list': [0.538731997836325, 0.538707074958785, 0.5387661514092502, 0.5387273824886324, 0.5388658429194105, 0.5390486106880376, 0.5390799950523473, 0.5391464560591208, 0.5392332245957416, 0.5394713765366801, 0.539604298550227, 0.5398239891003949, 0.5401101406573363, 0.5403298312075042, 0.5407027513010665, 0.5411975165737135, 0.5417033586808229, 0.5423310459670169, 0.5431691931079936, 0.5441218008717469, 0.5452294843179717, 0.5466602421026786, 0.5481722300067754, 0.5502140598259829, 0.5522872740095003, 0.5545395636834906, 0.5570743126362682, 0.5600299812986111, 0.5631222642526552, 0.5660705483586901, 0.5689099102591795, 0.5721886532600047, 0.5752790900749717, 0.5784193726450189, 0.5811091972802679, 0.5836919458490486, 0.5861916181593625, 0.588816827926915, 0.5910008104550548, 0.5930278711616461, 0.5949884708614639, 0.5966278423618765, 0.5982118296899779, 0.5996259722229914, 0.6006431948544412, 0.6015551875584995, 0.6024985646268676, 0.6029545609788968, 0.6035527100398581, 0.6040013218355791, 0.6044277799623757, 0.6049133145396376, 0.6055650016338331, 0.6058437686344662, 0.6063089956818808, 0.60699575941854, 0.6075163706382657, 0.6080129820499898, 0.608740360846344, 0.609233279979914, 0.6096892763319433, 0.6101526572402806, 0.6109298817917149, 0.6115501845216006, 0.6122424866754912, 0.6129643270546143, 0.613617860287887, 0.6141292408122273, 0.6147513896811901, 0.615364307854768, 0.6157963043987955, 0.6163021465059049, 0.6167913733613208, 0.6174098299521296, 0.6179101336420079, 0.6184768983386594, 0.6190104325319243, 0.6196547350698117, 0.6202251920446176, 0.6207587262378825, 0.6212479530932983, 0.6217390260877913, 0.6219753318896526, 0.6224571741887603, 0.6228171713087834, 0.6232417832965029, 0.6238067018540775, 0.6241814680867168, 0.6246891563329032, 0.6250694609827737, 0.6254571501889523, 0.6257709938320494, 0.6261937596806918, 0.6265020649065576, 0.6268879079736593, 0.6272414436069127, 0.6276042099355512, 0.6280195912278855, 0.6283777422088316, 0.6287432777460857, 0.6291494283430347, 0.6295038870458268, 0.629874961000312, 0.6302146505904875, 0.6304934175911209, 0.6306577239689775, 0.6309087988834552, 0.6311469508243933, 0.6314404869376429, 0.6317174077991992, 0.6320718665019911, 0.6322657111050803, 0.6325832470263315, 0.6328195528281928, 0.6330835507162097, 0.633458316948849, 0.6336724690817858, 0.6339456976651879, 0.6342078494141279, 0.6345456928652263, 0.6347616911372401, 0.6349961508000244, 0.6352730716615806, 0.6355259927151351, 0.6359099896431598, 0.6362422946770272, 0.6365413692075079, 0.6368293669035263, 0.6371192107386218, 0.6373573626795601, 0.6376822831571194, 0.6380035113565246, 0.6383524316420853, 0.6386385831990268, 0.6388619660273487, 0.6391259639153656, 0.6393862695252284, 0.6395653450157013, 0.6397628818969447, 0.639945649665572, 0.6401136483215827, 0.6404071844348322, 0.6406361056803853, 0.6409056419856334, 0.6410828713370293, 0.6412582545493483, 0.6414760989604391, 0.6416884049542988, 0.6418637881666178, 0.6420760941604775, 0.6423363997703403, 0.6425874746848179, 0.6429123951623772, 0.6431450086860844, 0.6435068519451844, 0.643802234197511, 0.6441050010061458, 0.6443173070000057, 0.6445573050800208, 0.6447437651268021, 0.6448969946701965, 0.6450834547169776, 0.6452126844523705, 0.6453825292474584, 0.6455542201816231, 0.6456612962480915, 0.6457905259834844, 0.6458883713545676, 0.6460489854542703, 0.646168984494278, 0.6462483684745907, 0.646401598017985, 0.6465917503429203, 0.6467375953300066, 0.6469683627146368, 0.6470883617546443, 0.6472212837681914, 0.6473985131195874, 0.647564665636521, 0.6477326642925318, 0.6478932783922344, 0.648100045968863, 0.6483049674064145, 0.6484101973338059, 0.6485855805461249, 0.6487221948378259, 0.6488735782421433], 'val_acc_list': [43.15, 43.85, 44.35, 45.25, 46.45, 47.75, 48.65, 49.65, 50.949999999999996, 51.7, 53.2, 55.25, 57.05, 58.85, 60.95, 62.5, 64.3, 65.64999999999999, 67.45, 69.19999999999999, 70.7, 71.55, 72.3, 73.55000000000001, 74.55000000000001, 75.75, 76.44999999999999, 76.85, 77.25, 77.60000000000001, 77.7, 77.7, 77.5, 77.55, 77.25, 77.25, 76.75, 76.6, 76.7, 76.55, 76.6, 76.55, 76.44999999999999, 76.5, 76.6, 76.8, 77.0, 77.05, 77.2, 77.10000000000001, 77.05, 77.25, 77.10000000000001, 77.25, 77.3, 77.45, 77.5, 77.55, 77.64999999999999, 77.64999999999999, 77.64999999999999, 77.55, 77.64999999999999, 77.60000000000001, 77.4, 77.5, 77.7, 77.60000000000001, 77.7, 77.7, 77.60000000000001, 77.75, 77.75, 77.8, 77.8, 77.9, 77.8, 77.85, 77.8, 77.95, 77.95, 77.9, 77.9, 77.85, 77.8, 77.8, 77.8, 77.85, 77.95, 77.9, 78.0, 78.0, 78.0, 78.05, 78.05, 78.2, 78.3, 78.25, 78.35, 78.35, 78.4, 78.4, 78.4, 78.4, 78.45, 78.60000000000001, 78.7, 78.7, 78.85, 78.95, 79.05, 79.05, 79.10000000000001, 79.10000000000001, 79.14999999999999, 79.2, 79.25, 79.25, 79.3, 79.4, 79.5, 79.55, 79.55, 79.60000000000001, 79.60000000000001, 79.60000000000001, 79.60000000000001, 79.5, 79.5, 79.5, 79.55, 79.55, 79.60000000000001, 79.60000000000001, 79.75, 79.85, 79.85, 79.80000000000001, 79.9, 79.9, 79.9, 79.9, 79.9, 79.95, 79.95, 79.95, 79.95, 79.95, 79.95, 79.95, 79.95, 79.9, 79.95, 80.0, 80.0, 80.0, 80.0, 80.05, 80.10000000000001, 80.10000000000001, 80.10000000000001, 80.10000000000001, 80.05, 80.05, 80.0, 80.05, 80.0, 80.0, 80.05, 80.15, 80.10000000000001, 80.2, 80.2, 80.25, 80.25, 80.30000000000001, 80.30000000000001, 80.4, 80.4, 80.4, 80.4, 80.4, 80.4, 80.4, 80.4, 80.35, 80.4], 'val_loss_list': [1.8760905265808105, 1.8406217098236084, 1.8031013011932373, 1.7626677751541138, 1.718809723854065, 1.6713380813598633, 1.6202312707901, 1.5657298564910889, 1.5081781148910522, 1.4481174945831299, 1.3862730264663696, 1.3234833478927612, 1.2607258558273315, 1.1991469860076904, 1.1398447751998901, 1.0839673280715942, 1.0324606895446777, 0.9859825968742371, 0.9447503089904785, 0.9087932109832764, 0.8779065012931824, 0.8515912890434265, 0.8292773962020874, 0.810215413570404, 0.7937140464782715, 0.7790980339050293, 0.7656794786453247, 0.7530709505081177, 0.7410959005355835, 0.7296913862228394, 0.7188209295272827, 0.7085042595863342, 0.6988702416419983, 0.6899908185005188, 0.6819051504135132, 0.6745840907096863, 0.6680551767349243, 0.6622010469436646, 0.6568798422813416, 0.6519676446914673, 0.6473066210746765, 0.6427897810935974, 0.6383178234100342, 0.6338950395584106, 0.6295496821403503, 0.6252490282058716, 0.6210442185401917, 0.6170089244842529, 0.613139271736145, 0.6094996333122253, 0.6061285734176636, 0.6030139327049255, 0.6001940965652466, 0.59759521484375, 0.5951926112174988, 0.5929995775222778, 0.5909979343414307, 0.589168131351471, 0.5874215364456177, 0.5857395529747009, 0.584128201007843, 0.5825379490852356, 0.5810138583183289, 0.5795109272003174, 0.5780439376831055, 0.5766468644142151, 0.5753198266029358, 0.5740700960159302, 0.5729005336761475, 0.571780264377594, 0.570725679397583, 0.5696660280227661, 0.5686397552490234, 0.5676763653755188, 0.5667454600334167, 0.5658080577850342, 0.5649206638336182, 0.5641109347343445, 0.5633698105812073, 0.5626772046089172, 0.5620319843292236, 0.5614104866981506, 0.560839056968689, 0.5603342056274414, 0.5598196983337402, 0.5592533946037292, 0.5586588978767395, 0.5580416917800903, 0.5574328899383545, 0.5569000244140625, 0.5564122796058655, 0.5559338927268982, 0.5555315017700195, 0.555198073387146, 0.5548978447914124, 0.5546256899833679, 0.5543645024299622, 0.5541175007820129, 0.55387282371521, 0.553604245185852, 0.5533640384674072, 0.5531447529792786, 0.5529254674911499, 0.5526518821716309, 0.5523615479469299, 0.5520479083061218, 0.5517603754997253, 0.551515519618988, 0.5513188242912292, 0.5511730909347534, 0.5510535836219788, 0.5509255528450012, 0.550787091255188, 0.5506114363670349, 0.5504212379455566, 0.5502511262893677, 0.5501092076301575, 0.5500050187110901, 0.5499281883239746, 0.5498413443565369, 0.5497194528579712, 0.5496122241020203, 0.549533486366272, 0.5494428873062134, 0.549354612827301, 0.5492905378341675, 0.5492316484451294, 0.54914790391922, 0.5490720868110657, 0.5490121841430664, 0.5489547848701477, 0.5488805770874023, 0.5488201975822449, 0.5487728118896484, 0.5487373471260071, 0.5487046241760254, 0.5486457943916321, 0.5486242771148682, 0.5486326813697815, 0.5486315488815308, 0.5486382246017456, 0.5486493706703186, 0.5486655831336975, 0.5486751198768616, 0.5486471056938171, 0.548596203327179, 0.5485482215881348, 0.5484973788261414, 0.5484516024589539, 0.5484161376953125, 0.5483877658843994, 0.548363447189331, 0.5483517050743103, 0.5483334064483643, 0.5483366847038269, 0.5483666658401489, 0.5483844876289368, 0.5484302043914795, 0.5484979748725891, 0.5485389232635498, 0.548588752746582, 0.5486657619476318, 0.5487518906593323, 0.5488378405570984, 0.5489147305488586, 0.5490185022354126, 0.5491634607315063, 0.5493050217628479, 0.5494615435600281, 0.5496171116828918, 0.5497663021087646, 0.5498899221420288, 0.5499760508537292, 0.5500307679176331, 0.5500702261924744, 0.5500912070274353, 0.5501234531402588, 0.5501512289047241, 0.5501838326454163, 0.5502700805664062, 0.5503683686256409, 0.5504642128944397, 0.5505492091178894, 0.5506478548049927, 0.5507619380950928, 0.5509000420570374, 0.5510390996932983], 'test_auc': 0.6478292467138524, 'test_acc': 79.60000000000001, 'test_loss': 0.5730694532394409, 'best_val_auc': 0.6431450086860844, 'best_val_acc': 80.0, 'best_val_loss': 0.5483334064483643})\n",
            "('0.002', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206], 'train_loss_list': [1.2874586582183838, 1.2716341018676758, 1.3163549900054932, 1.1972758769989014, 1.1490154266357422, 1.0875506401062012, 1.0474827289581299, 1.003740906715393, 0.945115864276886, 0.8961700201034546, 0.826058030128479, 0.8118289709091187, 0.7989859580993652, 0.7323603630065918, 0.6828819513320923, 0.6133619546890259, 0.5478776693344116, 0.5246506929397583, 0.47778400778770447, 0.4178697168827057, 0.4237020015716553, 0.353753000497818, 0.339811772108078, 0.28521135449409485, 0.26757389307022095, 0.23567703366279602, 0.21298031508922577, 0.18294550478458405, 0.17516352236270905, 0.152764692902565, 0.13764837384223938, 0.11841736733913422, 0.11956769973039627, 0.09885972738265991, 0.08758247643709183, 0.08249527961015701, 0.07988245785236359, 0.07660897821187973, 0.06604527682065964, 0.06090318039059639, 0.057947345077991486, 0.054299913346767426, 0.04559193551540375, 0.046337686479091644, 0.04397251084446907, 0.04330592229962349, 0.04329989477992058, 0.039856065064668655, 0.036264631897211075, 0.03657785430550575, 0.036321721971035004, 0.03408300504088402, 0.03282551467418671, 0.03306920826435089, 0.031425658613443375, 0.030958542600274086, 0.029662303626537323, 0.033177174627780914, 0.029278015717864037, 0.027813954278826714, 0.027895024046301842, 0.024852734059095383, 0.025109829381108284, 0.024230653420090675, 0.02523304894566536, 0.023879436776041985, 0.021761441603302956, 0.028752682730555534, 0.026010088622570038, 0.02633272111415863, 0.025459028780460358, 0.02467179298400879, 0.02062199078500271, 0.018742747604846954, 0.021345172077417374, 0.02353479713201523, 0.021792778745293617, 0.021147623658180237, 0.02132297493517399, 0.020292067900300026, 0.022665634751319885, 0.01842622458934784, 0.02060643397271633, 0.019942155107855797, 0.02086424082517624, 0.018681610003113747, 0.019364751875400543, 0.0176586601883173, 0.018224548548460007, 0.01598571613430977, 0.018557675182819366, 0.01590786501765251, 0.018575822934508324, 0.0180448479950428, 0.01607089675962925, 0.01582009345293045, 0.01609639637172222, 0.0156698040664196, 0.016326194629073143, 0.015797458589076996, 0.015308503061532974, 0.015056551434099674, 0.015868252143263817, 0.013569086790084839, 0.016038643196225166, 0.016372758895158768, 0.01552263367921114, 0.015143334865570068, 0.01306866854429245, 0.011699773371219635, 0.012964819557964802, 0.01261666789650917, 0.015132411383092403, 0.013843256048858166, 0.013050096109509468, 0.013858611695468426, 0.012878150679171085, 0.013131174258887768, 0.014176713302731514, 0.01327640563249588, 0.011588783003389835, 0.013245144858956337, 0.01164855994284153, 0.012783583253622055, 0.012345951050519943, 0.012787760235369205, 0.012502538971602917, 0.010805304162204266, 0.01077606063336134, 0.01212818268686533, 0.010205041617155075, 0.012945454567670822, 0.009984833188354969, 0.010748917236924171, 0.012068468146026134, 0.011419394053518772, 0.009812611155211926, 0.010821199044585228, 0.010740179568529129, 0.011092215776443481, 0.009276488795876503, 0.010493647307157516, 0.008714350871741772, 0.010183155536651611, 0.00982933584600687, 0.009721716865897179, 0.009467972442507744, 0.008619075641036034, 0.009187509305775166, 0.008497471921145916, 0.009400543756783009, 0.00800575315952301, 0.008136384189128876, 0.009366699494421482, 0.010335991159081459, 0.008717808872461319, 0.00831806380301714, 0.008143157698214054, 0.00857788510620594, 0.008465186692774296, 0.008738345466554165, 0.008343282155692577, 0.007884933613240719, 0.007157495245337486, 0.00883981492370367, 0.008282210677862167, 0.00949135608971119, 0.009031631052494049, 0.007665165700018406, 0.00746494997292757, 0.007513630203902721, 0.007265765685588121, 0.007015218026936054, 0.007596946787089109, 0.008085111156105995, 0.008399175480008125, 0.007987487129867077, 0.006912942510098219, 0.007124560419470072, 0.007780488580465317, 0.006710221990942955, 0.005890365224331617, 0.0073191397823393345, 0.006265376228839159, 0.00693075405433774, 0.007342146709561348, 0.006605221424251795, 0.00794233102351427, 0.0063257901929318905, 0.006406234577298164, 0.0065564983524382114, 0.007290731184184551, 0.007480822969228029, 0.0077836159616708755, 0.0065554408356547356, 0.0061803217977285385, 0.006791766732931137, 0.007113476283848286, 0.006384772714227438, 0.006746985949575901, 0.006153642665594816, 0.006593632511794567, 0.006418174598366022, 0.005865946412086487, 0.006265916395932436, 0.0054811742156744, 0.006375053431838751], 'val_auc_list': [0.5039040557240759, 0.5050047045275812, 0.5064160628019324, 0.5085401218964161, 0.5111486771149308, 0.5144330693180542, 0.5190252499719132, 0.5247847854173688, 0.5317327407032918, 0.5396233569261881, 0.5485795135378048, 0.5582466155488147, 0.5684561425682508, 0.5784128890012359, 0.5881414307381193, 0.5971906246489158, 0.6058904898326031, 0.6131684642175036, 0.6196038366475677, 0.6259584597236266, 0.6313002050331422, 0.6354061341422312, 0.6383710397708122, 0.6402510953825413, 0.6415711717784519, 0.6426472446916076, 0.6438479524772498, 0.6453102179530391, 0.647002443545669, 0.648852657004831, 0.6513278002471632, 0.6533430232558138, 0.6548965706100438, 0.6562763313110886, 0.6573734692731154, 0.658195006179081, 0.6587813167059882, 0.6588445118526007, 0.6590235647680036, 0.6590639394450062, 0.6589989888776542, 0.6590814936523985, 0.658990211773958, 0.6588831311088642, 0.658737431187507, 0.658505715649927, 0.6581388327154253, 0.657673646219526, 0.6573875126390293, 0.6570785585889225, 0.6569030165149984, 0.6568714189416919, 0.6568503538928211, 0.6569065273564767, 0.6569837658690035, 0.6570399393326593, 0.6569644562408719, 0.6570346730704415, 0.657168085046624, 0.6571996826199304, 0.6572418127176721, 0.6574489523649029, 0.6575402342433435, 0.6576139619143917, 0.657819346140883, 0.6579106280193238, 0.6579299376474553, 0.6580563279406808, 0.6581388327154253, 0.6583266627345242, 0.6585285361195372, 0.6586601926749803, 0.6589551033591731, 0.6592570357263229, 0.6594624199528143, 0.6596362066059993, 0.6597081788563082, 0.6596941354903944, 0.6596186523986068, 0.6596800921244804, 0.6597309993259184, 0.6597380210088755, 0.65986616672284, 0.6599820244916301, 0.6601277244129873, 0.6602488484439951, 0.6603805049994383, 0.6603331086394787, 0.6604910965060105, 0.6606859482080666, 0.6608386698123807, 0.6609808588922592, 0.6610580974047859, 0.6611634226491405, 0.6613688068756319, 0.6615390826873384, 0.6617058476575666, 0.6618375042130098, 0.6619972475002809, 0.6620621980676329, 0.6620270896528481, 0.6620148017076734, 0.6619533619817999, 0.6619270306707111, 0.6619112318840581, 0.6619656499269745, 0.6619358077744074, 0.6620165571284125, 0.662221941354904, 0.6623974834288282, 0.6625291399842714, 0.6626818615885856, 0.6627555892596337, 0.6628451157173352, 0.6628100073025504, 0.6626713290641502, 0.6626730844848894, 0.6627661217840692, 0.6627292579485451, 0.662859159083249, 0.6629802831142568, 0.6630838529378721, 0.6632155094933153, 0.663255884170318, 0.6633138130547129, 0.6634314262442422, 0.6635156864397258, 0.6635613273789462, 0.6637298477699135, 0.6638141079653972, 0.6639176777890126, 0.6638948573194022, 0.6638474609594427, 0.6638316621727897, 0.6638685260083137, 0.663886080215706, 0.663949275362319, 0.6640177367711493, 0.6640036934052355, 0.664094975283676, 0.6642143438939445, 0.664282805302775, 0.6644056847545219, 0.6645022328951803, 0.6645689388832715, 0.6645619172003147, 0.6646725087068869, 0.6647199050668463, 0.6647128833838896, 0.6647637905853275, 0.6647743231097629, 0.6648585833052465, 0.6649112459274239, 0.6649112459274239, 0.6649709302325582, 0.6650218374339962, 0.6650788886080216, 0.6651657819346141, 0.6653114818559713, 0.6654922901921132, 0.6656362346927311, 0.6656502780586452, 0.6656608105830807, 0.6656660768452982, 0.6656994298393438, 0.6656695876867769, 0.6655870829120323, 0.6655238877654196, 0.6655607516009436, 0.6655835720705539, 0.6656151696438604, 0.665620435906078, 0.6656397455342097, 0.6656099033816425, 0.6655502190765084, 0.6655712841253792, 0.6656660768452982, 0.6656397455342096, 0.6655976154364678, 0.6656362346927311, 0.6656572997416021, 0.6656116588023818, 0.6656309684305135, 0.665548463655769, 0.6654624480395461, 0.6654712251432423, 0.665455426356589, 0.6654396275699359, 0.6654896570610043, 0.6656046371194247, 0.6656572997416019, 0.6656432563756881, 0.6657810569037187, 0.6658240647118301, 0.665831086394787, 0.6659785417368834, 0.6660645573531064, 0.6661997247500282, 0.666285740366251, 0.6663805330861701, 0.6665157004830917, 0.6665174559038312, 0.6666315582518818, 0.6665595860015728, 0.6664753258060893, 0.6664911245927423, 0.6664963908549602], 'val_acc_list': [62.2, 63.449999999999996, 64.25, 65.4, 67.2, 68.60000000000001, 70.6, 71.95, 73.75, 75.25, 75.75, 76.75, 77.25, 77.7, 77.8, 77.7, 77.4, 77.3, 76.44999999999999, 75.6, 75.44999999999999, 75.6, 75.6, 75.94999999999999, 76.64999999999999, 77.3, 77.75, 78.05, 77.95, 77.60000000000001, 77.3, 77.25, 76.8, 76.64999999999999, 76.75, 77.2, 77.35, 77.9, 78.10000000000001, 77.95, 78.05, 78.14999999999999, 78.10000000000001, 78.14999999999999, 78.4, 78.3, 78.25, 78.4, 78.55, 78.5, 78.55, 78.45, 78.4, 78.4, 78.25, 78.3, 78.35, 78.5, 78.55, 78.60000000000001, 78.60000000000001, 78.7, 78.85, 78.85, 78.9, 78.95, 79.10000000000001, 79.14999999999999, 79.25, 79.25, 79.25, 79.25, 79.25, 79.25, 79.4, 79.45, 79.55, 79.55, 79.60000000000001, 79.65, 79.60000000000001, 79.55, 79.55, 79.55, 79.55, 79.4, 79.45, 79.55, 79.55, 79.65, 79.7, 79.75, 79.7, 79.80000000000001, 79.80000000000001, 79.80000000000001, 79.80000000000001, 79.75, 79.85, 79.9, 79.95, 80.0, 80.05, 80.10000000000001, 80.10000000000001, 80.05, 80.05, 80.10000000000001, 80.10000000000001, 80.25, 80.25, 80.25, 80.30000000000001, 80.35, 80.4, 80.4, 80.45, 80.5, 80.55, 80.7, 80.65, 80.60000000000001, 80.65, 80.65, 80.75, 80.80000000000001, 80.75, 80.75, 80.80000000000001, 80.9, 80.85, 80.80000000000001, 80.7, 80.7, 80.7, 80.7, 80.7, 80.75, 80.80000000000001, 80.75, 80.85, 80.9, 80.85, 80.85, 80.85, 80.85, 80.9, 80.9, 80.95, 80.95, 80.95, 80.95, 80.95, 80.9, 80.9, 80.9, 80.85, 80.85, 80.80000000000001, 80.80000000000001, 80.75, 80.75, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.85, 80.95, 80.95, 81.0, 81.05, 81.05, 81.05, 81.05, 81.05, 81.05, 81.10000000000001, 81.10000000000001, 81.05, 81.05, 81.05, 81.05, 81.05, 81.05, 81.0, 81.0, 81.0, 81.0, 81.05, 81.10000000000001, 81.05, 81.0, 81.0, 81.0, 81.0, 81.0, 80.95, 80.95, 80.95, 81.0, 81.0, 80.95, 80.95, 80.95, 81.0], 'val_loss_list': [1.2099837064743042, 1.180429458618164, 1.1478339433670044, 1.1122353076934814, 1.0745751857757568, 1.0362807512283325, 0.9989697337150574, 0.9641780853271484, 0.9328985810279846, 0.9050637483596802, 0.8797298669815063, 0.8551856875419617, 0.8300054669380188, 0.8037010431289673, 0.777069091796875, 0.7511487007141113, 0.7270233035087585, 0.7055341601371765, 0.686837911605835, 0.6702222228050232, 0.6545398831367493, 0.6388387680053711, 0.6229928135871887, 0.607873797416687, 0.5947388410568237, 0.5842465162277222, 0.5758506655693054, 0.5690714120864868, 0.5636835098266602, 0.5597215890884399, 0.556929886341095, 0.5550762414932251, 0.5537514686584473, 0.5525515079498291, 0.5511420369148254, 0.5495282411575317, 0.54788738489151, 0.5467478036880493, 0.5460875630378723, 0.5457110404968262, 0.5455837845802307, 0.5455754995346069, 0.5457006692886353, 0.5458738207817078, 0.546089768409729, 0.5464033484458923, 0.5467265844345093, 0.5470622777938843, 0.5473567247390747, 0.5476773977279663, 0.54803866147995, 0.5485881567001343, 0.549297571182251, 0.5501010417938232, 0.5508612990379333, 0.5514876246452332, 0.5520516633987427, 0.5526919960975647, 0.553357720375061, 0.5540119409561157, 0.5545880198478699, 0.5551237463951111, 0.5556044578552246, 0.5560494065284729, 0.5564829111099243, 0.5568953156471252, 0.5573668479919434, 0.5578291416168213, 0.5582460165023804, 0.5586938261985779, 0.5591549277305603, 0.5596191883087158, 0.5600976347923279, 0.560539722442627, 0.5609731674194336, 0.5614905953407288, 0.5620893239974976, 0.5627079606056213, 0.5633412003517151, 0.5640225410461426, 0.5646498799324036, 0.5651902556419373, 0.5656939744949341, 0.5661624670028687, 0.566593587398529, 0.5670415163040161, 0.5675334930419922, 0.5680516362190247, 0.5685300230979919, 0.5689910650253296, 0.5695006251335144, 0.5701032280921936, 0.5707782506942749, 0.5714235901832581, 0.5720711946487427, 0.5727394223213196, 0.5734003782272339, 0.5740373730659485, 0.5746684670448303, 0.5753112435340881, 0.575975775718689, 0.5766475200653076, 0.5773313641548157, 0.5780173540115356, 0.5786880850791931, 0.5793041586875916, 0.5798802375793457, 0.5804644227027893, 0.5810798406600952, 0.5817476511001587, 0.5824272036552429, 0.5830864310264587, 0.583783745765686, 0.5845513939857483, 0.5853718519210815, 0.5861943364143372, 0.586936354637146, 0.5876269340515137, 0.5882726311683655, 0.5888295769691467, 0.5893087983131409, 0.5897313356399536, 0.5902088284492493, 0.5908603668212891, 0.5916198492050171, 0.5924197435379028, 0.5933195352554321, 0.5943026542663574, 0.595271646976471, 0.5961311459541321, 0.5968817472457886, 0.597538948059082, 0.5981029868125916, 0.5986472964286804, 0.599283754825592, 0.6001102328300476, 0.6010865569114685, 0.6021108627319336, 0.603123664855957, 0.6040435433387756, 0.6048144698143005, 0.6055963039398193, 0.6064382195472717, 0.6072317957878113, 0.607991099357605, 0.6087583303451538, 0.6095806360244751, 0.6104340553283691, 0.6113023161888123, 0.6122121810913086, 0.6130754351615906, 0.6138471961021423, 0.6145314574241638, 0.615021288394928, 0.6154384613037109, 0.6159316301345825, 0.6164963841438293, 0.6170942187309265, 0.6177152991294861, 0.6183680891990662, 0.619125247001648, 0.6199073195457458, 0.6205353140830994, 0.6211816668510437, 0.6220628023147583, 0.6230549812316895, 0.6241597533226013, 0.625377893447876, 0.6264944672584534, 0.6275727152824402, 0.6285407543182373, 0.6295213103294373, 0.6305381059646606, 0.6315281987190247, 0.6324664950370789, 0.6332833170890808, 0.6340951323509216, 0.6348713636398315, 0.6355810761451721, 0.6362918615341187, 0.6370266079902649, 0.6377569437026978, 0.6384730935096741, 0.6391382813453674, 0.6398221254348755, 0.640562117099762, 0.6413073539733887, 0.6420744061470032, 0.6428462862968445, 0.643690288066864, 0.6446380615234375, 0.6457313299179077, 0.646721363067627, 0.6475924849510193, 0.6483394503593445, 0.6489441990852356, 0.6494477391242981, 0.6498583555221558, 0.6503189206123352, 0.6508685350418091, 0.6515448093414307, 0.6522776484489441, 0.6531094908714294, 0.6540802121162415, 0.6551184058189392, 0.6561266183853149, 0.6570862531661987], 'test_auc': 0.6679486481582291, 'test_acc': 81.69999999999999, 'test_loss': 0.6359236240386963, 'best_val_auc': 0.6590814936523985, 'best_val_acc': 78.14999999999999, 'best_val_loss': 0.5455754995346069})\n",
            "('0.005', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181], 'train_loss_list': [1.9468449354171753, 1.7527873516082764, 1.436561942100525, 1.22237229347229, 0.9728676676750183, 0.8067125678062439, 0.7221835255622864, 0.7021340727806091, 0.6462013125419617, 0.543231189250946, 0.43572235107421875, 0.35255542397499084, 0.27920717000961304, 0.2417764812707901, 0.19470489025115967, 0.1569713056087494, 0.12050916999578476, 0.0945754274725914, 0.08285398781299591, 0.06223269924521446, 0.050786685198545456, 0.04443729296326637, 0.038155823945999146, 0.037772759795188904, 0.028666891157627106, 0.03377755731344223, 0.028453940525650978, 0.02884785830974579, 0.025720087811350822, 0.025302231311798096, 0.02622367814183235, 0.02371249906718731, 0.021599017083644867, 0.020818175747990608, 0.019839582964777946, 0.020331282168626785, 0.018896909430623055, 0.017904167994856834, 0.01928889751434326, 0.01807093434035778, 0.019432308152318, 0.018627408891916275, 0.01753944531083107, 0.016056522727012634, 0.016332007944583893, 0.015311356633901596, 0.014612785540521145, 0.015074077062308788, 0.013942606747150421, 0.012692595832049847, 0.011811159551143646, 0.013544628396630287, 0.011626453138887882, 0.012215238064527512, 0.012220990844070911, 0.01280136313289404, 0.011194594204425812, 0.01104893907904625, 0.009955122135579586, 0.011495531536638737, 0.010548824444413185, 0.011067796498537064, 0.009570448659360409, 0.011016692034900188, 0.010357916355133057, 0.009195350110530853, 0.009336646646261215, 0.009272998198866844, 0.00878212135285139, 0.00953239668160677, 0.008256363682448864, 0.008239490911364555, 0.007862902246415615, 0.009165815077722073, 0.008170481771230698, 0.008662533015012741, 0.007028328254818916, 0.00849029328674078, 0.007753174286335707, 0.007938501425087452, 0.008513093926012516, 0.007703832350671291, 0.008417299948632717, 0.006873474922031164, 0.007572461850941181, 0.007031619548797607, 0.006740753073245287, 0.007573391310870647, 0.006907194387167692, 0.00623868964612484, 0.00610608933493495, 0.008077174425125122, 0.006347195710986853, 0.005468941293656826, 0.005711504723876715, 0.007043274585157633, 0.006372849456965923, 0.006723177619278431, 0.006653732620179653, 0.00453819939866662, 0.005819929763674736, 0.005668551195412874, 0.005369674414396286, 0.006508123129606247, 0.006065991707146168, 0.006978663615882397, 0.00608688173815608, 0.00586571404710412, 0.007192435674369335, 0.004171299748122692, 0.005704088602215052, 0.004368186462670565, 0.005405357573181391, 0.006360028404742479, 0.0068909525871276855, 0.004946962930262089, 0.0055367229506373405, 0.005198714789003134, 0.005224056076258421, 0.005082262679934502, 0.005329856649041176, 0.005313340574502945, 0.004187196027487516, 0.00426566181704402, 0.006186008919030428, 0.004470896441489458, 0.005055754911154509, 0.004255365580320358, 0.00603042496368289, 0.005360661540180445, 0.00598449632525444, 0.0045538838021457195, 0.004659439902752638, 0.004598685074597597, 0.0040443106554448605, 0.005272155627608299, 0.004157790448516607, 0.004386747255921364, 0.003329110099002719, 0.003671416314318776, 0.005377711728215218, 0.004317679908126593, 0.004491617903113365, 0.004045337904244661, 0.004301953129470348, 0.0036994549445807934, 0.004128715023398399, 0.0034296505618840456, 0.003915511537343264, 0.003551974194124341, 0.003906961530447006, 0.0041910638101398945, 0.004035015590488911, 0.004195817746222019, 0.00445121806114912, 0.004270956385880709, 0.004819478373974562, 0.004194753710180521, 0.004153406247496605, 0.0044804229401052, 0.004248468205332756, 0.004538927227258682, 0.004027684219181538, 0.004207103978842497, 0.004449634347110987, 0.0045647453516721725, 0.0040955496951937675, 0.005243493709713221, 0.0038447377737611532, 0.0036670388653874397, 0.0042785340920090675, 0.00390063039958477, 0.004070476163178682, 0.00389020424336195, 0.003985992167145014, 0.0031640073284506798, 0.0035557651426643133, 0.003010566346347332, 0.003838529344648123, 0.004058460704982281, 0.003371028695255518, 0.004146955441683531], 'val_auc_list': [0.5471409017364421, 0.5482839631908931, 0.5504969518361215, 0.5542204790384561, 0.5615754384444251, 0.5724932102510761, 0.585354005951865, 0.5974166449972551, 0.6077926815174367, 0.6162347230649214, 0.6241115541301898, 0.6314809598104649, 0.6391365823004247, 0.6470540435121783, 0.6538474040045072, 0.6580223772788998, 0.6609314957672416, 0.662583788390974, 0.6634505648493253, 0.664093424055936, 0.6645791800294704, 0.6649620062985757, 0.6649620062985755, 0.664887969142758, 0.6647904567911935, 0.6648229609083817, 0.6648681055155875, 0.6650017335529167, 0.665198564040334, 0.6653665019791396, 0.6657330761896507, 0.6660870099101441, 0.6665799890208315, 0.6672336829331715, 0.6675424720464592, 0.6678223686111351, 0.6680805957643524, 0.6680805957643524, 0.6679975296870937, 0.6680932362543699, 0.6684038311519459, 0.6686765045794689, 0.6690304382999624, 0.6693952067261854, 0.6696227355465025, 0.6700037560313195, 0.6702728178902662, 0.670675507786542, 0.6709247060183179, 0.6712316893473174, 0.6715278379705873, 0.671757172575193, 0.6718474617896044, 0.6719955361012395, 0.6722862673716448, 0.6725390771719973, 0.672824391089538, 0.6732577793187138, 0.673586432059172, 0.6738157666637774, 0.6742708243044118, 0.6745380803790703, 0.6749172950795991, 0.6751267660570339, 0.6753199849758748, 0.6753795758573864, 0.6754337493860334, 0.6754644477189333, 0.6755186212475803, 0.6756919765392505, 0.6756287740891624, 0.6756793360492328, 0.6757010054606918, 0.6756378030106035, 0.6758057409494093, 0.6759375632024501, 0.6760784143769322, 0.6762301002571437, 0.67623371182572, 0.6762662159429084, 0.6764251249602727, 0.6765695877033313, 0.6767248851521193, 0.6769740833838953, 0.6772016122042126, 0.6771853601456184, 0.6771600791655832, 0.6772612030857242, 0.6772810667128948, 0.6773352402415417, 0.6773478807315593, 0.6773496865158476, 0.6773388518101182, 0.6771727196556009, 0.6770427031868481, 0.6771871659299066, 0.6772323105371125, 0.6773370460258299, 0.6774092773973592, 0.6773659385744416, 0.6773966369073416, 0.6774869261217532, 0.6775970789633354, 0.6777505706278352, 0.6778065499407703, 0.6778643350379937, 0.6778697523908583, 0.6778787813122995, 0.6777740458235821, 0.6777632111178526, 0.6777866863135997, 0.6779437895466759, 0.6780503308196816, 0.678178541504146, 0.6784295455202105, 0.678536086793216, 0.6784494091473809, 0.6783446736586635, 0.6783934298344457, 0.678418710814481, 0.6785252520874867, 0.6784476033630926, 0.6783844009130046, 0.6783771777758516, 0.678528863656063, 0.6785722024789806, 0.6786534627719512, 0.6787166652220393, 0.6787365288492099, 0.6787618098292449, 0.6787473635549391, 0.6786968015948687, 0.6786029008118806, 0.6785288636560631, 0.6785433099303689, 0.6785396983617926, 0.678543309930369, 0.6785956776747277, 0.6785758140475572, 0.6786227644390512, 0.6786480454190864, 0.6787256941434804, 0.6787780618878391, 0.67890988414088, 0.6790001733552917, 0.679119355118315, 0.6792024211955736, 0.679314379821444, 0.6792981277628499, 0.6793378550171911, 0.6792565947242206, 0.679251177371356, 0.6792692352142383, 0.6792547889399324, 0.6792909046256969, 0.6792764583513912, 0.6793342434486145, 0.6794769004073848, 0.6795491317789142, 0.6797242928548727, 0.6797784663835197, 0.6799662679494959, 0.6800619745167722, 0.680177544711219, 0.6803400652971598, 0.680488139608795, 0.6807012221548063, 0.6807734535263356, 0.6807698419577591, 0.6808529080350177, 0.6807969287220825, 0.6806939990176533, 0.6806633006847533, 0.6805928750975123, 0.6805603709803243, 0.6806073213718182, 0.6806298936754211, 0.6807012221548062, 0.6807978316142268, 0.6808384617607119, 0.6809016642108, 0.6809061786715206], 'val_acc_list': [50.5, 53.949999999999996, 59.099999999999994, 65.05, 71.25, 75.94999999999999, 79.0, 80.0, 80.10000000000001, 79.25, 78.64999999999999, 76.55, 73.85000000000001, 72.39999999999999, 72.1, 72.55, 73.4, 73.95, 75.0, 75.9, 76.7, 77.64999999999999, 78.25, 78.75, 78.9, 79.3, 79.4, 79.5, 79.75, 79.9, 79.95, 79.9, 79.80000000000001, 79.80000000000001, 79.80000000000001, 79.95, 79.9, 80.0, 80.0, 80.15, 80.25, 80.2, 80.2, 80.15, 80.05, 80.15, 80.10000000000001, 80.15, 80.25, 80.25, 80.25, 80.30000000000001, 80.30000000000001, 80.30000000000001, 80.30000000000001, 80.30000000000001, 80.4, 80.35, 80.45, 80.5, 80.45, 80.30000000000001, 80.30000000000001, 80.35, 80.35, 80.7, 80.75, 80.80000000000001, 80.95, 81.0, 81.0, 81.0, 81.0, 80.95, 80.9, 81.0, 81.0, 81.0, 81.10000000000001, 81.15, 81.05, 81.05, 81.05, 81.10000000000001, 81.2, 81.3, 81.35, 81.35, 81.39999999999999, 81.3, 81.3, 81.25, 81.25, 81.3, 81.3, 81.25, 81.3, 81.25, 81.25, 81.15, 81.15, 81.10000000000001, 81.25, 81.25, 81.25, 81.39999999999999, 81.39999999999999, 81.45, 81.5, 81.55, 81.6, 81.55, 81.55, 81.65, 81.75, 81.75, 81.8, 81.8, 81.75, 81.69999999999999, 81.6, 81.55, 81.55, 81.5, 81.5, 81.55, 81.5, 81.6, 81.65, 81.75, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.65, 81.65, 81.65, 81.65, 81.65, 81.65, 81.65, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.75, 81.75, 81.75, 81.8, 81.75, 81.75, 81.75, 81.69999999999999, 81.65, 81.65, 81.65, 81.65, 81.65, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.75, 81.8, 81.85, 81.85, 81.85, 81.89999999999999, 81.95, 82.0, 81.95, 81.95, 81.95, 81.95, 81.89999999999999, 81.89999999999999, 81.85, 81.85, 81.85, 81.85, 81.89999999999999, 81.89999999999999, 81.95], 'val_loss_list': [1.8441132307052612, 1.6462242603302002, 1.4076123237609863, 1.163447380065918, 0.9807894229888916, 0.8918933272361755, 0.8699657917022705, 0.8498943448066711, 0.8042231202125549, 0.7443598508834839, 0.6936429738998413, 0.6696228384971619, 0.6675073504447937, 0.6698933839797974, 0.6665915846824646, 0.6571326851844788, 0.643339991569519, 0.6280052065849304, 0.6137323975563049, 0.6022604703903198, 0.5938258767127991, 0.5879781246185303, 0.5843308568000793, 0.582342267036438, 0.5812450647354126, 0.5807051658630371, 0.580464780330658, 0.5803976655006409, 0.5804930329322815, 0.5808020234107971, 0.58113032579422, 0.5814205408096313, 0.5816724300384521, 0.581808865070343, 0.5819401144981384, 0.5820601582527161, 0.5822607278823853, 0.5826343894004822, 0.5831254720687866, 0.5836166143417358, 0.5839858651161194, 0.5844376683235168, 0.5850035548210144, 0.585399866104126, 0.5857217907905579, 0.5859697461128235, 0.5861703753471375, 0.5864326357841492, 0.5868701338768005, 0.5874940752983093, 0.5881661772727966, 0.5888157486915588, 0.5893532633781433, 0.5897736549377441, 0.5900570750236511, 0.5903280973434448, 0.5905866026878357, 0.5908399820327759, 0.5912044048309326, 0.5916405916213989, 0.5921269059181213, 0.5926756858825684, 0.5932497978210449, 0.5939257740974426, 0.5948951244354248, 0.5960496068000793, 0.5972620844841003, 0.5985309481620789, 0.5998555421829224, 0.6009785532951355, 0.6018438935279846, 0.6026003956794739, 0.603299081325531, 0.6039866209030151, 0.6046223640441895, 0.6051639318466187, 0.6057974696159363, 0.6064298152923584, 0.60707688331604, 0.6078495383262634, 0.6086990833282471, 0.6094900369644165, 0.6103807091712952, 0.6114705204963684, 0.6127708554267883, 0.6140977144241333, 0.6153703927993774, 0.6165942549705505, 0.6178258061408997, 0.6190768480300903, 0.6202502846717834, 0.6214339733123779, 0.622536301612854, 0.6235567331314087, 0.6245389580726624, 0.62549889087677, 0.6263142824172974, 0.6270962953567505, 0.6278743147850037, 0.6286561489105225, 0.6296954154968262, 0.630955696105957, 0.6322863101959229, 0.633543074131012, 0.6347019076347351, 0.635810136795044, 0.6369436979293823, 0.6380343437194824, 0.6388565897941589, 0.6395958065986633, 0.6405086517333984, 0.6414929032325745, 0.6424739360809326, 0.643613338470459, 0.6449914574623108, 0.6463873982429504, 0.6476638317108154, 0.6489627957344055, 0.6501703262329102, 0.6509837508201599, 0.6516475677490234, 0.6524753570556641, 0.6534793972969055, 0.6545866131782532, 0.655716061592102, 0.6569247841835022, 0.6581136584281921, 0.6593283414840698, 0.6605095863342285, 0.661558985710144, 0.6624104380607605, 0.6631436944007874, 0.6638100743293762, 0.6646734476089478, 0.6657271385192871, 0.6668046712875366, 0.6679122447967529, 0.668874979019165, 0.6697668433189392, 0.6707122921943665, 0.6717283129692078, 0.6726903319358826, 0.6736380457878113, 0.6747310161590576, 0.675766110420227, 0.6765706539154053, 0.6772999167442322, 0.6781345009803772, 0.6791681051254272, 0.6803587675094604, 0.6816112399101257, 0.6829447150230408, 0.6843150854110718, 0.6854516863822937, 0.6863747835159302, 0.6871002912521362, 0.6874808669090271, 0.6878663897514343, 0.688604474067688, 0.6894496083259583, 0.6899866461753845, 0.6904498338699341, 0.6908578872680664, 0.6914897561073303, 0.6924524903297424, 0.6937103271484375, 0.6948121786117554, 0.6956249475479126, 0.6964162588119507, 0.6971452236175537, 0.6977760791778564, 0.6982974410057068, 0.6990725994110107, 0.7002170085906982, 0.7015653252601624, 0.7030483484268188, 0.7043936848640442, 0.7054399251937866, 0.7061060070991516, 0.7064926028251648, 0.7067373394966125, 0.7072086334228516], 'test_auc': 0.6964739603050133, 'test_acc': 82.3, 'test_loss': 0.670917272567749, 'best_val_auc': 0.6650017335529167, 'best_val_acc': 79.5, 'best_val_loss': 0.5803976655006409})\n",
            "('0.01', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180], 'train_loss_list': [1.8834102153778076, 1.3906965255737305, 0.9189426302909851, 0.7807917594909668, 0.6469530463218689, 0.5164323449134827, 0.3974836766719818, 0.308411568403244, 0.1658332496881485, 0.10453397035598755, 0.08346709609031677, 0.06359221041202545, 0.05412745848298073, 0.05117661878466606, 0.0431944876909256, 0.03260643035173416, 0.019856523722410202, 0.024794572964310646, 0.023015575483441353, 0.024696318432688713, 0.0211245846003294, 0.016593487933278084, 0.014957347884774208, 0.017894383519887924, 0.013627845793962479, 0.01649422012269497, 0.013572572730481625, 0.013438313268125057, 0.012027590535581112, 0.012071365490555763, 0.01298464648425579, 0.010175714269280434, 0.010050052776932716, 0.008730096742510796, 0.008105223067104816, 0.009734482504427433, 0.010167023167014122, 0.009744929149746895, 0.008584722876548767, 0.007946010679006577, 0.007911880500614643, 0.008445033803582191, 0.007511049509048462, 0.006569436751306057, 0.006929208058863878, 0.007984651252627373, 0.007288768421858549, 0.007944098673760891, 0.006351714953780174, 0.005883031524717808, 0.006339561194181442, 0.005069222301244736, 0.006517655216157436, 0.007430644705891609, 0.006119171157479286, 0.00463762367144227, 0.006487962789833546, 0.00546139944344759, 0.005424810107797384, 0.0053087268024683, 0.005571404471993446, 0.004880341701209545, 0.004928755573928356, 0.0036034889053553343, 0.005279722157865763, 0.005324411205947399, 0.004079269710928202, 0.004210783168673515, 0.004104517865926027, 0.004031107295304537, 0.004326818976551294, 0.004438070114701986, 0.003843662329018116, 0.0050072395242750645, 0.004137891344726086, 0.003072781953960657, 0.004961839411407709, 0.0060267699882388115, 0.003955799620598555, 0.003539817640557885, 0.0046772523783147335, 0.0045889499597251415, 0.0030723712407052517, 0.004487437661737204, 0.004476683214306831, 0.004205665085464716, 0.003971351310610771, 0.004467695485800505, 0.003683836432173848, 0.0035320764873176813, 0.0037136843893676996, 0.003101404057815671, 0.004536835942417383, 0.003953275270760059, 0.004189253784716129, 0.002747333375737071, 0.003575932467356324, 0.004275080282241106, 0.004090228118002415, 0.004582869820296764, 0.004081789869815111, 0.003542868420481682, 0.005414096172899008, 0.0038764935452491045, 0.0031156037002801895, 0.004152616485953331, 0.004675703123211861, 0.003192261094227433, 0.0030409004539251328, 0.005775197874754667, 0.004688766319304705, 0.0038246316835284233, 0.0032671932131052017, 0.003092646598815918, 0.004435175564140081, 0.003163981717079878, 0.004447683226317167, 0.004048831295222044, 0.0029720086604356766, 0.004316043108701706, 0.004168184474110603, 0.003238862846046686, 0.0031565334647893906, 0.004065083339810371, 0.005059337243437767, 0.004319785162806511, 0.003557988442480564, 0.0018747203284874558, 0.004950499627739191, 0.0035712027456611395, 0.004343022126704454, 0.004576005507260561, 0.0031848223879933357, 0.002644362859427929, 0.0031602077651768923, 0.004092410206794739, 0.003660212503746152, 0.003937100991606712, 0.003160820808261633, 0.002808902645483613, 0.004528963938355446, 0.0023029495496302843, 0.0037315383087843657, 0.003747636452317238, 0.002325724344700575, 0.002775922417640686, 0.0036817234940826893, 0.004842212423682213, 0.002874185098335147, 0.0037953800056129694, 0.005272465292364359, 0.0029235845431685448, 0.003932083956897259, 0.00322120008058846, 0.003167584305629134, 0.0035336457658559084, 0.003653007559478283, 0.004104099702090025, 0.004571815021336079, 0.003112250939011574, 0.004653810989111662, 0.0034321071580052376, 0.0029635385144501925, 0.004096677992492914, 0.0020186114124953747, 0.003185064997524023, 0.0038102990947663784, 0.0033024102449417114, 0.002579761203378439, 0.004155256319791079, 0.0030257466714829206, 0.004628336988389492, 0.002846807474270463, 0.004241275601089001, 0.003950904589146376, 0.002829070668667555, 0.0030754392500966787, 0.003062502248212695, 0.003363854018971324, 0.003482094034552574, 0.0037896966096013784], 'val_auc_list': [0.39546313148421397, 0.412247210149296, 0.45495761876601615, 0.5317490847222781, 0.6025426826584296, 0.6379279987904745, 0.6507705939879698, 0.6529102291017169, 0.6416513083734754, 0.6185459284734144, 0.5951802709566542, 0.5804229128510159, 0.5742604575144636, 0.5749934452144324, 0.580327222550759, 0.5879058943311153, 0.5955496355156464, 0.6022824050417306, 0.6073884394634453, 0.6105366503419015, 0.6117767966332325, 0.6123643350768107, 0.6126131298574788, 0.6127470962778386, 0.6130781847167279, 0.613839879506774, 0.6152465269205522, 0.6167412094105669, 0.6185688941454761, 0.6206970464231922, 0.6229094061651346, 0.6249476095606092, 0.6267982599675801, 0.6284479607440112, 0.6299617812940773, 0.6314373257240407, 0.6328784216459113, 0.6340841194291499, 0.6352381444502496, 0.6365395325337452, 0.6384246314488086, 0.6404819729043346, 0.6426771083922308, 0.6451286938848158, 0.6470367584719408, 0.6485754585000737, 0.6497160868791374, 0.6507227488378413, 0.6515590820620876, 0.6524202947644009, 0.6537159414298811, 0.6549465186911864, 0.6562574758047075, 0.6573732247057045, 0.658387541888429, 0.6593482725030094, 0.6603109169235951, 0.6613223633973119, 0.662502224799481, 0.6637691643748839, 0.6648562061858037, 0.665690625604045, 0.6665843730084456, 0.6672274118261728, 0.6679431752720953, 0.6686799905840743, 0.6693976678360021, 0.6701880697161251, 0.6707813495777187, 0.6713459223492351, 0.6717803563124021, 0.6720444615411114, 0.6723583257259544, 0.6727717078230647, 0.673456850372905, 0.674071182100555, 0.6746663757681539, 0.6753821392140764, 0.6761419201981171, 0.6769591153623122, 0.6778853974688003, 0.6785399191225582, 0.6789169389055709, 0.6791465956261877, 0.6792652515985064, 0.6792977863005938, 0.679383907570825, 0.6793341486146915, 0.6795446672752569, 0.6800805329566962, 0.6808403139407371, 0.6815675602226904, 0.6821034259041298, 0.6823924106109059, 0.6826201535255177, 0.6827560337518828, 0.6829053106202837, 0.683058415100695, 0.6833550550314917, 0.6838124546667202, 0.6840956979554811, 0.6844019069163035, 0.6850143248379484, 0.68537603417292, 0.6858200371661126, 0.6861836603070893, 0.6863176267274491, 0.6862870058313668, 0.6864075756096907, 0.6866353185243025, 0.686847750990873, 0.6872266845798909, 0.6876343252589858, 0.6879577584738547, 0.6883213816148314, 0.688686918561813, 0.6889471961785122, 0.6894658376059052, 0.6898103226868305, 0.6903308779202287, 0.6905299137447634, 0.6905490518048147, 0.690673449195149, 0.6909203301698119, 0.691180607786511, 0.6913413674909429, 0.691479161523313, 0.6914810753293181, 0.6914427992092154, 0.6913930402530817, 0.691465764881277, 0.6917413529460171, 0.6920552171308603, 0.6923748227337188, 0.6927154802026338, 0.6928743261010604, 0.692964274983302, 0.6929374816992301, 0.6928953779671171, 0.6930771895376053, 0.693442726484587, 0.6938541947756924, 0.6941450932884736, 0.6942637492607924, 0.6943039391869004, 0.694409198517183, 0.694694355611949, 0.6952474455474347, 0.6957316384667352, 0.6959976575014497, 0.6960933478017067, 0.6958636910810899, 0.6957335522727403, 0.695689534734622, 0.6956818795106015, 0.695769914586838, 0.6959019672011927, 0.6962636765361643, 0.6967153347533774, 0.6971516825225494, 0.6971612515525751, 0.6971957000606677, 0.6971421134925238, 0.6969124567719068, 0.6967095933353619, 0.6966196444531203, 0.6968225078896653, 0.6971000097604106, 0.6977047724580351, 0.6978521355204308, 0.6977086000700453, 0.6973966496912074, 0.6972722523008733, 0.6973124422269813, 0.6975918579037318, 0.69777366947422, 0.6977908937282663, 0.6980894474650682, 0.6984645534420759, 0.6987497105368418, 0.6990214709895718], 'val_acc_list': [47.55, 59.599999999999994, 74.5, 81.95, 82.95, 81.0, 77.45, 75.8, 75.4, 74.65, 74.25, 74.5, 75.2, 75.64999999999999, 76.2, 77.05, 77.60000000000001, 78.0, 78.10000000000001, 78.55, 78.7, 79.2, 79.45, 79.5, 79.7, 79.75, 79.65, 79.80000000000001, 80.2, 80.25, 80.2, 80.30000000000001, 80.35, 80.7, 80.85, 80.95, 81.2, 81.3, 81.35, 81.25, 81.39999999999999, 81.65, 81.75, 81.89999999999999, 82.0, 82.0, 81.95, 81.85, 81.89999999999999, 81.89999999999999, 82.1, 82.1, 82.25, 82.19999999999999, 82.35, 82.35, 82.25, 82.3, 82.3, 82.19999999999999, 82.25, 82.19999999999999, 82.25, 82.35, 82.25, 82.3, 82.35, 82.35, 82.55, 82.45, 82.45, 82.45, 82.6, 82.75, 82.8, 82.75, 82.8, 82.85, 82.89999999999999, 82.95, 82.95, 82.95, 82.95, 82.89999999999999, 82.89999999999999, 82.89999999999999, 82.85, 82.85, 82.85, 82.95, 82.85, 82.89999999999999, 82.89999999999999, 82.89999999999999, 82.89999999999999, 82.89999999999999, 82.85, 82.85, 82.85, 82.85, 82.89999999999999, 82.95, 83.05, 83.05, 83.05, 83.05, 83.05, 83.05, 83.0, 82.95, 82.95, 83.05, 83.05, 83.05, 83.05, 83.05, 83.2, 83.2, 83.15, 83.1, 83.0, 82.95, 82.95, 82.95, 82.95, 83.0, 83.05, 83.05, 83.05, 83.05, 83.0, 83.0, 83.05, 83.05, 83.05, 83.05, 83.05, 83.1, 83.15, 83.15, 83.1, 83.2, 83.25, 83.3, 83.2, 83.2, 83.3, 83.3, 83.25, 83.25, 83.25, 83.25, 83.2, 83.2, 83.2, 83.2, 83.2, 83.35000000000001, 83.35000000000001, 83.39999999999999, 83.35000000000001, 83.3, 83.3, 83.25, 83.25, 83.25, 83.25, 83.35000000000001, 83.35000000000001, 83.35000000000001, 83.39999999999999, 83.45, 83.5, 83.6, 83.55, 83.6, 83.55, 83.39999999999999, 83.45, 83.45, 83.45], 'val_loss_list': [1.6952346563339233, 1.2362275123596191, 0.9137550592422485, 0.832610547542572, 0.7692168951034546, 0.7053461074829102, 0.701962411403656, 0.6702907085418701, 0.6282939314842224, 0.6208271384239197, 0.637254536151886, 0.6532778739929199, 0.6596550345420837, 0.656814694404602, 0.6484097242355347, 0.6393311619758606, 0.6324152946472168, 0.627826988697052, 0.6245136857032776, 0.6217516660690308, 0.6198327541351318, 0.6187294721603394, 0.6182463765144348, 0.6181555390357971, 0.6178297996520996, 0.6169294118881226, 0.6153650283813477, 0.6135780811309814, 0.6118037104606628, 0.6103975772857666, 0.6092855930328369, 0.6084179282188416, 0.6077473759651184, 0.6072681546211243, 0.606916069984436, 0.6067164540290833, 0.6067550182342529, 0.606940507888794, 0.6073708534240723, 0.6078202128410339, 0.6080336570739746, 0.6080662608146667, 0.6080964207649231, 0.6083266735076904, 0.608869731426239, 0.6095070838928223, 0.6100717782974243, 0.6106219291687012, 0.6114705801010132, 0.6125305891036987, 0.6137900352478027, 0.6149601340293884, 0.6161605715751648, 0.6177822947502136, 0.6194388270378113, 0.6207362413406372, 0.6219067573547363, 0.6228688955307007, 0.6237281560897827, 0.6246548891067505, 0.6256023049354553, 0.6264292001724243, 0.6274052858352661, 0.628732442855835, 0.6301277875900269, 0.6314727067947388, 0.6327869892120361, 0.6339041590690613, 0.6349056363105774, 0.6363102197647095, 0.6377773880958557, 0.6391060948371887, 0.6405927538871765, 0.6422117948532104, 0.6439401507377625, 0.6456426978111267, 0.6471742987632751, 0.6480593681335449, 0.6483945846557617, 0.6485938429832458, 0.6489003300666809, 0.6490247845649719, 0.649270236492157, 0.6497189402580261, 0.6506721377372742, 0.6519474983215332, 0.6531726121902466, 0.6546220779418945, 0.6563791632652283, 0.6583572626113892, 0.6606318354606628, 0.662656843662262, 0.6639646291732788, 0.6643282175064087, 0.6639031171798706, 0.6630929708480835, 0.6626802086830139, 0.6626163125038147, 0.6633270978927612, 0.664638876914978, 0.66593998670578, 0.6674537062644958, 0.6696335673332214, 0.6720118522644043, 0.6737787127494812, 0.6744868159294128, 0.6738678216934204, 0.6727975010871887, 0.6724349856376648, 0.6726182699203491, 0.6726621985435486, 0.6725626587867737, 0.6727806329727173, 0.6733549237251282, 0.6744446158409119, 0.6758705973625183, 0.6774849891662598, 0.6787019371986389, 0.679637610912323, 0.6807026863098145, 0.6816455721855164, 0.6822838187217712, 0.6822591423988342, 0.6821156144142151, 0.6822172999382019, 0.6828742027282715, 0.68397057056427, 0.6846153736114502, 0.6849341988563538, 0.6852942705154419, 0.6857733130455017, 0.6864181756973267, 0.6875771284103394, 0.6887580156326294, 0.6899632811546326, 0.6905800104141235, 0.690691351890564, 0.69089674949646, 0.6914193630218506, 0.6921204924583435, 0.6933560967445374, 0.6949831247329712, 0.6957693099975586, 0.6961063742637634, 0.6965616345405579, 0.6969335675239563, 0.6971832513809204, 0.6969926953315735, 0.6966418623924255, 0.6964192390441895, 0.6957461833953857, 0.6948614716529846, 0.6944653987884521, 0.6941972970962524, 0.6943820118904114, 0.6957719326019287, 0.698682963848114, 0.7015994787216187, 0.7038483619689941, 0.7052226662635803, 0.7056024074554443, 0.7052953839302063, 0.7047073245048523, 0.702588677406311, 0.6996110081672668, 0.6974925398826599, 0.6964235305786133, 0.6967018842697144, 0.6984463930130005, 0.7005247473716736, 0.7027757167816162, 0.7051672339439392, 0.7081419825553894, 0.7105592489242554, 0.711958110332489, 0.7114740014076233, 0.7096453905105591, 0.7078428268432617, 0.706206738948822, 0.7054960131645203, 0.7051466703414917], 'test_auc': 0.6913610121438035, 'test_acc': 81.55, 'test_loss': 0.8233873844146729, 'best_val_auc': 0.6314373257240407, 'best_val_acc': 80.95, 'best_val_loss': 0.6067164540290833})\n",
            "('0.02', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190], 'train_loss_list': [1.2924176454544067, 0.786502480506897, 0.6139957904815674, 0.4070813059806824, 0.19096092879772186, 0.1196393072605133, 0.0752146989107132, 0.048988644033670425, 0.03235406056046486, 0.030299775302410126, 0.023068608716130257, 0.02353086695075035, 0.020320210605859756, 0.01792970485985279, 0.013389251194894314, 0.014091179706156254, 0.009848007932305336, 0.008689913898706436, 0.008045250549912453, 0.010601600632071495, 0.007391143590211868, 0.006843447219580412, 0.00425566965714097, 0.004545577336102724, 0.004819023422896862, 0.0060425507836043835, 0.0034949430264532566, 0.005699178669601679, 0.003459697589278221, 0.005880935583263636, 0.003945068456232548, 0.00599360466003418, 0.00486058508977294, 0.004186972044408321, 0.0034085118677467108, 0.005581285804510117, 0.0051532406359910965, 0.004660062957555056, 0.002886539325118065, 0.0026989239268004894, 0.004472647327929735, 0.00385730410926044, 0.00412064790725708, 0.0025459034368395805, 0.0023522337432950735, 0.0027400723192840815, 0.004288306459784508, 0.002468758262693882, 0.0037339753471314907, 0.003074810141697526, 0.0037221794482320547, 0.002788133919239044, 0.00343358819372952, 0.0022133036982268095, 0.002922943327575922, 0.0038321909960359335, 0.004855629056692123, 0.002910735085606575, 0.002962272148579359, 0.003193566109985113, 0.003828879678621888, 0.004432033747434616, 0.003215702250599861, 0.003961051814258099, 0.003311875509098172, 0.004024199675768614, 0.002695640316233039, 0.003567738924175501, 0.002378969918936491, 0.0023024538531899452, 0.003180075902491808, 0.003843202954158187, 0.0038824749644845724, 0.004581979010254145, 0.004189540632069111, 0.0019921008497476578, 0.003473621094599366, 0.004344653338193893, 0.0030570286326110363, 0.002282515401020646, 0.0036995240952819586, 0.002091186586767435, 0.005139148328453302, 0.0029315194115042686, 0.0052025169134140015, 0.00168599933385849, 0.0045928191393613815, 0.0023602829314768314, 0.0034640731755644083, 0.003067784244194627, 0.0035234028473496437, 0.004211556166410446, 0.0033167193178087473, 0.00308678625151515, 0.003311465959995985, 0.002909057540819049, 0.0025390831287950277, 0.005744203459471464, 0.0032453075982630253, 0.004149652551859617, 0.002112503396347165, 0.0021696542389690876, 0.004567321389913559, 0.0029518455266952515, 0.0041821785271167755, 0.0026405698154121637, 0.004329998046159744, 0.003771377494558692, 0.004602470900863409, 0.0037838509306311607, 0.0041448380798101425, 0.0024799557868391275, 0.002385364845395088, 0.004725337494164705, 0.002614329569041729, 0.0022455856669694185, 0.003062424249947071, 0.0033095485996454954, 0.003061979077756405, 0.0027909218333661556, 0.005113158840686083, 0.0031535224989056587, 0.002636956050992012, 0.0024167371448129416, 0.00368577241897583, 0.0036012614145874977, 0.004750940017402172, 0.0031418576836586, 0.002995268674567342, 0.0020360960625112057, 0.0032905684784054756, 0.002305865753442049, 0.005380370654165745, 0.0033305489923805, 0.0036395660135895014, 0.0038335174322128296, 0.003555416828021407, 0.004831613041460514, 0.003580508753657341, 0.003390425816178322, 0.003919984679669142, 0.0030960319563746452, 0.0038348555099219084, 0.004212693776935339, 0.0053823115304112434, 0.006279394496232271, 0.0047393073327839375, 0.0028132253792136908, 0.002946185413748026, 0.0037681451067328453, 0.0027281276416033506, 0.004457593429833651, 0.002059865975752473, 0.0027581024914979935, 0.0017788432305678725, 0.003421328729018569, 0.0021464317105710506, 0.002986603882163763, 0.0022230229806154966, 0.0044243172742426395, 0.002604244276881218, 0.0018403098220005631, 0.005474441219121218, 0.0022173088509589434, 0.002400835044682026, 0.003546604420989752, 0.004349031951278448, 0.004878218751400709, 0.003503840183839202, 0.0021282166708260775, 0.004329731687903404, 0.0028493115678429604, 0.00464879022911191, 0.001958089880645275, 0.0041601527482271194, 0.004299251362681389, 0.005448546260595322, 0.002085083397105336, 0.00400337902829051, 0.0012565234210342169, 0.004009131342172623, 0.0026838011108338833, 0.0026087432634085417, 0.002464129589498043, 0.0029473903123289347, 0.0020512649789452553, 0.002005096757784486, 0.005610477179288864, 0.004475520458072424, 0.0035548771265894175, 0.004238597583025694], 'val_auc_list': [0.504878533917976, 0.5569049813614076, 0.6111300545749354, 0.6367783206264129, 0.6332832515508482, 0.61264405172276, 0.6108316950197041, 0.6157670593291528, 0.6202229172105512, 0.621286711100929, 0.6209919034451649, 0.6210664933339727, 0.6214447706272122, 0.620958160400228, 0.6210487338366375, 0.6226097936524004, 0.6243999509837873, 0.6256093727523135, 0.6266145603014852, 0.6271189300258045, 0.6279944732444293, 0.6291630481690846, 0.6302392737075969, 0.6308732877624631, 0.6311219207251558, 0.6312036144128976, 0.6316493777960108, 0.6325995109034434, 0.634025598539459, 0.6360519571854037, 0.6383784513363133, 0.6407173771353576, 0.6426673699427612, 0.6439140866556913, 0.6453188628949046, 0.6463258263938098, 0.6473381177419154, 0.6484445344258976, 0.6495598308585474, 0.6503909753338342, 0.6513890590840716, 0.6522841377497651, 0.6531632328678569, 0.6541311254726248, 0.6549516142495103, 0.6559106271056104, 0.6568057057713038, 0.6574663590721729, 0.6582548807538551, 0.6593222265436999, 0.6603007748468687, 0.6610892965285511, 0.6618476270647635, 0.662694755087652, 0.6635525388089416, 0.6641581376680714, 0.6648028074213387, 0.6654048543810016, 0.6659900298181961, 0.6667900951731462, 0.6675182345638889, 0.6681256093727523, 0.6685553892082639, 0.6689656335967067, 0.6694984185167623, 0.669805657820661, 0.6702514212037742, 0.6705977314018104, 0.6708286048671679, 0.6710719099806599, 0.6715709518557786, 0.6723363861909252, 0.6731817382640802, 0.6736168459487923, 0.6739063257553558, 0.6740288662869687, 0.6741958055619194, 0.6745794107043595, 0.6746753119899694, 0.6748582348125218, 0.6748386993654532, 0.674732142381442, 0.6748955297569258, 0.675389243782844, 0.6759415641499683, 0.6767371896305847, 0.6771278985719589, 0.6775647822064044, 0.6781064468751276, 0.6784563089726308, 0.6785895052026447, 0.6787013900358564, 0.6786374558454498, 0.6787866356230653, 0.6787688761257302, 0.6786161444486475, 0.6785308988614386, 0.6785841773534442, 0.6786605431919855, 0.6791027546756316, 0.6798362219155749, 0.6803388156901609, 0.680909783529487, 0.6811149057237084, 0.6810882664777056, 0.6808200980679442, 0.6804045258303009, 0.6802446903542843, 0.680123925772405, 0.6799587624471877, 0.6798131345690392, 0.6796532990930224, 0.6799694181455889, 0.6804507005233724, 0.681024332287299, 0.6815686608806224, 0.6818110780192478, 0.681596188101492, 0.6808502892134141, 0.6798966042065147, 0.6797119054342285, 0.6799214675027838, 0.6803920941821662, 0.6808307537663454, 0.6815588931570882, 0.6821964591114213, 0.6819638096963303, 0.6814070494548722, 0.6806229676475237, 0.6802100593344805, 0.6800511118333308, 0.6801292536216055, 0.6801843080633445, 0.680500427115911, 0.6809888132926286, 0.6818865558829222, 0.6828535605128232, 0.6832309498311959, 0.682729244031477, 0.6815642210062887, 0.680577680929319, 0.6797740636749018, 0.6793442838393902, 0.6799827377685902, 0.6812303424563871, 0.6821911312622209, 0.6823811578837072, 0.6820206400878029, 0.681843045114451, 0.6818679084107204, 0.682065038831141, 0.6821875793627539, 0.6819895609674664, 0.6818670204358536, 0.6817782229491777, 0.681707184959837, 0.6816041798752928, 0.681522486187551, 0.6816912014122352, 0.6821618280916177, 0.6822470736788266, 0.6819673615957975, 0.6817462558539742, 0.6815757646795565, 0.6815260380870181, 0.6817098488844373, 0.6819842331182657, 0.6821121014990792, 0.6818208457427821, 0.6811069139499075, 0.6807872429978742, 0.681057187357369, 0.6813626507115341, 0.6818759001845213, 0.6822808167237635, 0.6825685205805936, 0.6821955711365545, 0.6817995343459798, 0.6816219393726279, 0.6817409280047738, 0.681946938173862, 0.6819380584251943, 0.681851036888252, 0.6819043153802575, 0.6816077317747599, 0.6811646323162469, 0.6810927063520393, 0.6813395633649986, 0.6818101900443809, 0.6817515837031748, 0.6818013102957133], 'val_acc_list': [66.10000000000001, 79.85, 80.0, 73.8, 73.15, 74.95, 77.95, 80.5, 81.15, 81.39999999999999, 81.45, 81.8, 82.0, 82.35, 82.3, 82.35, 82.05, 82.05, 82.05, 82.1, 82.1, 82.1, 82.19999999999999, 82.1, 82.1, 82.35, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.5, 82.39999999999999, 82.5, 82.5, 82.55, 82.55, 82.39999999999999, 82.39999999999999, 82.45, 82.6, 82.55, 82.6, 82.5, 82.55, 82.6, 82.65, 82.6, 82.65, 82.6, 82.55, 82.6, 82.6, 82.45, 82.45, 82.55, 82.6, 82.69999999999999, 82.69999999999999, 82.69999999999999, 82.75, 82.8, 82.69999999999999, 82.65, 82.65, 82.65, 82.69999999999999, 82.8, 82.75, 82.75, 82.69999999999999, 82.69999999999999, 82.75, 82.75, 82.75, 82.69999999999999, 82.65, 82.75, 82.69999999999999, 82.65, 82.69999999999999, 82.65, 82.5, 82.6, 82.6, 82.55, 82.55, 82.65, 82.6, 82.5, 82.45, 82.5, 82.55, 82.5, 82.55, 82.55, 82.6, 82.55, 82.65, 82.65, 82.65, 82.6, 82.6, 82.6, 82.6, 82.6, 82.65, 82.69999999999999, 82.65, 82.75, 82.69999999999999, 82.65, 82.69999999999999, 82.75, 82.8, 82.8, 82.85, 82.85, 82.75, 82.65, 82.55, 82.6, 82.6, 82.6, 82.6, 82.69999999999999, 82.8, 82.65, 82.6, 82.6, 82.65, 82.69999999999999, 82.69999999999999, 82.6, 82.6, 82.55, 82.55, 82.6, 82.55, 82.6, 82.65, 82.69999999999999, 82.55, 82.55, 82.5, 82.55, 82.69999999999999, 82.75, 82.69999999999999, 82.55, 82.5, 82.6, 82.65, 82.69999999999999, 82.75, 82.75, 82.69999999999999, 82.55, 82.5, 82.5, 82.55, 82.65, 82.65, 82.69999999999999, 82.69999999999999, 82.6, 82.55, 82.55, 82.55, 82.65, 82.69999999999999, 82.65, 82.5, 82.5, 82.45, 82.55, 82.6, 82.55, 82.45, 82.35, 82.45, 82.55, 82.5, 82.55, 82.5, 82.45, 82.45, 82.35, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.39999999999999], 'val_loss_list': [1.0460201501846313, 0.8685118556022644, 0.7678074836730957, 0.691903293132782, 0.6449340581893921, 0.6460537910461426, 0.6465651988983154, 0.6597880721092224, 0.6784144043922424, 0.7002031207084656, 0.720713198184967, 0.7370282411575317, 0.7506201863288879, 0.7626010775566101, 0.7742162346839905, 0.7855185866355896, 0.794044554233551, 0.7999464273452759, 0.8030053377151489, 0.8049005270004272, 0.8080950379371643, 0.8139672875404358, 0.820847749710083, 0.8270902037620544, 0.832226037979126, 0.8359702825546265, 0.8389906883239746, 0.8411121368408203, 0.8428610563278198, 0.8447078466415405, 0.8461336493492126, 0.8463625907897949, 0.8447529077529907, 0.8419139385223389, 0.8390879034996033, 0.8383409380912781, 0.8395569324493408, 0.8420175909996033, 0.8451790809631348, 0.8466776609420776, 0.845862627029419, 0.8430368304252625, 0.8394849300384521, 0.8361188173294067, 0.8332842588424683, 0.8314546942710876, 0.8309481143951416, 0.8308074474334717, 0.8315169215202332, 0.8328138589859009, 0.8335505723953247, 0.8324616551399231, 0.8300788402557373, 0.8256840109825134, 0.821487545967102, 0.8187346458435059, 0.8174753785133362, 0.8174124360084534, 0.8192340135574341, 0.8217505216598511, 0.8232617974281311, 0.8224401473999023, 0.8195734024047852, 0.8179411292076111, 0.8170559406280518, 0.8171210289001465, 0.8185645937919617, 0.8183992505073547, 0.8174391388893127, 0.8163048624992371, 0.8136765360832214, 0.8106762766838074, 0.8096457719802856, 0.8105102181434631, 0.8120601177215576, 0.8136230707168579, 0.8143048286437988, 0.8138257265090942, 0.8116654753684998, 0.8076645731925964, 0.8036949634552002, 0.801522970199585, 0.8020092844963074, 0.805444598197937, 0.8099737167358398, 0.8148902058601379, 0.8171869516372681, 0.8182096481323242, 0.8174275755882263, 0.8142442107200623, 0.8096485733985901, 0.8052147030830383, 0.8031794428825378, 0.8018019795417786, 0.800132691860199, 0.800683856010437, 0.8037574291229248, 0.8079062700271606, 0.8106672763824463, 0.8123210072517395, 0.8145835995674133, 0.8151347637176514, 0.8136277794837952, 0.8093048334121704, 0.8049473762512207, 0.8026964068412781, 0.8029555678367615, 0.8055881261825562, 0.8086344599723816, 0.8122497200965881, 0.8140852451324463, 0.8140538930892944, 0.814999520778656, 0.8149496912956238, 0.815157413482666, 0.8162136077880859, 0.8153679966926575, 0.8124518990516663, 0.8087171912193298, 0.8064802289009094, 0.8073679208755493, 0.809286892414093, 0.8105847835540771, 0.812940776348114, 0.8165093064308167, 0.8184886574745178, 0.8170978426933289, 0.8143467903137207, 0.8146375417709351, 0.8178977370262146, 0.8213508129119873, 0.8233891725540161, 0.8213716745376587, 0.8167640566825867, 0.8137944340705872, 0.8153702020645142, 0.8206287622451782, 0.8252747654914856, 0.8251256346702576, 0.8226664066314697, 0.8175289630889893, 0.8133693337440491, 0.8131439089775085, 0.819341778755188, 0.828110933303833, 0.831131100654602, 0.8252869248390198, 0.8153470754623413, 0.8096597790718079, 0.8104503750801086, 0.8158338665962219, 0.8220576047897339, 0.8272736668586731, 0.8270618319511414, 0.822987973690033, 0.8186235427856445, 0.8166019320487976, 0.817283570766449, 0.8197250962257385, 0.8230947256088257, 0.8240531086921692, 0.8227997422218323, 0.8209877610206604, 0.818623423576355, 0.8181983828544617, 0.8219716548919678, 0.8284779787063599, 0.8331882357597351, 0.8321517705917358, 0.8262667059898376, 0.8219351172447205, 0.8212544918060303, 0.8246640563011169, 0.8304795622825623, 0.834962785243988, 0.8348265290260315, 0.8326898813247681, 0.8303987383842468, 0.8284271955490112, 0.8281900882720947, 0.8283170461654663, 0.8278393745422363, 0.8273726105690002, 0.8272925019264221, 0.8280697464942932, 0.8290920257568359, 0.8289236426353455, 0.8286654949188232, 0.8297555446624756, 0.831134021282196, 0.8344438672065735], 'test_auc': 0.6708284141314083, 'test_acc': 81.5, 'test_loss': 0.8826530575752258, 'best_val_auc': 0.6332832515508482, 'best_val_acc': 73.15, 'best_val_loss': 0.6449340581893921})\n",
            "('0.05', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210], 'train_loss_list': [1.013910174369812, 0.4669172167778015, 0.1615229845046997, 0.06678345054388046, 0.050551753491163254, 0.022031476721167564, 0.023166382685303688, 0.015381977893412113, 0.018483422696590424, 0.010982836596667767, 0.010948789305984974, 0.005454318132251501, 0.005669157952070236, 0.007119371090084314, 0.009032560512423515, 0.005134116858243942, 0.0018426976166665554, 0.009043862111866474, 0.005358927883207798, 0.00703605217859149, 0.008451603353023529, 0.007379199378192425, 0.0020915379282087088, 0.0027909206692129374, 0.005354592576622963, 0.0013014853466302156, 0.006980070844292641, 0.004871662240475416, 0.0027435666415840387, 0.003049211110919714, 0.0039197467267513275, 0.00584398815408349, 0.0029417872428894043, 0.004821606446057558, 0.005632685963064432, 0.0008526350720785558, 0.003495336277410388, 0.003984544426202774, 0.002682810416445136, 0.00666101323440671, 0.005119903478771448, 0.007562192156910896, 0.004506505094468594, 0.007302478421479464, 0.005556151270866394, 0.0034572691656649113, 0.004690120927989483, 0.0061494773253798485, 0.0068628909066319466, 0.00554729625582695, 0.0043148985132575035, 0.003094462212175131, 0.0096353180706501, 0.007819627411663532, 0.0024770363233983517, 0.005207387264817953, 0.006321979220956564, 0.00505190622061491, 0.0068216826766729355, 0.007828359492123127, 0.005117356777191162, 0.009166047908365726, 0.0028157385531812906, 0.004224632866680622, 0.007929678075015545, 0.005783883389085531, 0.006143908016383648, 0.007113939616829157, 0.00679873488843441, 0.004643614403903484, 0.0051094586960971355, 0.003508820431306958, 0.0029888611752539873, 0.004690550267696381, 0.006716000381857157, 0.006611004006117582, 0.004926562309265137, 0.004579657223075628, 0.0027749785222113132, 0.0026518458034843206, 0.006332495249807835, 0.004175588022917509, 0.005038956180214882, 0.008028107695281506, 0.0039162402972579, 0.0036517903208732605, 0.002750384621322155, 0.009858820587396622, 0.009042827412486076, 0.004788725636899471, 0.00318325636908412, 0.005690919701009989, 0.008717113174498081, 0.0012245788238942623, 0.004293526988476515, 0.00292999017983675, 0.002625267021358013, 0.006110718939453363, 0.0046030073426663876, 0.001794750802218914, 0.0031230084132403135, 0.003845280036330223, 0.004546368028968573, 0.0032724214252084494, 0.007272996474057436, 0.0025460892356932163, 0.007889394648373127, 0.0039286185055971146, 0.0031444502528756857, 0.005412851460278034, 0.006236609071493149, 0.00318133388645947, 0.0036778890062123537, 0.006309313699603081, 0.007311779074370861, 0.003462781896814704, 0.006243413779884577, 0.0030693053267896175, 0.0033938277047127485, 0.0042884438298642635, 0.0020941696129739285, 0.0033278611954301596, 0.005773743614554405, 0.004680686164647341, 0.004138358868658543, 0.002484661526978016, 0.006510396022349596, 0.0027788972947746515, 0.005654068663716316, 0.004289706237614155, 0.0032696647103875875, 0.006014521233737469, 0.0034187298733741045, 0.004389513283967972, 0.005453731864690781, 0.0030864542350172997, 0.0022330177016556263, 0.007832221686840057, 0.002830218756571412, 0.003437414765357971, 0.004882108420133591, 0.00641253124922514, 0.0015708815772086382, 0.007263534236699343, 0.004951495677232742, 0.002900074701756239, 0.004401479847729206, 0.0036715022288262844, 0.00664255628362298, 0.0030282556544989347, 0.006635029334574938, 0.0054985834285616875, 0.005715701729059219, 0.0034814276732504368, 0.004523687530308962, 0.0034595890901982784, 0.006912725046277046, 0.003629082115367055, 0.005789576098322868, 0.002437100512906909, 0.0028984982054680586, 0.005678087472915649, 0.005684224888682365, 0.0037500702310353518, 0.005187179893255234, 0.004376553930342197, 0.004837288986891508, 0.004404493607580662, 0.0018246789695695043, 0.004502679221332073, 0.005938177928328514, 0.004303310066461563, 0.00358149828389287, 0.005389128811657429, 0.0031753669027239084, 0.0037764492444694042, 0.003745388938114047, 0.00421888055279851, 0.005237000063061714, 0.0058676209300756454, 0.003338746028020978, 0.0026123893912881613, 0.002642771927639842, 0.0038744693156331778, 0.003538358025252819, 0.005757323931902647, 0.005382061004638672, 0.002350365975871682, 0.005347044672816992, 0.007781976368278265, 0.003060195129364729, 0.002640567719936371, 0.005321858450770378, 0.00593260582536459, 0.005026023369282484, 0.005769195035099983, 0.0032526517752557993, 0.005864739418029785, 0.002830162411555648, 0.004472886212170124, 0.005324193742126226, 0.0031156667973846197, 0.001346843782812357, 0.0028610082808882, 0.0028604594990611076, 0.004117934964597225, 0.0015427429461851716, 0.004933861084282398, 0.005322137847542763, 0.0024571276735514402, 0.0018770188326016068], 'val_auc_list': [0.6338068099737078, 0.6780539423882581, 0.6589559677559158, 0.5992025656583168, 0.5838308269047412, 0.5825433027072318, 0.584078219352229, 0.5898549232902834, 0.5994002990378782, 0.6050903253300973, 0.608371435381815, 0.6118078428823206, 0.6157336179249372, 0.619599802086042, 0.6227707592961775, 0.6251002210279969, 0.6275335153563896, 0.6319829678425933, 0.6392440264655745, 0.6465953743029672, 0.6522619253994395, 0.6560540724047268, 0.6587176042298691, 0.6610470659616885, 0.6632320649504493, 0.6664481667677905, 0.6692019878073444, 0.6712190488572998, 0.6724812920747739, 0.6738482707809657, 0.6755764063448036, 0.6774056658287827, 0.6789207188466094, 0.680824015486406, 0.6820356967438097, 0.6826911964404381, 0.6830234607494726, 0.6834965762329895, 0.6837331339747479, 0.6840942908323944, 0.685282496894051, 0.6880363179336049, 0.6909472422062349, 0.6928487330617434, 0.6914673080812458, 0.6893888503654908, 0.6897211146745253, 0.6899685071220132, 0.6897752882031725, 0.6916731674901042, 0.6936125798156656, 0.6945588107826991, 0.6953190459680448, 0.6954418392996445, 0.6941344514749646, 0.6921733697379445, 0.6909671058334055, 0.6899323914362486, 0.6898547427118547, 0.6924839646355206, 0.6945371413712403, 0.6950066452861806, 0.6941344514749646, 0.693973736673312, 0.6942409927479704, 0.6959402357631966, 0.6980060529889341, 0.6980042472046459, 0.6959059258617202, 0.6922437953251857, 0.6901418624136836, 0.6896055444800786, 0.6895206726185317, 0.6911531016150934, 0.6939448441247003, 0.6973776400566293, 0.6997432174742134, 0.7005233162867296, 0.700114306145445, 0.6989992343474618, 0.6983997139637688, 0.6982101066135045, 0.698573069255439, 0.698798792291468, 0.6985008378839099, 0.6974625119181762, 0.6968485452601775, 0.6973812516252058, 0.6991274450319264, 0.7002668949178008, 0.7001693825662363, 0.6998136430614545, 0.6995301349282019, 0.7001097916847244, 0.7008971136343937, 0.7018090346999509, 0.7027155384126434, 0.7024374476322557, 0.7012113201005461, 0.7003896882494005, 0.700183828840542, 0.6996294530640547, 0.6990606310132617, 0.6993910895380082, 0.7008718326543585, 0.7019336338158391, 0.7021936667533443, 0.7020636502845916, 0.7033132530120482, 0.7033222819334893, 0.7026938690011846, 0.7024645343965792, 0.7024139724365087, 0.7013991216665222, 0.699941853745919, 0.6998010025714367, 0.6991960648348791, 0.6991978706191674, 0.6992574615006789, 0.7002614775649361, 0.70099282020167, 0.7016609603883159, 0.7009205888301409, 0.6997820418364105, 0.6994506804195199, 0.6992213458149145, 0.6985306333246656, 0.6978561728930108, 0.6971970616278063, 0.6973361070180001, 0.6980493918118517, 0.6985550114125567, 0.699530134928202, 0.6990299326803616, 0.6973541648608823, 0.695172777440698, 0.6947375834272341, 0.6953768310652683, 0.6965921238912485, 0.6987644823899917, 0.7007057004998412, 0.7012356981884372, 0.7010533139753257, 0.6996384819854959, 0.6975473837797233, 0.6957307647857618, 0.6951998642050216, 0.6965505908526191, 0.6985459824911155, 0.700518801826009, 0.7018334127878421, 0.7003896882494005, 0.6987382985178123, 0.6972033818728149, 0.6968205556037098, 0.6974164644188263, 0.698308521857213, 0.6989360318973736, 0.6987238522435064, 0.6992105111091849, 0.6999680376180983, 0.7003761448672388, 0.7001431986940567, 0.6978047080407963, 0.6961397749270464, 0.6961181055155875, 0.6962210352200167, 0.6965280185490162, 0.6971401794227269, 0.6982850466614661, 0.6980864103897604, 0.697880550980902, 0.6987094059692006, 0.6993233726271995, 0.6988520629279709, 0.6989856909653, 0.699366711450117, 0.6973677082430442, 0.6958309858137586, 0.6971239273641328, 0.6973406214787206, 0.6973749313801971, 0.6980340426454018, 0.6969505720724625, 0.6939638048597266, 0.6917860290081189, 0.6923891609603883, 0.6962083947299992, 0.6974345222617087, 0.697288253734362, 0.6961343575741816, 0.6960296220854643, 0.695764171795094, 0.693375119181763, 0.6906375502008033, 0.6884742206235012, 0.6887920386582299, 0.6921706610615124, 0.6952079902343185, 0.6966345598220219, 0.6971293447169975, 0.6963239649244459, 0.6960657377712289, 0.6944856765190257, 0.6920478677299124, 0.6926392620843085, 0.6950328291583601, 0.6958093164022999, 0.6961795021813875, 0.6971148984426917, 0.6971058695212504], 'val_acc_list': [73.6, 81.85, 77.10000000000001, 75.9, 75.94999999999999, 76.75, 77.95, 78.8, 78.95, 79.10000000000001, 79.2, 79.25, 79.75, 80.45, 80.95, 81.10000000000001, 81.25, 81.5, 81.85, 82.05, 82.1, 81.85, 81.85, 82.05, 82.1, 82.25, 82.5, 82.35, 82.39999999999999, 82.39999999999999, 82.45, 82.25, 82.25, 82.39999999999999, 82.45, 82.45, 82.69999999999999, 82.65, 82.6, 82.6, 82.65, 82.75, 82.8, 82.6, 82.35, 82.3, 82.6, 82.65, 82.8, 82.75, 82.75, 82.35, 82.45, 82.5, 83.0, 82.89999999999999, 82.89999999999999, 82.85, 82.8, 82.69999999999999, 82.75, 82.8, 83.05, 83.1, 83.2, 83.0, 82.8, 82.85, 83.05, 83.05, 83.05, 83.25, 83.0, 83.05, 83.05, 83.0, 83.1, 83.15, 83.05, 83.05, 83.05, 83.15, 83.1, 83.05, 83.15, 83.2, 83.15, 82.89999999999999, 82.89999999999999, 82.95, 83.05, 83.3, 83.2, 83.0, 82.85, 82.69999999999999, 82.8, 82.95, 83.05, 83.2, 83.15, 82.95, 82.75, 82.8, 82.89999999999999, 83.0, 83.05, 83.15, 83.0, 82.75, 82.5, 82.5, 82.6, 82.8, 82.89999999999999, 83.0, 83.3, 83.3, 83.25, 82.8, 82.8, 82.8, 82.89999999999999, 83.2, 83.25, 83.35000000000001, 83.05, 83.0, 83.0, 82.95, 83.0, 83.0, 83.05, 83.0, 82.8, 82.75, 82.75, 82.95, 82.95, 82.89999999999999, 82.69999999999999, 82.75, 82.85, 82.8, 82.8, 82.95, 82.75, 82.85, 82.85, 83.1, 83.1, 82.95, 82.69999999999999, 82.69999999999999, 82.95, 83.1, 83.1, 82.95, 82.8, 83.0, 83.05, 82.95, 82.85, 82.69999999999999, 82.85, 83.1, 83.05, 83.05, 83.15, 83.15, 82.75, 82.85, 82.8, 82.95, 83.15, 83.2, 82.85, 82.65, 82.85, 82.85, 82.89999999999999, 83.05, 83.05, 82.85, 82.75, 82.69999999999999, 82.8, 83.05, 83.1, 82.95, 82.85, 82.89999999999999, 83.2, 83.1, 83.1, 82.89999999999999, 83.0, 83.0, 82.95, 82.8, 82.89999999999999, 82.89999999999999, 82.89999999999999, 82.8, 82.65, 82.8, 83.0, 82.95, 82.8, 82.75, 82.85], 'val_loss_list': [0.7355036735534668, 0.6201246380805969, 0.6083237528800964, 0.7692049741744995, 0.8714165687561035, 0.920510470867157, 0.9676781296730042, 1.0095818042755127, 1.0386579036712646, 1.0678263902664185, 1.0889335870742798, 1.1022487878799438, 1.1207501888275146, 1.1372787952423096, 1.1454275846481323, 1.146885633468628, 1.152355432510376, 1.1704084873199463, 1.1833090782165527, 1.1771678924560547, 1.1484273672103882, 1.1117610931396484, 1.0816469192504883, 1.0627446174621582, 1.0555042028427124, 1.0610941648483276, 1.0616137981414795, 1.0447190999984741, 1.022829532623291, 1.0176661014556885, 1.0212945938110352, 1.0176541805267334, 1.0085080862045288, 0.9997534155845642, 0.9865608811378479, 0.9685943126678467, 0.9516957998275757, 0.9382397532463074, 0.925709068775177, 0.9279560446739197, 0.9413096308708191, 0.9469516277313232, 0.9320716261863708, 0.8937418460845947, 0.8527272343635559, 0.8398523926734924, 0.8575584888458252, 0.9019909501075745, 0.9334441423416138, 0.9149206280708313, 0.8621106147766113, 0.8154446482658386, 0.7963487505912781, 0.8082871437072754, 0.8436776995658875, 0.8842272162437439, 0.8980220556259155, 0.8767178058624268, 0.8422881960868835, 0.8108146786689758, 0.8023468255996704, 0.8207631707191467, 0.8538058400154114, 0.8728917837142944, 0.854100227355957, 0.8107722401618958, 0.7840425968170166, 0.7891598343849182, 0.8162511587142944, 0.8480871319770813, 0.8600428700447083, 0.8393279314041138, 0.8208425045013428, 0.8195817470550537, 0.8260155320167542, 0.8224805593490601, 0.809490442276001, 0.7992226481437683, 0.7898094058036804, 0.7939857244491577, 0.8049818277359009, 0.8089084029197693, 0.8054010272026062, 0.8101797699928284, 0.820846676826477, 0.8266680836677551, 0.8158519268035889, 0.7989446520805359, 0.7907083630561829, 0.7941765785217285, 0.800201416015625, 0.8076284527778625, 0.8096519112586975, 0.8014392256736755, 0.7945610880851746, 0.7883298397064209, 0.7896735668182373, 0.7965425848960876, 0.8133066296577454, 0.8402755260467529, 0.8442133665084839, 0.8256617188453674, 0.8007791638374329, 0.7894243597984314, 0.7968729734420776, 0.8123053312301636, 0.8269383907318115, 0.8376312255859375, 0.8262317776679993, 0.8164427876472473, 0.811992883682251, 0.8094422221183777, 0.8062269687652588, 0.7999660968780518, 0.7973033785820007, 0.810043454170227, 0.8298150300979614, 0.8442999720573425, 0.8403128385543823, 0.8163751363754272, 0.7978150844573975, 0.7969034910202026, 0.817500650882721, 0.8535401821136475, 0.8709456324577332, 0.8488596677780151, 0.822350263595581, 0.816909670829773, 0.8195385932922363, 0.8239371180534363, 0.828343391418457, 0.8359851241111755, 0.8448951244354248, 0.8372219204902649, 0.8128328919410706, 0.8036141395568848, 0.8155763149261475, 0.8311588168144226, 0.8476355075836182, 0.8492770791053772, 0.8316947221755981, 0.8241333961486816, 0.8295658826828003, 0.8344502449035645, 0.8362066745758057, 0.8317959308624268, 0.8262657523155212, 0.8227989673614502, 0.8215212225914001, 0.8394713997840881, 0.8588292002677917, 0.8488191962242126, 0.8207830190658569, 0.8163043856620789, 0.8353139758110046, 0.8534117341041565, 0.8483232855796814, 0.8236302137374878, 0.8125046491622925, 0.8216699361801147, 0.83486407995224, 0.8305157423019409, 0.8185296654701233, 0.8156372308731079, 0.8178564310073853, 0.8347146511077881, 0.8584237098693848, 0.8583315014839172, 0.8488410115242004, 0.8312785625457764, 0.8089277148246765, 0.8029809594154358, 0.8132207989692688, 0.8385694622993469, 0.8634265661239624, 0.8598355054855347, 0.8387694358825684, 0.8130396604537964, 0.803209662437439, 0.8244475722312927, 0.8511462807655334, 0.8684661984443665, 0.868887186050415, 0.851492702960968, 0.8343426585197449, 0.8310716152191162, 0.8431154489517212, 0.8610045313835144, 0.8639760613441467, 0.8549367189407349, 0.8428231477737427, 0.8419889211654663, 0.8585494160652161, 0.8754246830940247, 0.8675392270088196, 0.854508101940155, 0.8541285991668701, 0.8612911105155945, 0.8730174899101257, 0.879332423210144, 0.8797014355659485, 0.8878766298294067, 0.8811686038970947, 0.8593324422836304, 0.8408646583557129, 0.8541017770767212, 0.888636589050293, 0.8987290263175964, 0.8744298815727234, 0.8467605113983154, 0.8473330736160278], 'test_auc': 0.6739439303211758, 'test_acc': 82.19999999999999, 'test_loss': 0.8742157220840454, 'best_val_auc': 0.6589559677559158, 'best_val_acc': 77.10000000000001, 'best_val_loss': 0.6083237528800964})\n",
            "('0.1', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161], 'train_loss_list': [1.5004674196243286, 3.6685471534729004, 1.5810447931289673, 0.4259319305419922, 0.369558721780777, 0.2041487842798233, 0.19993971288204193, 0.12785704433918, 0.11916542053222656, 0.144079327583313, 0.12338805943727493, 0.07684242725372314, 0.07002460956573486, 0.05800480395555496, 0.03839028999209404, 0.08099786937236786, 0.052519045770168304, 0.04139569401741028, 0.039643947035074234, 0.06094186753034592, 0.04096518084406853, 0.028148896992206573, 0.03395582735538483, 0.03958689048886299, 0.04778209701180458, 0.029700500890612602, 0.06656298786401749, 0.03700976073741913, 0.020570244640111923, 0.034733399748802185, 0.057333849370479584, 0.024158800020813942, 0.026485679671168327, 0.005109657533466816, 0.001010625041089952, 0.05619502067565918, 0.01756598986685276, 0.03934576362371445, 0.01974223181605339, 0.00988862756639719, 0.03580576181411743, 0.03133304417133331, 0.037400439381599426, 0.03003224916756153, 0.01048498135060072, 0.01820070669054985, 0.027478497475385666, 0.009003696031868458, 0.02635478414595127, 0.034824322909116745, 0.01997677981853485, 0.04438677430152893, 0.027232909575104713, 0.026955824345350266, 0.02125084400177002, 0.016571713611483574, 0.06456427276134491, 0.05076475441455841, 0.01176432240754366, 0.011221442371606827, 0.011096271686255932, 0.028074687346816063, 0.01915682666003704, 0.012934446334838867, 0.022301876917481422, 0.004427925683557987, 0.04574555903673172, 0.009401371702551842, 0.0003596892929635942, 0.0011373863089829683, 0.009188312105834484, 0.0017133114160969853, 0.04578298702836037, 0.019465738907456398, 0.019803857430815697, 0.00519405584782362, 0.010152880102396011, 0.026060029864311218, 0.013828255236148834, 0.0037361944559961557, 0.020201250910758972, 0.010901079513132572, 0.009944895282387733, 0.018401319161057472, 0.022034481167793274, 0.05909455940127373, 0.001803604420274496, 0.01292482390999794, 0.01953830011188984, 0.05138605460524559, 0.01701335236430168, 0.01349733117967844, 0.013209488242864609, 0.018214447423815727, 0.00990225188434124, 0.03193766996264458, 0.03337208926677704, 0.014133360236883163, 0.009129884652793407, 0.006451526191085577, 0.024859488010406494, 0.012739703990519047, 0.006256974767893553, 0.0022131791338324547, 0.012950878590345383, 0.029028205201029778, 0.02596193552017212, 0.031009159982204437, 0.04580453038215637, 0.005124750081449747, 0.030683910474181175, 0.0005786860710941255, 0.04450361430644989, 0.029050996527075768, 0.010746907442808151, 0.0314650759100914, 0.009632589295506477, 0.011495820246636868, 0.0006821398274041712, 0.010808160528540611, 0.0034521007910370827, 0.028205161914229393, 0.027348782867193222, 0.03367648646235466, 0.01846178248524666, 0.018331890925765038, 0.005572934169322252, 0.03951358050107956, 0.027577504515647888, 0.009793062694370747, 0.011796252802014351, 0.03662837669253349, 0.02929152175784111, 0.017887888476252556, 0.0060045248828828335, 0.004399364814162254, 0.023139601573348045, 0.013749170117080212, 0.010434179566800594, 0.010543566197156906, 0.01067583542317152, 0.012953490950167179, 0.028564510866999626, 0.027577051892876625, 0.023059803992509842, 0.012652995996177197, 0.040432438254356384, 0.04017816111445427, 0.0007423985516652465, 0.020834241062402725, 0.03543565794825554, 0.024159681051969528, 0.031445182859897614, 0.01152113452553749, 0.02247455157339573, 0.05220817029476166, 0.03201692923903465, 0.009424128569662571, 0.012464914470911026, 0.022047335281968117, 0.015184750780463219, 0.0031352159567177296], 'val_auc_list': [0.5756744183520714, 0.6090898723659988, 0.6536332518160377, 0.5567184793253191, 0.5373914586044795, 0.542163026144063, 0.5498987218498332, 0.5532285012462914, 0.5536321657232743, 0.5503901788251734, 0.541673379323328, 0.5340164253428886, 0.5328307740764139, 0.5368882356242046, 0.5416145492986556, 0.5457923861276992, 0.549974748343256, 0.5549535785851469, 0.5610556097596295, 0.5657375746462505, 0.5693551686249523, 0.5730234469326024, 0.5777651469211984, 0.5815637563604308, 0.5837974871433769, 0.5866638669608771, 0.5908371784034074, 0.5944538673048065, 0.59668669301045, 0.5983990992670684, 0.6004219470384966, 0.6027199383099311, 0.6043536028412186, 0.6066298722573895, 0.6093405787788335, 0.6110285479482803, 0.6120096517443555, 0.6136740889039333, 0.615854420126023, 0.6173912413859268, 0.619260226015904, 0.6212885042511481, 0.6230515948367149, 0.6246472461212912, 0.6252120143581462, 0.6254609106163757, 0.626163250603234, 0.62875177168882, 0.6311665179322966, 0.6335151935326796, 0.6349642222942262, 0.636495613090314, 0.6381030303798247, 0.6396353262532153, 0.6415224124292457, 0.6428691674555924, 0.6439045758898267, 0.6437126995016644, 0.644469344126682, 0.6457672249786854, 0.6463989689359368, 0.6455753485905231, 0.6444602933536554, 0.6448730086036649, 0.6444177547204307, 0.6443797414737192, 0.643380536131591, 0.6435072469539624, 0.6435561211283056, 0.6441308452154899, 0.6443688805460874, 0.6423614190888043, 0.6396887258140718, 0.6387121474045099, 0.6390615072433335, 0.6401874234078332, 0.642620271197363, 0.6454993220971003, 0.6452721476941344, 0.6437027436513353, 0.643653869476992, 0.644039432407922, 0.6434076884506706, 0.6424048627993317, 0.6413423020460177, 0.6408951938585076, 0.642724355087168, 0.6460966731168509, 0.648950381852114, 0.6497377991054216, 0.6503930750725418, 0.6505577991416247, 0.6502591236317494, 0.6497613311152906, 0.6520086380577765, 0.6531390796087894, 0.6538559008324901, 0.6543138699476323, 0.6548677772568556, 0.6558289693522724, 0.6561982408917546, 0.6561303600940556, 0.6557049737618089, 0.6526168500051588, 0.6528675564179937, 0.6538712871466352, 0.6547745542946822, 0.6552343335644298, 0.6561910002733333, 0.6590962984148476, 0.6637990800794296, 0.6682574908722954, 0.6690322370433659, 0.6695173584775875, 0.669044908125603, 0.6693119059298854, 0.6677081089495854, 0.6665586607752167, 0.6668971596864088, 0.6649087048524814, 0.6612757245596346, 0.6594999628918305, 0.657465349115468, 0.6532015299426723, 0.6525064305742354, 0.6530006027814836, 0.6529444879887192, 0.652647622633449, 0.6548523909427104, 0.6570643998703929, 0.6578029429493574, 0.6574137597092168, 0.6572707574953978, 0.6572417950217129, 0.6572327442486863, 0.656559366735513, 0.6569087265743367, 0.6576889032092231, 0.6579015963753464, 0.656952170284864, 0.6549953931565295, 0.6549048854262643, 0.6562787927716908, 0.65674943296907, 0.6577432078473823, 0.6591542233622173, 0.660493737770143, 0.661678483959315, 0.6617762323080014, 0.6614359232422041, 0.6606738481533707, 0.6611707355925269, 0.6626822146879565, 0.6647584620202412, 0.6674736939281984, 0.6697816410499621, 0.6716008464282935, 0.6728082195500318, 0.6739151290911758, 0.6730625462720772, 0.6696884180877889, 0.6657060779561182], 'val_acc_list': [83.45, 78.75, 77.8, 58.35, 79.10000000000001, 81.8, 81.2, 81.0, 80.45, 80.35, 81.05, 81.5, 81.6, 81.5, 81.3, 81.45, 81.45, 81.5, 81.3, 81.3, 81.39999999999999, 81.35, 81.45, 81.55, 81.55, 81.6, 81.55, 81.6, 81.6, 81.65, 81.65, 81.65, 81.6, 81.6, 81.45, 81.5, 81.39999999999999, 81.39999999999999, 81.45, 81.39999999999999, 81.35, 81.39999999999999, 81.45, 81.5, 81.5, 81.55, 81.5, 81.65, 81.69999999999999, 81.75, 81.85, 81.89999999999999, 81.89999999999999, 81.89999999999999, 81.95, 82.0, 82.05, 82.0, 81.95, 82.05, 82.05, 82.05, 82.0, 82.0, 82.05, 82.05, 82.0, 82.05, 82.0, 82.1, 82.1, 82.05, 82.1, 82.05, 82.05, 82.0, 82.15, 82.25, 82.19999999999999, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.39999999999999, 82.35, 82.39999999999999, 82.39999999999999, 82.5, 82.5, 82.5, 82.5, 82.6, 82.5, 82.65, 82.69999999999999, 82.75, 82.85, 82.8, 82.69999999999999, 82.5, 82.45, 82.45, 82.5, 82.39999999999999, 82.35, 82.35, 82.3, 82.35, 82.39999999999999, 82.45, 82.39999999999999, 82.39999999999999, 82.5, 82.39999999999999, 82.3, 82.19999999999999, 82.15, 82.35, 82.39999999999999, 82.5, 82.45, 82.45, 82.39999999999999, 82.45, 82.6, 82.69999999999999, 82.6, 82.65, 82.69999999999999, 82.75, 82.55, 82.35, 82.3, 82.39999999999999, 82.6, 82.65, 82.55, 82.45, 82.45, 82.39999999999999, 82.45, 82.45, 82.39999999999999, 82.3, 82.35, 82.35, 82.45, 82.55, 82.69999999999999, 82.55, 82.39999999999999, 82.5, 82.5, 82.39999999999999, 82.39999999999999, 82.35, 82.3, 82.45, 82.35, 82.35, 82.5, 82.6], 'val_loss_list': [3.6985950469970703, 3.5606260299682617, 1.6985368728637695, 3.126338243484497, 2.0085084438323975, 2.5941598415374756, 2.930595874786377, 3.110938549041748, 3.2845253944396973, 3.4891021251678467, 3.689540147781372, 3.8593387603759766, 3.9536936283111572, 3.9779226779937744, 3.977766275405884, 3.949765205383301, 3.927915096282959, 3.9149115085601807, 3.8839292526245117, 3.851661443710327, 3.812696933746338, 3.7731921672821045, 3.7320947647094727, 3.69016432762146, 3.634920835494995, 3.598680257797241, 3.53959321975708, 3.479330539703369, 3.4055211544036865, 3.33197021484375, 3.2605531215667725, 3.2039527893066406, 3.164048671722412, 3.1177425384521484, 3.056523084640503, 2.9873151779174805, 2.9254424571990967, 2.8734164237976074, 2.8193788528442383, 2.7787954807281494, 2.731431245803833, 2.6810412406921387, 2.6212470531463623, 2.5695834159851074, 2.5152902603149414, 2.4573607444763184, 2.406895160675049, 2.366657018661499, 2.3284645080566406, 2.2963619232177734, 2.265082836151123, 2.2355129718780518, 2.2022829055786133, 2.1682193279266357, 2.1481716632843018, 2.1190295219421387, 2.099034070968628, 2.0801098346710205, 2.0595688819885254, 2.0462489128112793, 2.031304121017456, 2.009122610092163, 1.9912960529327393, 1.9685708284378052, 1.9509713649749756, 1.944397211074829, 1.9362605810165405, 1.921278953552246, 1.9078197479248047, 1.896274447441101, 1.890335202217102, 1.899233341217041, 1.8990862369537354, 1.8762316703796387, 1.8466441631317139, 1.8237208127975464, 1.8002361059188843, 1.7757325172424316, 1.758155345916748, 1.7496260404586792, 1.7295281887054443, 1.7144619226455688, 1.7100236415863037, 1.7023746967315674, 1.6849511861801147, 1.6652863025665283, 1.647566556930542, 1.6467134952545166, 1.6373157501220703, 1.626030683517456, 1.6126465797424316, 1.6063481569290161, 1.5982376337051392, 1.5900332927703857, 1.575355052947998, 1.574683666229248, 1.5882641077041626, 1.5892313718795776, 1.5798237323760986, 1.5644221305847168, 1.5413389205932617, 1.5204499959945679, 1.5025566816329956, 1.4992729425430298, 1.4908957481384277, 1.472406029701233, 1.4537413120269775, 1.43919837474823, 1.4257102012634277, 1.4188952445983887, 1.4183940887451172, 1.4193260669708252, 1.4188772439956665, 1.4061439037322998, 1.3889814615249634, 1.3727577924728394, 1.3744193315505981, 1.377113699913025, 1.3788071870803833, 1.375313401222229, 1.3773938417434692, 1.3678640127182007, 1.371280312538147, 1.3826665878295898, 1.3952386379241943, 1.399375319480896, 1.4078667163848877, 1.42617928981781, 1.4328373670578003, 1.4208608865737915, 1.3996917009353638, 1.386895775794983, 1.391067385673523, 1.3982808589935303, 1.4200372695922852, 1.4364888668060303, 1.4421212673187256, 1.4323607683181763, 1.4209258556365967, 1.419716477394104, 1.4062445163726807, 1.3950804471969604, 1.395097255706787, 1.3868409395217896, 1.3721308708190918, 1.3537323474884033, 1.3478937149047852, 1.3448498249053955, 1.3497296571731567, 1.3520424365997314, 1.329995036125183, 1.3142073154449463, 1.3078244924545288, 1.2969716787338257, 1.2871321439743042, 1.2805149555206299, 1.2796998023986816, 1.2784212827682495, 1.2735525369644165, 1.279531478881836, 1.2794605493545532, 1.2872321605682373], 'test_auc': 0.6512784754620411, 'test_acc': 82.8, 'test_loss': 1.2583823204040527, 'best_val_auc': 0.6739151290911758, 'best_val_acc': 82.35, 'best_val_loss': 1.2735525369644165})\n",
            "('0.2', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], 'train_loss_list': [0.9227175116539001, 22.92800521850586, 22.71942901611328, 22.026708602905273, 21.116437911987305, 19.933992385864258, 16.923561096191406, 12.2068510055542, 5.748352527618408, 4.326027870178223, 4.2571821212768555, 4.3967814445495605, 4.305884838104248, 4.448596000671387, 4.442193508148193, 4.088123321533203, 4.356334209442139, 4.434608459472656, 4.300789833068848, 4.427009105682373, 4.614651679992676, 4.146683216094971, 4.212639331817627, 4.490705490112305, 4.322580814361572, 4.207108974456787, 4.182480335235596, 4.320115089416504, 4.365220546722412, 4.29434061050415, 4.110308647155762, 4.385809421539307, 3.66087007522583, 3.9821889400482178, 3.944650888442993, 3.83707332611084, 3.9564881324768066, 3.955822229385376, 4.076498031616211, 3.823974609375, 3.8484396934509277, 3.8631088733673096, 4.051031589508057, 4.07572603225708, 3.8102097511291504, 3.880272150039673, 3.8429291248321533, 3.8117477893829346, 3.8856887817382812, 3.8432369232177734, 3.5859878063201904, 4.073732376098633, 4.009286403656006, 3.7562241554260254, 3.860504150390625, 3.820608615875244, 3.809072971343994, 3.809922456741333, 3.4278807640075684, 3.4823315143585205, 3.758063316345215, 3.6353979110717773, 4.011236667633057, 3.7478644847869873, 3.7532577514648438, 4.015087604522705, 3.5682413578033447, 3.608880043029785, 3.777388334274292, 3.74617862701416, 3.5417654514312744, 3.974400520324707, 3.705209255218506, 3.892791748046875, 3.7652997970581055, 3.782020330429077, 3.8510749340057373, 3.869332790374756, 3.756100654602051, 3.897472620010376, 3.5701348781585693, 3.7000434398651123, 3.9409737586975098, 3.9532439708709717, 3.9381914138793945, 3.621354579925537, 3.5802295207977295, 3.993027925491333, 3.975989580154419, 3.588080406188965, 4.045569896697998, 3.8687222003936768, 3.774468421936035, 3.7255094051361084, 3.5214426517486572, 3.6506454944610596, 3.496227502822876, 3.809114933013916, 3.9967682361602783, 3.4928858280181885, 3.7733702659606934, 3.6897027492523193, 3.8178741931915283, 3.596212863922119, 3.7787537574768066, 3.9196653366088867, 3.7426421642303467, 3.700274705886841, 3.7234997749328613, 3.8100409507751465, 3.79182767868042, 3.8236968517303467, 3.6393613815307617, 3.5753917694091797, 3.977806329727173, 3.8281259536743164, 3.6446335315704346, 3.670823574066162, 3.5655038356781006, 3.900871515274048, 3.6407477855682373, 3.4536542892456055, 3.8161730766296387, 3.815559148788452, 3.7447524070739746, 3.615868330001831, 3.7354509830474854, 3.6947927474975586, 3.6611487865448, 3.6623380184173584, 3.6500208377838135, 3.4603476524353027, 3.6276330947875977, 3.5121755599975586, 3.8029065132141113, 3.914469003677368, 3.806556463241577, 3.7533833980560303, 3.7157135009765625, 4.066006660461426, 3.86834716796875, 3.6654052734375, 3.9564120769500732, 3.564164161682129, 3.7360825538635254, 3.613401412963867, 3.4965906143188477, 3.4390387535095215, 3.6132466793060303, 3.786539316177368, 3.6901493072509766, 3.811753511428833, 3.4934961795806885, 3.6649627685546875, 3.562993049621582, 3.488962411880493, 3.4024691581726074, 3.4075167179107666, 3.5490148067474365, 3.373304843902588, 3.3963263034820557, 3.603119134902954, 3.6595075130462646, 3.5550734996795654, 3.672795295715332, 3.412473201751709, 3.289818525314331, 3.4004454612731934, 3.497889995574951, 3.7317075729370117, 3.396616220474243, 3.604719638824463, 3.7853317260742188, 3.5888671875, 3.269169330596924, 3.7486183643341064, 3.3164803981781006, 3.3218164443969727, 3.225344181060791, 3.7969043254852295, 3.4833173751831055, 3.4007794857025146, 3.4211950302124023, 3.5873863697052, 3.5242066383361816, 3.2570908069610596, 3.522188425064087, 3.3916330337524414, 3.4494495391845703, 3.2713382244110107, 3.3465991020202637, 3.456357479095459, 3.4251043796539307, 3.6179370880126953, 3.47050404548645, 3.1253576278686523, 3.1658031940460205, 3.4418604373931885, 3.503141164779663, 3.6654040813446045, 3.4052915573120117, 3.540864944458008, 3.2784972190856934, 3.4817819595336914, 3.516690254211426, 3.429597854614258, 3.324357271194458, 3.466137647628784, 3.202984571456909, 3.608994960784912, 3.3583076000213623, 3.39803409576416, 3.5496878623962402], 'val_auc_list': [0.5003545949893271, 0.5046448432760363, 0.5056252457589034, 0.5055646837433997, 0.5199547452533424, 0.525556292832266, 0.5699605381417818, 0.6224976477362093, 0.5689976898663072, 0.5240167888439502, 0.5210790922368274, 0.5326578474328726, 0.5523571789686552, 0.563483035613976, 0.5663496376811594, 0.5672264703404112, 0.5712841253791708, 0.5684982726659925, 0.5721486700932479, 0.5740840214582631, 0.5768689964610717, 0.5771823390630266, 0.5800980929109089, 0.5841364383215369, 0.5746053814178183, 0.5717089371980676, 0.573483667565442, 0.5669315596562182, 0.5660740366250984, 0.5709602502527805, 0.5717668660824626, 0.5739444655094934, 0.5717010378047409, 0.5718616588023817, 0.5707101027974385, 0.5656843332209864, 0.5675880870126953, 0.5645696410515673, 0.5625447632288507, 0.5543320273003033, 0.5516365787551961, 0.550674608190091, 0.5511555934726435, 0.5565903760813391, 0.5552887316031907, 0.5559566691944726, 0.5603724300640378, 0.561538029434895, 0.5652568882709809, 0.5652305569598922, 0.5658396879564094, 0.5646749662959218, 0.5627141613301876, 0.5626702758117066, 0.5610965411189754, 0.5596351533535558, 0.5596737726098192, 0.5595921455454443, 0.5624578699022582, 0.5634400278058646, 0.5636954415234243, 0.5633048604089429, 0.5629836184136614, 0.5616170233681609, 0.5604092938995618, 0.5599563953488372, 0.5611000519604539, 0.5617793997865409, 0.5612299530951579, 0.5629809852825525, 0.5619496755982475, 0.5660231294236603, 0.5667463627682282, 0.5671694191663859, 0.5676732249185485, 0.5677241321199866, 0.5691758650713403, 0.569093360296596, 0.567980423547916, 0.566874508482193, 0.5658133566453207, 0.5647504493877092, 0.5651550738681047, 0.5609640068531626, 0.561078986911583, 0.56064539798899, 0.5586802044714076, 0.5642694641051567, 0.5642606870014606, 0.5605444612964836, 0.5581746432985057, 0.5569879788787777, 0.5552448460847096, 0.5549385251657116, 0.5535780740927986, 0.5530101954836536, 0.5531927592405348, 0.553491180766206, 0.5540774912931131, 0.5546243048533873, 0.555132499157398, 0.5535912397483428, 0.5533630350522414, 0.5519499213571508, 0.5530900671272891, 0.5516848528255253, 0.5515224764071452, 0.5496485647680036, 0.5473129774744411, 0.5470154336591394, 0.5492061987417145, 0.551991173744523, 0.5508624382091899, 0.5515330089315806, 0.553187492978317, 0.5543416821143692, 0.5541020671834626, 0.553671989102348, 0.5541204991012246, 0.5507211268396809, 0.5532515658352993, 0.5526766655431974, 0.5516708094596113, 0.556050584204022, 0.5532840411189753, 0.5524309066397034, 0.552424762667116, 0.5544355971239187, 0.5560821817773285, 0.5555555555555556, 0.5567079892708684, 0.5597992851926751, 0.5611079513537806, 0.5594253805752162, 0.5590839512414336, 0.5579280066846423, 0.5575312815975734, 0.5600231013369285, 0.5586600171329064, 0.5595649365239861, 0.5551658521514438, 0.556660592910909, 0.5569116180766206, 0.5566518158072127, 0.5533419700033704, 0.5489112880575217, 0.5525924053477137, 0.5583993371531288, 0.5569897342995168, 0.5563990352207617, 0.5605786920008988, 0.5561497654757892, 0.5580754620267386, 0.5521965579710145, 0.5550543829345018, 0.5510634338838334, 0.5524045753286148, 0.5525169222559263, 0.5526573559150657, 0.5536754999438266, 0.5544101435231997, 0.5551798955173576, 0.5550148859678689, 0.5512802283451297, 0.5525423758566453, 0.5527433715312886, 0.5504674685428603, 0.5496239888776542, 0.5499162664307381, 0.559987115211774, 0.5597027370520167, 0.5570441523424334, 0.5610403676553196, 0.5650260504437704, 0.5649514450623526, 0.5598949556229638, 0.5588223935512864, 0.5527995449949444, 0.5519174460734748, 0.5568273578811369, 0.5559900221885182, 0.5537948685540951, 0.5531708164812943, 0.5533138832715426, 0.5534727488484441, 0.5545374115267948, 0.5576927803055837, 0.5564016683518707, 0.5561655642624425, 0.5547234861251545, 0.5509984833164813, 0.5510651893045725, 0.5521500393214246, 0.553931791371756, 0.5546067506459949, 0.5557047663183913, 0.5573899702280642, 0.5612931482417706, 0.5656299151780699, 0.5629081353218741, 0.55823432760364, 0.5568958192899675, 0.5582650474665768, 0.5574127906976745, 0.5536588234468037, 0.5572907889562971, 0.5592454499494439, 0.558371250421301, 0.5553879128749579, 0.5559382372767105, 0.5572512919896642, 0.5536061608246264, 0.5537035866756544], 'val_acc_list': [17.2, 18.0, 18.65, 20.349999999999998, 25.35, 30.75, 42.85, 65.45, 83.15, 82.89999999999999, 82.89999999999999, 83.05, 83.35000000000001, 83.45, 83.45, 83.15, 83.15, 82.65, 82.3, 82.25, 82.25, 81.95, 81.55, 81.3, 81.55, 82.1, 81.69999999999999, 81.89999999999999, 82.15, 82.25, 82.35, 82.39999999999999, 82.45, 82.55, 82.55, 82.75, 82.95, 83.15, 83.05, 82.95, 82.75, 82.75, 82.6, 82.6, 82.8, 82.85, 82.75, 83.05, 83.25, 83.1, 83.25, 83.25, 83.35000000000001, 83.3, 83.0, 83.0, 83.0, 83.05, 83.05, 83.1, 83.2, 83.3, 83.15, 83.2, 83.25, 83.2, 83.05, 83.0, 83.0, 83.0, 83.0, 83.1, 83.15, 83.05, 83.1, 83.2, 83.2, 83.1, 83.15, 83.2, 83.3, 83.35000000000001, 83.3, 83.2, 83.1, 83.2, 83.15, 83.2, 83.5, 83.35000000000001, 83.25, 83.25, 83.25, 83.15, 83.2, 83.25, 83.2, 83.15, 83.15, 82.95, 82.89999999999999, 82.69999999999999, 82.69999999999999, 82.75, 82.8, 82.75, 82.95, 82.95, 82.85, 82.8, 82.89999999999999, 82.95, 83.1, 83.1, 83.1, 83.05, 83.05, 82.95, 82.89999999999999, 82.89999999999999, 82.85, 82.8, 82.95, 82.89999999999999, 82.89999999999999, 83.1, 82.95, 83.05, 83.1, 82.95, 83.05, 82.89999999999999, 82.8, 82.69999999999999, 82.8, 82.8, 82.69999999999999, 82.6, 82.5, 82.55, 82.65, 82.8, 82.8, 82.75, 82.69999999999999, 82.6, 82.6, 82.69999999999999, 82.95, 83.0, 82.95, 82.39999999999999, 82.45, 82.6, 82.45, 82.65, 82.75, 82.6, 82.69999999999999, 82.8, 82.69999999999999, 82.75, 82.65, 82.75, 82.75, 82.85, 82.95, 82.8, 82.8, 82.5, 82.69999999999999, 82.8, 82.89999999999999, 82.75, 82.45, 82.39999999999999, 82.65, 82.55, 82.65, 82.75, 82.75, 82.69999999999999, 82.75, 82.69999999999999, 82.85, 83.0, 82.85, 82.89999999999999, 82.85, 82.85, 82.8, 82.89999999999999, 82.89999999999999, 82.75, 82.69999999999999, 82.6, 82.65, 82.6, 82.5, 82.55, 82.85, 83.0, 83.0, 83.05, 83.05, 83.1, 83.05, 82.95, 82.95, 82.89999999999999, 82.89999999999999, 82.55, 82.6], 'val_loss_list': [22.83136749267578, 22.597156524658203, 22.42595100402832, 21.902090072631836, 20.588438034057617, 19.1807918548584, 15.610065460205078, 9.250779151916504, 4.964923858642578, 4.865257263183594, 4.7274017333984375, 4.814449787139893, 4.682075500488281, 4.583676338195801, 4.613772392272949, 4.613635540008545, 4.699257850646973, 4.756994247436523, 5.006351470947266, 4.988670349121094, 4.905816555023193, 5.01792573928833, 5.082842826843262, 5.166276454925537, 5.101728439331055, 4.990714073181152, 5.102760314941406, 5.003914833068848, 4.890011787414551, 4.930026054382324, 4.917240619659424, 4.921072959899902, 4.831289291381836, 4.873205184936523, 4.866893768310547, 4.736227512359619, 4.73059606552124, 4.684779167175293, 4.719182968139648, 4.734464168548584, 4.811385154724121, 4.8373589515686035, 4.823470115661621, 4.84945011138916, 4.82938289642334, 4.769092082977295, 4.800542831420898, 4.71621561050415, 4.712424278259277, 4.64906644821167, 4.603823184967041, 4.6588826179504395, 4.610044002532959, 4.649251461029053, 4.6939377784729, 4.700010776519775, 4.703885078430176, 4.707720756530762, 4.755037307739258, 4.699941635131836, 4.671080112457275, 4.637099742889404, 4.677889823913574, 4.662898540496826, 4.624828338623047, 4.625072002410889, 4.702518939971924, 4.723019599914551, 4.696521759033203, 4.681804656982422, 4.701077461242676, 4.694561004638672, 4.691715717315674, 4.713779926300049, 4.682885646820068, 4.651062488555908, 4.67770528793335, 4.699298858642578, 4.683584213256836, 4.672750949859619, 4.642000675201416, 4.594783306121826, 4.644532680511475, 4.645883560180664, 4.741150856018066, 4.71875524520874, 4.683248519897461, 4.74542760848999, 4.584334373474121, 4.602145195007324, 4.674497604370117, 4.648582458496094, 4.6638312339782715, 4.670384883880615, 4.613724708557129, 4.631361484527588, 4.659281253814697, 4.675946235656738, 4.672921657562256, 4.689942359924316, 4.746633529663086, 4.783674716949463, 4.776391506195068, 4.753419399261475, 4.807834148406982, 4.768285751342773, 4.760220527648926, 4.74783182144165, 4.7465105056762695, 4.740683555603027, 4.74352502822876, 4.776575088500977, 4.709213733673096, 4.670637607574463, 4.700613498687744, 4.712356090545654, 4.706584930419922, 4.732937335968018, 4.784641265869141, 4.745295524597168, 4.792998313903809, 4.783621311187744, 4.758451461791992, 4.8655853271484375, 4.754371166229248, 4.695262908935547, 4.7046308517456055, 4.7329230308532715, 4.736916542053223, 4.713634490966797, 4.7142863273620605, 4.743815898895264, 4.776453495025635, 4.793126106262207, 4.798983573913574, 4.788702011108398, 4.834444522857666, 4.921028137207031, 4.894984245300293, 4.871810436248779, 4.773677825927734, 4.775649547576904, 4.773208141326904, 4.782661437988281, 4.836870193481445, 4.872364521026611, 4.888535499572754, 4.904865264892578, 4.729585647583008, 4.76241397857666, 4.827204704284668, 4.859139442443848, 4.890017509460449, 4.832911968231201, 4.8197550773620605, 4.77661657333374, 4.770374774932861, 4.824986457824707, 4.803311824798584, 4.830057621002197, 4.797204971313477, 4.8285136222839355, 4.894484996795654, 4.847292900085449, 4.80432653427124, 4.793770790100098, 4.737644195556641, 4.755838394165039, 4.789027690887451, 4.9688191413879395, 4.886034965515137, 4.7716898918151855, 4.824723243713379, 4.877583980560303, 4.968461036682129, 4.8098015785217285, 4.9148993492126465, 4.9100117683410645, 4.8356099128723145, 4.884085655212402, 4.835801124572754, 4.855644702911377, 4.872348308563232, 4.814624786376953, 4.801469802856445, 4.7809672355651855, 4.817431449890137, 4.801285266876221, 4.781216144561768, 4.7895355224609375, 4.7223076820373535, 4.710213661193848, 4.7396697998046875, 4.8178253173828125, 4.836652755737305, 4.856593608856201, 4.862436294555664, 4.877712726593018, 4.93472957611084, 4.882310390472412, 4.787510395050049, 4.783285617828369, 4.83137321472168, 4.8497772216796875, 4.798770427703857, 4.833764553070068, 4.820032119750977, 4.7882609367370605, 4.784987449645996, 4.7964091300964355, 4.819076061248779, 4.851619720458984, 4.83506965637207], 'test_auc': 0.5446245457331758, 'test_acc': 81.8, 'test_loss': 5.0931267738342285, 'best_val_auc': 0.563483035613976, 'best_val_acc': 83.45, 'best_val_loss': 4.583676338195801})\n",
            "('0.5', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184], 'train_loss_list': [2.8949689865112305, 4.6281962394714355, 4.610926628112793, 4.731812477111816, 4.438232898712158, 4.645465850830078, 4.9476799964904785, 4.429597854614258, 4.645465850830078, 4.818159580230713, 4.507310390472412, 4.602292060852051, 4.4900407791137695, 4.835428237915039, 4.576387882232666, 4.610926628112793, 4.740447521209717, 4.8440632820129395, 4.749081611633301, 4.515944957733154, 4.507310390472412, 4.472771644592285, 4.455502033233643, 4.515944957733154, 4.464136600494385, 4.29144287109375, 4.6281962394714355, 4.49867582321167, 4.8095245361328125, 4.541849136352539, 4.533214569091797, 4.576387405395508, 4.576387882232666, 4.559118270874023, 4.973584175109863, 4.429598331451416, 4.403693675994873, 4.550483703613281, 4.576387405395508, 4.714542865753174, 4.654099941253662, 4.783620834350586, 4.766351222991943, 4.680004119873047, 4.472771167755127, 4.360520839691162, 4.576387882232666, 4.507310390472412, 4.533214569091797, 4.308712482452393, 4.377790451049805, 4.515944957733154, 5.016757011413574, 4.498675346374512, 4.6972737312316895, 4.541849136352539, 4.636830806732178, 4.429598331451416, 4.481406211853027, 4.481406211853027, 4.80088996887207, 4.636830806732178, 4.507310390472412, 4.688639163970947, 4.420963287353516, 4.4036946296691895, 4.757716178894043, 4.80088996887207, 4.680004119873047, 4.4900407791137695, 4.671369552612305, 4.403693675994873, 4.395059108734131, 4.438232898712158, 4.464137077331543, 4.697273254394531, 4.498675346374512, 4.6281962394714355, 4.438232898712158, 4.680004596710205, 5.016757488250732, 4.455502510070801, 4.464136600494385, 4.861332893371582, 4.680004119873047, 4.80088996887207, 4.7404465675354, 4.792255401611328, 4.740447521209717, 4.498675346374512, 4.498675346374512, 4.334616184234619, 4.291443347930908, 4.757716655731201, 4.541849136352539, 4.325982093811035, 4.360520839691162, 4.464137077331543, 4.861332893371582, 4.619561672210693, 4.291443347930908, 4.731812000274658, 4.49867582321167, 4.749082088470459, 4.645465850830078, 4.731812477111816, 4.999488353729248, 4.369155406951904, 4.654099941253662, 4.6627349853515625, 4.723177433013916, 4.65410041809082, 4.541849136352539, 4.351885795593262, 4.9217753410339355, 4.913141250610352, 4.585022449493408, 4.59365701675415, 4.464137077331543, 4.585022449493408, 4.498675346374512, 4.939044952392578, 4.481406211853027, 5.051296234130859, 4.481406211853027, 4.524580001831055, 4.610926151275635, 4.515944957733154, 4.714542865753174, 4.59365701675415, 4.4468674659729, 5.042661190032959, 4.256904125213623, 4.680003643035889, 4.49867582321167, 4.576387882232666, 4.964949131011963, 4.412328720092773, 4.723177909851074, 4.930410385131836, 4.783620357513428, 4.688638687133789, 4.602292060852051, 4.895871162414551, 4.705908298492432, 4.688638687133789, 4.541849136352539, 4.9563140869140625, 4.49867582321167, 4.524580001831055, 4.602292060852051, 4.654099941253662, 4.6627349853515625, 4.524580001831055, 4.766351222991943, 4.610926628112793, 4.334616661071777, 4.533214092254639, 4.585022449493408, 4.6627349853515625, 4.524580001831055, 4.80088996887207, 4.65410041809082, 4.619561672210693, 4.714542865753174, 4.610927104949951, 4.576387405395508, 4.559118270874023, 4.559118747711182, 4.369155406951904, 4.8440632820129395, 4.636830806732178, 4.351885795593262, 4.766351222991943, 4.636831283569336, 4.403694152832031, 4.818159580230713, 4.749082088470459, 4.4900407791137695, 4.731812477111816, 4.533214569091797, 4.550483703613281, 4.481406211853027, 4.576387405395508, 4.783620834350586], 'val_auc_list': [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], 'val_acc_list': [83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95, 83.95], 'val_loss_list': [4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778213500977, 4.434778213500977, 4.434778690338135, 4.434778213500977, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778213500977, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778213500977, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434779167175293, 4.434778213500977, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778213500977, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778213500977, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778213500977, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778213500977, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434779167175293, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434779167175293, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135, 4.434778690338135], 'test_auc': 0.5, 'test_acc': 82.89999999999999, 'test_loss': 4.724905014038086, 'best_val_auc': 0.5, 'best_val_acc': 83.95, 'best_val_loss': 4.434778213500977})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCoWiLv8fHFJOiQeaSHQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "('1e-06', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181], 'train_loss_list': [1.3790960311889648, 1.0789425373077393, 0.8449634313583374, 0.6981685161590576, 0.5717640519142151, 0.4464212656021118, 0.30410513281822205, 0.22057129442691803, 0.1279383897781372, 0.09107081592082977, 0.0653827115893364, 0.05135875195264816, 0.041431594640016556, 0.03355333209037781, 0.03173272684216499, 0.022161614149808884, 0.02264183759689331, 0.022333631291985512, 0.019099567085504532, 0.01699657365679741, 0.014971847645938396, 0.014035518281161785, 0.01377565786242485, 0.013232222758233547, 0.013253754004836082, 0.010590380057692528, 0.013414101675152779, 0.011346643790602684, 0.011476465500891209, 0.01144404523074627, 0.009445134550333023, 0.009275995194911957, 0.010644104331731796, 0.007793635595589876, 0.007181733846664429, 0.00746116740629077, 0.006928578019142151, 0.006829515099525452, 0.007443828042596579, 0.005011860281229019, 0.005732121877372265, 0.00818693544715643, 0.007266143336892128, 0.006629812531173229, 0.006230092141777277, 0.006104543339461088, 0.0063897306099534035, 0.004783264361321926, 0.006134803872555494, 0.006319401320070028, 0.005238729063421488, 0.004449944477528334, 0.006313806399703026, 0.005739576183259487, 0.004431534558534622, 0.004972013644874096, 0.0057253181003034115, 0.004489092621952295, 0.004848300013691187, 0.005769007373601198, 0.006360684055835009, 0.004249716643244028, 0.004919909406453371, 0.005103510804474354, 0.004793110769242048, 0.004785363562405109, 0.004560478497296572, 0.0037262081168591976, 0.00439715851098299, 0.0036822212859988213, 0.005531506612896919, 0.004418693482875824, 0.004859188571572304, 0.0044064875692129135, 0.0036508559715002775, 0.003768038237467408, 0.004465674981474876, 0.004259605426341295, 0.0035749312955886126, 0.004013586789369583, 0.003646355587989092, 0.0034865406341850758, 0.003938290756195784, 0.0033656256273388863, 0.00372795551083982, 0.003017255337908864, 0.005182572174817324, 0.004120035097002983, 0.004302430432289839, 0.0032622450962662697, 0.004828788805752993, 0.0029709807131439447, 0.0037241834215819836, 0.004625995643436909, 0.0025772610679268837, 0.006427062675356865, 0.004454914480447769, 0.0033879843540489674, 0.004065775312483311, 0.0035876152105629444, 0.004355318378657103, 0.004097689408808947, 0.002675557741895318, 0.0032658663112670183, 0.002899537794291973, 0.0036512950900942087, 0.004078928846865892, 0.0029966013971716166, 0.002875323873013258, 0.003909436985850334, 0.004733594134449959, 0.0027615728322416544, 0.0022218755912035704, 0.004185196943581104, 0.0026903385296463966, 0.004304420668631792, 0.003508907277137041, 0.004036613740026951, 0.004796178545802832, 0.002406264655292034, 0.0030162078328430653, 0.0026604956947267056, 0.004045228008180857, 0.002237530890852213, 0.004453920293599367, 0.0030506106559187174, 0.003339242422953248, 0.003525546519085765, 0.003767186077311635, 0.0029811272397637367, 0.003893475281074643, 0.003514103591442108, 0.0039057480171322823, 0.003203506814315915, 0.00442824000492692, 0.0035146954469382763, 0.0034198567736893892, 0.005586994346231222, 0.003629493061453104, 0.0036928104236721992, 0.003561000805348158, 0.0032208762131631374, 0.0030668023973703384, 0.003494871314615011, 0.004346265457570553, 0.0031166395638138056, 0.0018407483585178852, 0.0036805691197514534, 0.003517397679388523, 0.0037602437660098076, 0.002173854038119316, 0.0025189362931996584, 0.005489787086844444, 0.0017752381972968578, 0.0031318189576268196, 0.003004422876983881, 0.005007643718272448, 0.002702914411202073, 0.0022851917892694473, 0.002915110904723406, 0.003950404468923807, 0.003472845768555999, 0.0031853734981268644, 0.002700026147067547, 0.004080890212208033, 0.0032624087762087584, 0.0031359035056084394, 0.0031408355571329594, 0.003281280165538192, 0.0032400719355791807, 0.002914914395660162, 0.0030265559908002615, 0.0034675158094614744, 0.0035176652017980814, 0.0035145548172295094, 0.003007765393704176, 0.003178904764354229, 0.0023504632990807295, 0.003477713791653514, 0.0019045111257582903, 0.002815723419189453, 0.004320040345191956], 'val_auc_list': [0.4838062328296703, 0.5005777100503663, 0.5311838226877289, 0.5704394888965201, 0.5986900469322344, 0.6175076551053115, 0.6311097756410257, 0.6326640481913919, 0.6208183092948718, 0.6097148294413919, 0.6053077781593408, 0.6061841804029305, 0.6097237723214286, 0.6141755380036631, 0.6183822687728937, 0.6214872367216117, 0.6235709277701466, 0.6251466632326007, 0.6263020833333333, 0.6270193023122711, 0.6270944225045788, 0.6268350789835164, 0.6266866271749084, 0.6265399639423077, 0.6267635359432234, 0.6270908453525642, 0.6272947430173993, 0.627682864010989, 0.6282981341575091, 0.6289813701923077, 0.6300062242445055, 0.6310793698489011, 0.6323260073260073, 0.6335744333791209, 0.63491944253663, 0.636275183150183, 0.6376094608516484, 0.6389115441849818, 0.6402583419184982, 0.641606928228022, 0.6428768171932234, 0.6439839457417583, 0.644947988209707, 0.6459871508699634, 0.646885016025641, 0.647620120764652, 0.6484285571199635, 0.6494408911401098, 0.6504174536401099, 0.6515514108287546, 0.6525172418727107, 0.6534938043727107, 0.6543058178800366, 0.6549944196428572, 0.6556436727335164, 0.6563644688644689, 0.6571657509157509, 0.6579867073031134, 0.6588040865384615, 0.6594730139652014, 0.6601634043040292, 0.6608627375228937, 0.6616568652701464, 0.6622739239926742, 0.6628641540750916, 0.6634525955815018, 0.6641608716804029, 0.6648369534111721, 0.6652250744047619, 0.6655434409340659, 0.6658385559752747, 0.6660549736721612, 0.6664770776098901, 0.6668938158195971, 0.6674375429258242, 0.6679311899038461, 0.6685571915064104, 0.6693405877976191, 0.6701204069368133, 0.6709100632440476, 0.6715673649267399, 0.6720234518086081, 0.6723453954899267, 0.6725653903388277, 0.6730232657967032, 0.6734149639423077, 0.6737440619276556, 0.6739998282967032, 0.6744183550824177, 0.6749960651327839, 0.675632798191392, 0.676078153617216, 0.6762069310897436, 0.6760405935210623, 0.6759493761446886, 0.6758062900641025, 0.67593506753663, 0.6761836796016484, 0.6765252976190477, 0.6771512992216117, 0.6778899811126374, 0.6786465487637362, 0.6793494591346153, 0.6797787173763737, 0.6800702552655679, 0.6800845638736265, 0.6800362723214286, 0.6800595238095237, 0.6800738324175823, 0.6804011418269231, 0.6807159312042125, 0.6809895833333333, 0.6811666523580585, 0.6813294127747251, 0.6814671331272893, 0.681651356456044, 0.6818355797847986, 0.6820162259615384, 0.6823238610347986, 0.6827155591804029, 0.6830482343177656, 0.6834238352793041, 0.6838083791208791, 0.6838978079212454, 0.6839765052655677, 0.6844576322115385, 0.6849119305173992, 0.6853197258470697, 0.6855450864239927, 0.6855647607600733, 0.6854359832875457, 0.6854180975274725, 0.6853197258470697, 0.6852472885187728, 0.6852660685668499, 0.6855629721840659, 0.6857954870650184, 0.6857847556089743, 0.6857919099130036, 0.6858384128891941, 0.6861424708104396, 0.6865967691163003, 0.6869920444139195, 0.6872335021749084, 0.687176267742674, 0.6870331816620879, 0.6870331816620879, 0.6871100904304028, 0.686945541437729, 0.68701171875, 0.6870367588141026, 0.6871637477106227, 0.6873926854395604, 0.6880580357142856, 0.6887090773809522, 0.689261747367216, 0.6894119877518315, 0.6894567021520147, 0.6892599587912088, 0.6892206101190477, 0.6892635359432234, 0.6895103594322345, 0.689562228136447, 0.689678485576923, 0.6897947430173994, 0.6900397779304028, 0.6900022178342491, 0.6898591317536631, 0.6898627089056776, 0.6901488810668499, 0.69027586996337, 0.6904386303800366, 0.6904636704441391, 0.6905352134844323, 0.6906353737408425, 0.6907301682692308, 0.6909805689102564, 0.6914080385760074, 0.6916387648809523, 0.6918766454899268, 0.6919356684981685, 0.6917675423534798], 'val_acc_list': [62.35000000000001, 68.89999999999999, 77.14999999999999, 81.05, 79.80000000000001, 75.85, 73.45, 72.8, 73.1, 73.95, 75.35, 77.10000000000001, 78.3, 78.85, 79.05, 78.9, 78.85, 78.85, 78.85, 78.9, 78.60000000000001, 78.85, 78.75, 78.95, 78.9, 78.95, 79.25, 79.25, 79.25, 79.3, 79.5, 79.7, 79.95, 80.0, 80.10000000000001, 80.05, 80.05, 80.05, 80.15, 80.30000000000001, 80.35, 80.45, 80.55, 80.65, 80.7, 80.7, 80.65, 80.80000000000001, 80.85, 80.9, 81.0, 80.95, 81.10000000000001, 81.05, 81.10000000000001, 81.10000000000001, 81.10000000000001, 81.10000000000001, 81.15, 81.2, 81.15, 81.15, 81.15, 81.3, 81.3, 81.25, 81.25, 81.25, 81.35, 81.39999999999999, 81.45, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.45, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.35, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.5, 81.5, 81.5, 81.5, 81.5, 81.5, 81.5, 81.5, 81.5, 81.55, 81.65, 81.65, 81.5, 81.55, 81.55, 81.55, 81.55, 81.5, 81.55, 81.55, 81.55, 81.55, 81.55, 81.6, 81.6, 81.6, 81.6, 81.65, 81.65, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.65, 81.55, 81.55, 81.55, 81.55, 81.55, 81.55, 81.6, 81.65, 81.69999999999999, 81.8, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.8, 81.8, 81.75, 81.8, 81.69999999999999, 81.69999999999999, 81.8, 81.8, 81.8, 81.8, 81.85, 81.8, 81.8, 81.8, 81.85, 81.85, 81.89999999999999, 81.89999999999999, 81.89999999999999, 81.89999999999999, 82.0, 82.0, 82.05, 82.05, 82.0, 82.0, 82.0, 82.05, 82.05, 82.05, 82.15, 82.15, 82.15, 82.15, 82.25, 82.19999999999999, 82.15, 82.05, 82.0, 82.05, 82.0, 82.1, 82.1, 82.05, 82.05, 82.15, 82.19999999999999, 82.19999999999999], 'val_loss_list': [1.3955051898956299, 1.1188151836395264, 0.9009166359901428, 0.8131701946258545, 0.7231038808822632, 0.6793888807296753, 0.6581724286079407, 0.6249282956123352, 0.6290119886398315, 0.6518859267234802, 0.6685433983802795, 0.6757859587669373, 0.6778971552848816, 0.6779165267944336, 0.6768683791160583, 0.6765662431716919, 0.6779236197471619, 0.6806066036224365, 0.6836040616035461, 0.6868204474449158, 0.6899881958961487, 0.6929569244384766, 0.6956575512886047, 0.6977341175079346, 0.6991246938705444, 0.6998669505119324, 0.7005304098129272, 0.7012497782707214, 0.7023954391479492, 0.7038556933403015, 0.7053017020225525, 0.706341028213501, 0.707249104976654, 0.7080448865890503, 0.7088550329208374, 0.7097497582435608, 0.7105742692947388, 0.7114508748054504, 0.7126867771148682, 0.714518666267395, 0.71656334400177, 0.7187609672546387, 0.7209888696670532, 0.7227107882499695, 0.7239867448806763, 0.7255197763442993, 0.7269729971885681, 0.7282963395118713, 0.7291621565818787, 0.7299695611000061, 0.7312545776367188, 0.7328126430511475, 0.7344673275947571, 0.7364851236343384, 0.7385293245315552, 0.7403802275657654, 0.7416355609893799, 0.7426702380180359, 0.7433111071586609, 0.7432332038879395, 0.7431054711341858, 0.7436041235923767, 0.7446509599685669, 0.746266782283783, 0.7488129138946533, 0.7520067095756531, 0.7550899386405945, 0.7572821378707886, 0.7587372064590454, 0.759567141532898, 0.7595439553260803, 0.759445309638977, 0.7591161727905273, 0.7581855058670044, 0.7580822110176086, 0.7590094208717346, 0.7603412866592407, 0.7616868615150452, 0.7628963589668274, 0.7639772295951843, 0.764808714389801, 0.7653868198394775, 0.7669751644134521, 0.7691735625267029, 0.7707991003990173, 0.7713471055030823, 0.7708993554115295, 0.7707954049110413, 0.7710593938827515, 0.770781934261322, 0.771162211894989, 0.7726001739501953, 0.7750681042671204, 0.7785598039627075, 0.7821427583694458, 0.7843978404998779, 0.78411865234375, 0.7829800844192505, 0.7818557620048523, 0.7802870273590088, 0.7784541249275208, 0.7781499624252319, 0.7802260518074036, 0.7829076647758484, 0.7843917608261108, 0.7858003973960876, 0.7872726321220398, 0.7882264256477356, 0.7893956303596497, 0.7900927066802979, 0.7912176251411438, 0.7915992140769958, 0.791244387626648, 0.7917096614837646, 0.7937279343605042, 0.7952885031700134, 0.7952982783317566, 0.7939828634262085, 0.7916339039802551, 0.7896255850791931, 0.7889136672019958, 0.7900720238685608, 0.7938706278800964, 0.7993975281715393, 0.8041766881942749, 0.8072158098220825, 0.8084714412689209, 0.8072936534881592, 0.8046311140060425, 0.8020094633102417, 0.7999231219291687, 0.7983987927436829, 0.7988759279251099, 0.8008013367652893, 0.804097592830658, 0.8081377744674683, 0.8114376068115234, 0.8125153183937073, 0.8108090162277222, 0.8085194826126099, 0.8060762286186218, 0.8039664030075073, 0.8026171326637268, 0.8037245273590088, 0.8054715394973755, 0.80647212266922, 0.8082684278488159, 0.8108076453208923, 0.8131119012832642, 0.8147725462913513, 0.8156248927116394, 0.815874457359314, 0.8156493306159973, 0.8146954774856567, 0.8134158253669739, 0.8124697804450989, 0.8128254413604736, 0.8136809468269348, 0.8148811459541321, 0.8151716589927673, 0.8139699697494507, 0.8124024868011475, 0.8119107484817505, 0.8143380880355835, 0.819222629070282, 0.824665904045105, 0.8270623683929443, 0.8251829743385315, 0.8217896819114685, 0.820317804813385, 0.8196290135383606, 0.8179180026054382, 0.8161869049072266, 0.8158979415893555, 0.8176937103271484, 0.8208519816398621, 0.8224706649780273, 0.8231205344200134, 0.8239885568618774, 0.822898268699646, 0.8215966820716858, 0.8221129179000854], 'test_auc': 0.6943614236509759, 'test_acc': 83.1, 'test_loss': 0.7486572265625, 'best_val_auc': 0.6326640481913919, 'best_val_acc': 72.8, 'best_val_loss': 0.6249282956123352})\n",
            "('1e-05', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187], 'train_loss_list': [1.4257510900497437, 1.105708360671997, 0.764779806137085, 0.6857157349586487, 0.6130619049072266, 0.46451133489608765, 0.31730756163597107, 0.27589577436447144, 0.20946994423866272, 0.12749747931957245, 0.0860312283039093, 0.06869789212942123, 0.05240226164460182, 0.04923247918486595, 0.032646745443344116, 0.02988501824438572, 0.025182493031024933, 0.027302689850330353, 0.026935825124382973, 0.024334482848644257, 0.02530152164399624, 0.02338656224310398, 0.021095074713230133, 0.02265419065952301, 0.022609209641814232, 0.024755923077464104, 0.02231631986796856, 0.02026505023241043, 0.02039807289838791, 0.020066022872924805, 0.019199945032596588, 0.019455937668681145, 0.01721247285604477, 0.01874675042927265, 0.017952416092157364, 0.016849540174007416, 0.017831413075327873, 0.01729566790163517, 0.014876436442136765, 0.01822543516755104, 0.01394856907427311, 0.015773627907037735, 0.014776628464460373, 0.013123811222612858, 0.015795808285474777, 0.01454337127506733, 0.013067337684333324, 0.01389375887811184, 0.013509189710021019, 0.013471297919750214, 0.011807008646428585, 0.012226659804582596, 0.014561448246240616, 0.012924229726195335, 0.012906912714242935, 0.011768016032874584, 0.012351375073194504, 0.012128577567636967, 0.012697450816631317, 0.010357585735619068, 0.01200538408011198, 0.011683843098580837, 0.013185935094952583, 0.011750435456633568, 0.012057687155902386, 0.010182341560721397, 0.010178008116781712, 0.012008044868707657, 0.010252795182168484, 0.01044935267418623, 0.009917992167174816, 0.011156480759382248, 0.010137729346752167, 0.009856046177446842, 0.011273614130914211, 0.009685908444225788, 0.01051995251327753, 0.00979628600180149, 0.008985552005469799, 0.010157751850783825, 0.009239217266440392, 0.008750664070248604, 0.010115011595189571, 0.009182807989418507, 0.008966113440692425, 0.01074547041207552, 0.009969072416424751, 0.010657228529453278, 0.007963125593960285, 0.00983958225697279, 0.009058220311999321, 0.009589233435690403, 0.008011904545128345, 0.00929077435284853, 0.008725243620574474, 0.009884492494165897, 0.009782722219824791, 0.008978045545518398, 0.008771667256951332, 0.010101024061441422, 0.008500502444803715, 0.008630744181573391, 0.008837496861815453, 0.009084617719054222, 0.00844381470233202, 0.008179733529686928, 0.010526860132813454, 0.008923986926674843, 0.00900809932500124, 0.00877625122666359, 0.008005215786397457, 0.0072590941563248634, 0.008156674914062023, 0.009876524098217487, 0.009717651642858982, 0.008134657517075539, 0.008654091507196426, 0.00845312513411045, 0.007018466480076313, 0.00988121796399355, 0.0077203731052577496, 0.008039643056690693, 0.007385707926005125, 0.008721244521439075, 0.006474258378148079, 0.006881173700094223, 0.008867680095136166, 0.008050767704844475, 0.008461276069283485, 0.008606893941760063, 0.007609497290104628, 0.0079481927677989, 0.00792764127254486, 0.006505606230348349, 0.008609035983681679, 0.008765745908021927, 0.008027558214962482, 0.007936378009617329, 0.007944725453853607, 0.00857271533459425, 0.008244399912655354, 0.00868892390280962, 0.007530763745307922, 0.0073759895749390125, 0.008077047765254974, 0.009021513164043427, 0.008405416272580624, 0.007353328634053469, 0.010279146023094654, 0.007344541605561972, 0.007112476043403149, 0.00670351879671216, 0.007992067374289036, 0.007631240878254175, 0.008602573536336422, 0.007475706748664379, 0.008417454548180103, 0.007471947465091944, 0.007850498892366886, 0.006874254904687405, 0.006862661801278591, 0.007958843372762203, 0.005529910326004028, 0.007135533262044191, 0.007808353751897812, 0.005720785818994045, 0.007965554483234882, 0.006847575772553682, 0.00635878462344408, 0.006825519725680351, 0.0070519037544727325, 0.008735278621315956, 0.005859022028744221, 0.00991331972181797, 0.007308831438422203, 0.006838357541710138, 0.00772316288203001, 0.008413901552557945, 0.006943422369658947, 0.007165791932493448, 0.006205657031387091, 0.007417670451104641, 0.006511711981147528, 0.0055792913772165775, 0.007385155186057091, 0.006686973385512829, 0.007143753115087748, 0.006496685557067394], 'val_auc_list': [0.49456884560716197, 0.5119025687915238, 0.5455103189398305, 0.5873235318475052, 0.6241999462546415, 0.6502140518761017, 0.6619518353701923, 0.6610898815066955, 0.6541959407043345, 0.6484140893964464, 0.645645696399568, 0.6468997547656753, 0.6515120529881914, 0.6564234999045091, 0.6613586082994326, 0.6663866725031649, 0.6709533378740836, 0.6748236797317464, 0.678278255608193, 0.6809046797334364, 0.6821621183107732, 0.6831170279830786, 0.6842172867382483, 0.6853428970776384, 0.686454986572111, 0.6879642508860379, 0.6895546402694704, 0.6909726388802035, 0.6918565641166916, 0.6928148540002265, 0.6937275110321645, 0.6946807305988552, 0.6954531088647729, 0.6961578829061027, 0.6966902661747332, 0.6973460271532368, 0.6978598192601055, 0.6979713662306757, 0.6975556002494596, 0.6974322225395865, 0.697283493245493, 0.6972750427174194, 0.6973122250409428, 0.6969657533899294, 0.6970468784594349, 0.6973291260970899, 0.697682358170562, 0.6981724887988251, 0.69883416514698, 0.6994671096996852, 0.700210756170153, 0.7006721550029662, 0.7011774965817614, 0.7012434107007346, 0.701123413202091, 0.7009510224293916, 0.7008208842970597, 0.7005403267650195, 0.7005893398278459, 0.7008648270430419, 0.7009915849641445, 0.700891868732877, 0.7010355277101266, 0.7012772128130286, 0.7012941138691756, 0.7010135563371355, 0.7004744126460463, 0.6996614718453756, 0.6987217731236025, 0.6981758690100546, 0.6983524850467906, 0.6991121875205982, 0.6999809018065538, 0.7007380691219395, 0.70124172059512, 0.701478335381178, 0.7014952364373249, 0.7013211555590109, 0.7009729938023828, 0.7003763865203936, 0.6999775215953244, 0.7000265346581507, 0.7004051183158435, 0.7004904686493859, 0.7006299023625987, 0.7007262383826366, 0.700577509088543, 0.7007431394387835, 0.7005065246527256, 0.6998304824068455, 0.6989837394938809, 0.6984631869645535, 0.6986744501663908, 0.6991037369925246, 0.6998068209282399, 0.700570748666084, 0.7012620018624964, 0.701630444886501, 0.7012569315456523, 0.7003037119789616, 0.6992262696495904, 0.6986254371035645, 0.6983786816838184, 0.6983634707332862, 0.698628817314794, 0.6990074009724867, 0.6989651483321192, 0.6988992342131459, 0.6990074009724868, 0.6986710699551616, 0.6986017756249588, 0.6989921900219545, 0.6994738701221439, 0.699800060505781, 0.699901466842663, 0.699732456281193, 0.6992811980820681, 0.6987014918562261, 0.698324598304148, 0.6982840357693952, 0.6986085360474177, 0.6993961252638677, 0.6994299273761617, 0.6993352814617385, 0.6991645807946538, 0.6987133225955291, 0.6981978403830456, 0.6975403892989273, 0.6973375766251634, 0.6972429307107401, 0.6972818031398782, 0.697392505057641, 0.6975961627842124, 0.6983144576704599, 0.699000640550028, 0.6996209093106227, 0.6997358364924224, 0.6990581041409277, 0.6987048720674556, 0.6978429182039585, 0.6968254746239093, 0.6965060446627309, 0.6967342089207154, 0.6964790029728958, 0.69642660969884, 0.6967494198712478, 0.6967764615610831, 0.6967680110330094, 0.6970181466639851, 0.6971803968029961, 0.6973629282093837, 0.6975572903550742, 0.6973781391599161, 0.6972260296545932, 0.6970756102548847, 0.6968153339902211, 0.6963437945237199, 0.6959905624502475, 0.6957826794596395, 0.6957843695652541, 0.6960987292095884, 0.6962761902991318, 0.696069152361331, 0.6963742164247844, 0.6962626694542142, 0.6962964715665082, 0.6961105599488911, 0.695909437380742, 0.696392807586546, 0.6975674309887625, 0.6981471372146044, 0.6976958790154797, 0.6966192817389159, 0.6957463421889234, 0.6951404393260535, 0.6939404643396166, 0.6933117450509483, 0.6940215894091222, 0.6955088823500581, 0.6957632432450704, 0.6950677647846215, 0.6945252408823028, 0.693786664728679, 0.6932230145061764, 0.6932720275690029, 0.6941255309044263, 0.6956660621722253, 0.696609986158035], 'val_acc_list': [57.05, 66.95, 77.25, 80.7, 80.7, 79.55, 76.3, 72.35000000000001, 74.25, 78.14999999999999, 79.60000000000001, 80.35, 80.7, 80.55, 80.30000000000001, 80.4, 80.35, 80.4, 80.45, 80.80000000000001, 81.05, 81.05, 81.10000000000001, 81.05, 81.10000000000001, 81.15, 80.95, 80.95, 80.95, 81.0, 81.05, 80.95, 81.05, 81.2, 81.15, 81.15, 81.15, 81.15, 81.10000000000001, 81.10000000000001, 81.15, 81.15, 81.05, 81.10000000000001, 81.15, 81.15, 81.15, 81.2, 81.15, 81.39999999999999, 81.39999999999999, 81.39999999999999, 81.35, 81.3, 81.35, 81.35, 81.39999999999999, 81.45, 81.45, 81.6, 81.6, 81.55, 81.6, 81.35, 81.45, 81.35, 81.3, 81.35, 81.35, 81.39999999999999, 81.45, 81.35, 81.45, 81.39999999999999, 81.55, 81.39999999999999, 81.39999999999999, 81.55, 81.6, 81.55, 81.55, 81.69999999999999, 81.6, 81.75, 81.65, 81.65, 81.65, 81.6, 81.69999999999999, 81.6, 81.65, 81.75, 81.69999999999999, 81.55, 81.55, 81.55, 81.6, 81.5, 81.65, 81.69999999999999, 81.8, 81.8, 81.8, 81.8, 81.8, 81.75, 81.75, 81.65, 81.65, 81.75, 81.75, 81.69999999999999, 81.69999999999999, 81.8, 81.85, 81.8, 81.8, 81.65, 81.8, 81.6, 81.75, 81.8, 81.75, 81.65, 81.69999999999999, 81.75, 81.8, 81.75, 81.6, 81.5, 81.65, 81.75, 81.65, 81.65, 81.69999999999999, 81.69999999999999, 81.75, 81.65, 81.6, 81.55, 81.5, 81.5, 81.6, 81.65, 81.6, 81.55, 81.6, 81.55, 81.45, 81.5, 81.55, 81.65, 81.55, 81.6, 81.5, 81.6, 81.6, 81.6, 81.55, 81.55, 81.65, 81.65, 81.75, 81.69999999999999, 81.6, 81.6, 81.5, 81.39999999999999, 81.35, 81.55, 81.75, 81.8, 81.85, 81.69999999999999, 81.69999999999999, 81.69999999999999, 81.65, 81.55, 81.45, 81.5, 81.6, 81.55, 81.45, 81.6, 81.55, 81.5, 81.39999999999999, 81.45], 'val_loss_list': [1.2416163682937622, 0.9529305696487427, 0.8452709913253784, 0.8494787812232971, 0.759093165397644, 0.6445896029472351, 0.6071711182594299, 0.6229426264762878, 0.5942181348800659, 0.5688542127609253, 0.5760625600814819, 0.5941379070281982, 0.6049065589904785, 0.6060476303100586, 0.6012566685676575, 0.5957025289535522, 0.5914706587791443, 0.5894110202789307, 0.5898735523223877, 0.5935196876525879, 0.6000955700874329, 0.6082270741462708, 0.6158503293991089, 0.621674120426178, 0.6256056427955627, 0.6277982592582703, 0.6287181973457336, 0.6295062303543091, 0.6308638453483582, 0.6329772472381592, 0.6348497271537781, 0.6359134912490845, 0.6365929245948792, 0.6368465423583984, 0.6365982294082642, 0.6359460949897766, 0.6354730129241943, 0.6357095837593079, 0.6367947459220886, 0.6382364630699158, 0.6398195624351501, 0.6411226987838745, 0.6416563987731934, 0.6415823698043823, 0.6407599449157715, 0.6394113898277283, 0.6390228867530823, 0.6399105787277222, 0.6413783431053162, 0.6425240635871887, 0.6434518098831177, 0.6440227031707764, 0.6441307663917542, 0.6435341835021973, 0.6425297260284424, 0.6414729952812195, 0.640598475933075, 0.6411711573600769, 0.6427550315856934, 0.6446141600608826, 0.6467943787574768, 0.6486993432044983, 0.6501373052597046, 0.6511255502700806, 0.6506536602973938, 0.6497392058372498, 0.649473249912262, 0.6503809690475464, 0.6524842977523804, 0.6542004346847534, 0.6555063724517822, 0.6564037203788757, 0.6565531492233276, 0.6563037037849426, 0.6554805040359497, 0.6542852520942688, 0.6540647149085999, 0.6554409861564636, 0.6573712825775146, 0.6592492461204529, 0.6604828834533691, 0.6611334681510925, 0.6616300940513611, 0.6614819169044495, 0.6608075499534607, 0.6605621576309204, 0.6610328555107117, 0.6620126962661743, 0.6633329391479492, 0.6646318435668945, 0.6654260158538818, 0.6658003926277161, 0.6657423973083496, 0.6655265688896179, 0.6658132076263428, 0.666305422782898, 0.6669459342956543, 0.6672652363777161, 0.6686049103736877, 0.6698919534683228, 0.6708295345306396, 0.6714314222335815, 0.6716010570526123, 0.6714521646499634, 0.6713268160820007, 0.671380877494812, 0.671612560749054, 0.6716045141220093, 0.6724272966384888, 0.6741923093795776, 0.6755245327949524, 0.6770294308662415, 0.6777969002723694, 0.6775477528572083, 0.676348090171814, 0.6755473613739014, 0.6752225160598755, 0.6752543449401855, 0.6752168536186218, 0.6757030487060547, 0.6775059103965759, 0.6800033450126648, 0.6824986338615417, 0.68365079164505, 0.6840035319328308, 0.6839277744293213, 0.6829593777656555, 0.6812089085578918, 0.6808287501335144, 0.6825433969497681, 0.6845187544822693, 0.684897243976593, 0.6847671270370483, 0.6843612194061279, 0.6838258504867554, 0.6836618185043335, 0.6839900016784668, 0.6848587989807129, 0.6860320568084717, 0.6862092614173889, 0.6863237023353577, 0.6881729364395142, 0.6909030079841614, 0.6929200291633606, 0.6929996609687805, 0.6927120089530945, 0.6929336190223694, 0.6914790868759155, 0.6896584630012512, 0.6885761618614197, 0.6877582669258118, 0.6878007650375366, 0.6900324821472168, 0.6927710771560669, 0.6941239237785339, 0.6944391131401062, 0.6936752796173096, 0.6927821040153503, 0.6918443441390991, 0.6925286054611206, 0.6942457556724548, 0.6962652802467346, 0.6978834867477417, 0.6996376514434814, 0.7009149789810181, 0.6989134550094604, 0.6948402523994446, 0.691893458366394, 0.6931966543197632, 0.6973998546600342, 0.701352059841156, 0.7033345699310303, 0.7033435106277466, 0.7028087973594666, 0.7034523487091064, 0.7052307724952698, 0.7063630819320679, 0.7041845917701721, 0.7008398771286011, 0.6985820531845093, 0.6998552083969116, 0.7029005289077759, 0.7070226073265076, 0.7109626531600952, 0.7130693197250366, 0.711529552936554, 0.7057065367698669, 0.7007165551185608], 'test_auc': 0.6976248945796623, 'test_acc': 81.55, 'test_loss': 0.6928102970123291, 'best_val_auc': 0.6484140893964464, 'best_val_acc': 78.14999999999999, 'best_val_loss': 0.5688542127609253})\n",
            "('0.0001', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211], 'train_loss_list': [1.8901957273483276, 1.4526393413543701, 1.0728319883346558, 0.8129461407661438, 0.6515805125236511, 0.6478962898254395, 0.6373637914657593, 0.4817233681678772, 0.364006906747818, 0.29147377610206604, 0.25895029306411743, 0.22372226417064667, 0.16990652680397034, 0.13623511791229248, 0.12790845334529877, 0.11684038490056992, 0.09860993176698685, 0.09328713268041611, 0.09071514755487442, 0.08542915433645248, 0.08879218995571136, 0.0863700658082962, 0.08227216452360153, 0.08532016724348068, 0.07500097900629044, 0.07876300066709518, 0.07249724119901657, 0.07085651159286499, 0.06890743970870972, 0.06073211506009102, 0.06315720826387405, 0.0619351752102375, 0.06077122688293457, 0.0592220276594162, 0.05999525636434555, 0.05950489640235901, 0.060332585126161575, 0.05825303867459297, 0.055527959018945694, 0.05427704006433487, 0.05493104085326195, 0.05144103989005089, 0.05404988303780556, 0.051426518708467484, 0.05038198456168175, 0.050839390605688095, 0.04754161462187767, 0.04949091002345085, 0.047467660158872604, 0.045711297541856766, 0.047044314444065094, 0.04494418948888779, 0.04861726239323616, 0.04582829400897026, 0.047808244824409485, 0.045211516320705414, 0.04538438469171524, 0.04588612914085388, 0.04406815394759178, 0.041503287851810455, 0.045968666672706604, 0.042382076382637024, 0.03986923769116402, 0.04076540097594261, 0.04204808548092842, 0.04275071620941162, 0.041306547820568085, 0.042288731783628464, 0.0397627055644989, 0.04353731498122215, 0.040119096636772156, 0.040748100727796555, 0.043155211955308914, 0.038236722350120544, 0.04157800227403641, 0.0399588868021965, 0.039567410945892334, 0.03953667730093002, 0.03837147355079651, 0.03852693736553192, 0.03903478384017944, 0.03672831505537033, 0.03829634189605713, 0.038687918335199356, 0.03592687100172043, 0.038514189422130585, 0.03541853278875351, 0.03650541231036186, 0.035850994288921356, 0.03614484891295433, 0.0354170985519886, 0.03968265652656555, 0.03633086755871773, 0.03333934023976326, 0.03758647292852402, 0.036317337304353714, 0.03414560854434967, 0.0351218543946743, 0.03538530692458153, 0.035210054367780685, 0.03285053372383118, 0.034794025123119354, 0.03408027067780495, 0.03379330784082413, 0.03509921208024025, 0.03302798420190811, 0.03333750367164612, 0.03486236184835434, 0.03385887295007706, 0.03410424292087555, 0.03292081505060196, 0.0357530452311039, 0.035910800099372864, 0.03256576880812645, 0.037187717854976654, 0.03506110608577728, 0.03364725410938263, 0.0349070280790329, 0.03340640664100647, 0.03381061181426048, 0.032563529908657074, 0.03261661157011986, 0.03229363635182381, 0.033401623368263245, 0.03257305547595024, 0.0329635813832283, 0.03140832111239433, 0.03278861194849014, 0.03235114738345146, 0.03345080465078354, 0.03335142880678177, 0.031055422499775887, 0.03076961264014244, 0.033866025507450104, 0.036090169101953506, 0.03301483765244484, 0.029469475150108337, 0.030360905453562737, 0.030091645196080208, 0.029984455555677414, 0.032409314066171646, 0.03255220502614975, 0.029817277565598488, 0.0330149345099926, 0.02849321812391281, 0.02918710745871067, 0.03100861981511116, 0.031140517443418503, 0.02838466688990593, 0.03125833347439766, 0.03058837167918682, 0.0314408540725708, 0.03172312304377556, 0.030064163729548454, 0.02977735549211502, 0.029605863615870476, 0.03423209488391876, 0.03280295431613922, 0.03286799043416977, 0.030795471742749214, 0.032390642911195755, 0.029475558549165726, 0.030202124267816544, 0.03372673690319061, 0.029850361868739128, 0.03438166528940201, 0.029713701456785202, 0.029117174446582794, 0.02875571697950363, 0.029389936476945877, 0.029707442969083786, 0.027892909944057465, 0.029066743329167366, 0.027952400967478752, 0.029613027349114418, 0.03036598488688469, 0.028789162635803223, 0.028898656368255615, 0.028766274452209473, 0.029597368091344833, 0.02949131466448307, 0.029992325231432915, 0.029128609225153923, 0.02761002816259861, 0.03028179705142975, 0.030753137543797493, 0.029556291177868843, 0.027615277096629143, 0.030071794986724854, 0.027339838445186615, 0.028308836743235588, 0.026024622842669487, 0.02947111427783966, 0.029843175783753395, 0.029364101588726044, 0.02995411865413189, 0.030179524794220924, 0.02709299325942993, 0.028444020077586174, 0.029188284650444984, 0.03157345578074455, 0.029086165130138397, 0.02643449790775776, 0.029149489477276802, 0.02621845155954361, 0.029883675277233124, 0.028660329058766365, 0.027733510360121727, 0.029114937409758568, 0.029126804322004318, 0.02565315179526806, 0.028581008315086365], 'val_auc_list': [0.4962439686804774, 0.5179531073936032, 0.5611294097252318, 0.6155846407211579, 0.6430939585680853, 0.6510899713963767, 0.6600809713674843, 0.6699441651498078, 0.6733950189246193, 0.6710601398399352, 0.6697094131923378, 0.6750256421368929, 0.6834586547629367, 0.689562205657161, 0.694168761376441, 0.6979013175002167, 0.7009296177515818, 0.7030369680159486, 0.704393112016411, 0.7043317153506111, 0.7032500505619601, 0.7022875675363323, 0.7018054231313744, 0.7021268527346798, 0.7026234434139436, 0.7021665799890209, 0.700891696281529, 0.7000790933518246, 0.6996998786512959, 0.6997305769841958, 0.700619022854006, 0.7017656958770333, 0.7020907370489151, 0.7015706711739043, 0.7006677790297884, 0.6999400479616307, 0.7003030106035654, 0.7013323076478576, 0.703226575366213, 0.703833318887059, 0.703499248793736, 0.7028997284100431, 0.7022008898904972, 0.7016248447025513, 0.7017909768570686, 0.7021864436161913, 0.7018686255814627, 0.7017259686226923, 0.7022243650862443, 0.7035353644795007, 0.7039795874144059, 0.7034793851665656, 0.7030297448787958, 0.703062248995984, 0.7027697119412902, 0.7034486868336655, 0.704487012799399, 0.7059316402299847, 0.707443081679235, 0.7068706480598654, 0.7056535494495969, 0.7043407442720522, 0.70331144722776, 0.7027335962555257, 0.7029466788015372, 0.7050341654387333, 0.7079450897113635, 0.7100054895842363, 0.7099765970356245, 0.7084001473519979, 0.7067225737482303, 0.7050720869087862, 0.7036689925168299, 0.7027913813527492, 0.70325005056196, 0.7037683106526826, 0.7047885787755339, 0.7070367802143828, 0.7096298864522839, 0.7104984686949236, 0.7086818497009622, 0.7057348097425673, 0.7049781861257982, 0.7061790326774724, 0.7083477796076392, 0.7090412007743203, 0.7091170437144261, 0.7070367802143827, 0.70521113229898, 0.7049131778914219, 0.7052761405333564, 0.7048246944612983, 0.7051695992603508, 0.7057420328797204, 0.7059984542486493, 0.7059406691514258, 0.7042035046661467, 0.7030766952702898, 0.7039362485914883, 0.7055054751379619, 0.706020123660108, 0.7054603305307562, 0.7055163098436913, 0.7056138221952558, 0.706114024443096, 0.7063559995377193, 0.7055903469995088, 0.7057871774869261, 0.7063794747334662, 0.7057889832712144, 0.7045159053480108, 0.7039958394729999, 0.7042937938805582, 0.7044545086822109, 0.7048210828927219, 0.705740227095432, 0.7050793100459392, 0.702051009794574, 0.7008429401057468, 0.7029502903701135, 0.704245037704776, 0.7036527404582358, 0.7044490913293462, 0.7040698766288174, 0.7020528155788621, 0.6996150067897489, 0.6976503134841525, 0.6986759989598683, 0.7031037820346133, 0.7060056773858021, 0.7067749414925891, 0.7052499566611771, 0.7048770622056572, 0.7051533472017567, 0.7046080003467106, 0.7046386986796105, 0.7050955621045334, 0.7063903094391957, 0.7081527549045101, 0.7083730605876744, 0.7070259455086534, 0.7038152610441767, 0.6999481739909277, 0.6994642238016816, 0.7008826673600879, 0.7021828320476149, 0.7030649576724163, 0.7040256349137557, 0.7048662274999278, 0.7037854656034208, 0.7034974430094478, 0.7046757172575193, 0.704682037502528, 0.7041466224610673, 0.7026207347375111, 0.7011896506890873, 0.7009097541244113, 0.7010081693681199, 0.702167482881165, 0.7048581014706308, 0.706066171159458, 0.7068300179133802, 0.7071406128109561, 0.7066205469359452, 0.7054955433243766, 0.7051000765652539, 0.7047262792175899, 0.7040527216780792, 0.7051849484268007, 0.7061961876282106, 0.7062548756175783, 0.705894621652076, 0.7045041677501372, 0.7049194981364306, 0.7051686963682066, 0.7041971844211378, 0.7047028040218427, 0.7075649721186906, 0.7075740010401318, 0.7046143205917194, 0.7006145083932853, 0.6965406590390337, 0.6948739201409957, 0.6973496504001617, 0.7015426815174366, 0.7050440972523186, 0.7064471916442749, 0.704168291872526, 0.7008673181936378, 0.6998199633064632, 0.7004176779058681, 0.7008059215278379, 0.7025557265031348, 0.7039263167779031, 0.7028265941463696, 0.6992041908641761, 0.6952639695472538, 0.6949028126896074, 0.6965894152148161, 0.6982254557799543, 0.6998903888937044, 0.7019959333737829, 0.7032491476698159, 0.703973267169397, 0.7040545274623675, 0.701934536707983, 0.6997513435035105, 0.6997116162491693, 0.70106956603392, 0.7025051645430643, 0.7029638337522753, 0.7037890771719973], 'val_acc_list': [56.3, 63.05, 72.39999999999999, 79.45, 81.3, 78.2, 77.9, 80.95, 81.45, 81.05, 79.75, 79.75, 80.10000000000001, 80.95, 81.55, 82.0, 81.95, 81.89999999999999, 81.65, 81.69999999999999, 82.0, 82.1, 81.89999999999999, 81.89999999999999, 81.8, 81.85, 82.0, 81.75, 81.69999999999999, 81.85, 82.05, 82.1, 82.25, 82.3, 82.25, 82.45, 82.39999999999999, 82.35, 82.19999999999999, 82.15, 82.19999999999999, 82.3, 82.39999999999999, 82.3, 82.65, 82.39999999999999, 82.55, 82.6, 82.45, 82.35, 82.15, 82.25, 82.19999999999999, 82.25, 82.05, 82.1, 82.25, 82.45, 82.39999999999999, 82.35, 82.45, 82.35, 82.19999999999999, 82.19999999999999, 82.15, 82.05, 82.19999999999999, 82.45, 82.39999999999999, 82.5, 82.25, 81.89999999999999, 82.15, 81.95, 81.95, 81.89999999999999, 82.3, 82.25, 82.19999999999999, 82.35, 82.15, 82.25, 82.15, 82.05, 82.19999999999999, 82.25, 82.05, 82.15, 81.89999999999999, 82.05, 82.05, 82.1, 82.0, 82.1, 82.25, 82.0, 81.89999999999999, 81.85, 82.05, 82.19999999999999, 82.3, 82.1, 81.95, 81.89999999999999, 81.89999999999999, 82.15, 82.19999999999999, 82.15, 82.19999999999999, 82.3, 82.19999999999999, 82.05, 82.15, 82.1, 82.15, 81.89999999999999, 81.89999999999999, 82.1, 82.3, 82.0, 82.25, 82.1, 82.0, 82.1, 81.89999999999999, 82.05, 82.1, 82.15, 82.19999999999999, 82.1, 82.19999999999999, 82.25, 81.95, 82.25, 82.15, 82.19999999999999, 82.3, 82.1, 82.05, 82.25, 82.25, 82.15, 82.25, 82.1, 82.0, 82.0, 82.05, 81.69999999999999, 81.89999999999999, 82.15, 82.19999999999999, 82.19999999999999, 82.15, 82.15, 82.05, 82.19999999999999, 82.0, 81.95, 82.05, 82.0, 81.89999999999999, 82.1, 81.89999999999999, 81.95, 81.95, 81.85, 82.0, 81.95, 81.8, 81.85, 81.8, 82.19999999999999, 82.3, 82.0, 82.0, 82.1, 82.19999999999999, 82.0, 82.15, 82.15, 82.19999999999999, 82.0, 81.8, 81.89999999999999, 81.8, 81.85, 81.8, 82.25, 82.15, 82.3, 82.35, 82.25, 82.05, 82.1, 82.05, 82.1, 82.1, 82.19999999999999, 81.89999999999999, 81.89999999999999, 81.89999999999999, 82.05, 82.15, 82.3, 82.3, 81.75, 81.75, 81.89999999999999, 81.89999999999999, 82.05, 82.1, 81.95], 'val_loss_list': [1.607340931892395, 1.2366158962249756, 0.8962016701698303, 0.7360406517982483, 0.7162054777145386, 0.7495951056480408, 0.6942874789237976, 0.5975513458251953, 0.5528891682624817, 0.5268791317939758, 0.5134819746017456, 0.5046377182006836, 0.49068760871887207, 0.48592039942741394, 0.491556853055954, 0.4923761785030365, 0.4855254292488098, 0.4783608615398407, 0.4750921130180359, 0.4769475758075714, 0.4821913540363312, 0.4861904978752136, 0.4880983233451843, 0.4892587959766388, 0.49066096544265747, 0.49286556243896484, 0.4943162500858307, 0.49390241503715515, 0.49204474687576294, 0.4910487234592438, 0.49176275730133057, 0.49243590235710144, 0.4933215379714966, 0.49419426918029785, 0.4943372905254364, 0.495430052280426, 0.4967822730541229, 0.49761682748794556, 0.4964223802089691, 0.4958885908126831, 0.496956467628479, 0.498544305562973, 0.5001325011253357, 0.500090479850769, 0.5000672936439514, 0.5002606511116028, 0.5020658373832703, 0.5044698715209961, 0.5054592490196228, 0.5055517554283142, 0.5066472291946411, 0.5077610611915588, 0.50653076171875, 0.5029268264770508, 0.5023247003555298, 0.5054187178611755, 0.5084389448165894, 0.5047028064727783, 0.5013276934623718, 0.506364107131958, 0.513501763343811, 0.5144189596176147, 0.5138737559318542, 0.5160080194473267, 0.5179571509361267, 0.5148047804832458, 0.5094459056854248, 0.5078303813934326, 0.5106046795845032, 0.5109620690345764, 0.5085307359695435, 0.5114366412162781, 0.5192520022392273, 0.5221338272094727, 0.5204196572303772, 0.5199523568153381, 0.5222657322883606, 0.5208868384361267, 0.5166627168655396, 0.5141971111297607, 0.5150570869445801, 0.5175358057022095, 0.5174626708030701, 0.5167333483695984, 0.517162024974823, 0.5186551809310913, 0.5199508666992188, 0.5224552154541016, 0.5239799618721008, 0.52485591173172, 0.5260775685310364, 0.5242526531219482, 0.522599458694458, 0.5255748629570007, 0.5290253162384033, 0.5276025533676147, 0.5247313976287842, 0.5250824689865112, 0.528202474117279, 0.5305975675582886, 0.5307188034057617, 0.5319718718528748, 0.5331541895866394, 0.5334447026252747, 0.5332838296890259, 0.5364251136779785, 0.5355066061019897, 0.5283697247505188, 0.5281732678413391, 0.5359557271003723, 0.5383928418159485, 0.5339561104774475, 0.5322405099868774, 0.538186252117157, 0.540608823299408, 0.5353906750679016, 0.5358149409294128, 0.5424513220787048, 0.5408691167831421, 0.5335263013839722, 0.5367617011070251, 0.5452014803886414, 0.5419321060180664, 0.5389834642410278, 0.5433366894721985, 0.5463266968727112, 0.5488417744636536, 0.5501284599304199, 0.544198751449585, 0.5390920639038086, 0.5383740067481995, 0.5384799838066101, 0.5400320291519165, 0.544134259223938, 0.5451551675796509, 0.5402617454528809, 0.5397455096244812, 0.5424515008926392, 0.5401661396026611, 0.5372692942619324, 0.5375810861587524, 0.5429757833480835, 0.5537533164024353, 0.5545145869255066, 0.5488856434822083, 0.5517445802688599, 0.5547671914100647, 0.5492954254150391, 0.548881471157074, 0.5522316098213196, 0.5482619404792786, 0.5433633327484131, 0.5478708148002625, 0.5554103851318359, 0.5548382997512817, 0.5500432848930359, 0.5526741743087769, 0.559669017791748, 0.5524267554283142, 0.5494074821472168, 0.55643230676651, 0.5545003414154053, 0.5475538969039917, 0.5458003878593445, 0.5522850751876831, 0.5521845817565918, 0.5491871237754822, 0.5578480958938599, 0.5602055788040161, 0.5496160984039307, 0.5485407710075378, 0.5603495836257935, 0.5622799396514893, 0.5530039668083191, 0.5536642074584961, 0.5571827292442322, 0.5527524352073669, 0.5507296919822693, 0.5540024042129517, 0.558708667755127, 0.5599873065948486, 0.5593866109848022, 0.5663245916366577, 0.5701934695243835, 0.562755286693573, 0.5519680380821228, 0.5510885715484619, 0.5580453872680664, 0.5601246953010559, 0.5576074719429016, 0.5600001215934753, 0.5602574348449707, 0.5542646646499634, 0.553796648979187, 0.5615819692611694, 0.5638769865036011, 0.5602493286132812, 0.5658891201019287, 0.5727358460426331, 0.5663766860961914, 0.5586071610450745, 0.5588648915290833, 0.5613469481468201, 0.5607572197914124, 0.5597428679466248, 0.561409056186676, 0.5673527121543884, 0.5699776411056519, 0.5669312477111816, 0.5647489428520203, 0.5639292597770691, 0.5614693760871887], 'test_auc': 0.6893832376578646, 'test_acc': 82.89999999999999, 'test_loss': 0.5585106015205383, 'best_val_auc': 0.704393112016411, 'best_val_acc': 81.65, 'best_val_loss': 0.4750921130180359})\n",
            "('0.001', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159], 'train_loss_list': [2.740952730178833, 2.147446870803833, 1.501986026763916, 0.9574285745620728, 0.9043257236480713, 0.9092754125595093, 0.8383274674415588, 0.6847752332687378, 0.6687765717506409, 0.6031496524810791, 0.5613224506378174, 0.5134382247924805, 0.503880500793457, 0.4868796169757843, 0.46753719449043274, 0.418565034866333, 0.4280773401260376, 0.38011276721954346, 0.37801969051361084, 0.3631768524646759, 0.3278540372848511, 0.3579023778438568, 0.3491024374961853, 0.3310718238353729, 0.308829128742218, 0.2889457941055298, 0.28586283326148987, 0.2863149642944336, 0.2702668309211731, 0.2687835693359375, 0.25320613384246826, 0.2531391680240631, 0.2524406313896179, 0.24802365899085999, 0.2341383993625641, 0.2378782480955124, 0.2541230320930481, 0.22794252634048462, 0.23391465842723846, 0.22155748307704926, 0.21168115735054016, 0.21842266619205475, 0.21804757416248322, 0.20942403376102448, 0.20513063669204712, 0.2075682133436203, 0.20671093463897705, 0.20043113827705383, 0.19572784006595612, 0.21192167699337006, 0.18863631784915924, 0.19501647353172302, 0.18365947902202606, 0.19315575063228607, 0.19534559547901154, 0.18914707005023956, 0.1883639544248581, 0.1929609477519989, 0.18982180953025818, 0.18990057706832886, 0.19092054665088654, 0.183772012591362, 0.17970579862594604, 0.18268734216690063, 0.1898050308227539, 0.17830653488636017, 0.18103566765785217, 0.18371161818504333, 0.1774059236049652, 0.17817692458629608, 0.18072664737701416, 0.1756148338317871, 0.17572931945323944, 0.17185750603675842, 0.17007087171077728, 0.17288467288017273, 0.17770133912563324, 0.17388513684272766, 0.17950350046157837, 0.17539595067501068, 0.1743556559085846, 0.17578129470348358, 0.1728236973285675, 0.1705702841281891, 0.16475138068199158, 0.17965717613697052, 0.17704500257968903, 0.17194807529449463, 0.17561405897140503, 0.16604705154895782, 0.17441648244857788, 0.17473328113555908, 0.16786044836044312, 0.16674639284610748, 0.16444306075572968, 0.17366856336593628, 0.16175197064876556, 0.162861630320549, 0.16844013333320618, 0.17000870406627655, 0.16697034239768982, 0.16584137082099915, 0.16486848890781403, 0.16425655782222748, 0.1690071076154709, 0.1653112918138504, 0.16466298699378967, 0.16677992045879364, 0.16769470274448395, 0.1662154346704483, 0.16580821573734283, 0.15902800858020782, 0.17001622915267944, 0.16771860420703888, 0.17408621311187744, 0.1647178828716278, 0.16118261218070984, 0.16422849893569946, 0.16396592557430267, 0.16228464245796204, 0.16701771318912506, 0.1696382761001587, 0.16894198954105377, 0.16311731934547424, 0.15878885984420776, 0.1596292406320572, 0.16646508872509003, 0.16321884095668793, 0.15639890730381012, 0.1636129766702652, 0.15906083583831787, 0.16085191071033478, 0.16723082959651947, 0.16825711727142334, 0.15951913595199585, 0.16550104320049286, 0.16651326417922974, 0.16270124912261963, 0.1598542332649231, 0.15364831686019897, 0.15995053946971893, 0.15376314520835876, 0.16423237323760986, 0.1563805639743805, 0.16104251146316528, 0.16076987981796265, 0.16432419419288635, 0.16380904614925385, 0.15420445799827576, 0.15423265099525452, 0.1629294455051422, 0.1564842164516449, 0.16181007027626038, 0.15904347598552704, 0.16089023649692535, 0.15477293729782104, 0.16387450695037842, 0.16100814938545227, 0.16803310811519623, 0.15860021114349365], 'val_auc_list': [0.48097310536479676, 0.4840186519711742, 0.493331449295841, 0.5158438132918844, 0.5614883896189533, 0.60875323818944, 0.6361754038905374, 0.6476680325938486, 0.6495351137487636, 0.6403127502237294, 0.6196335547077387, 0.5982516132071029, 0.5869228957656257, 0.5896924308793745, 0.6051189298667043, 0.6266271018793275, 0.6431388064622486, 0.6517375535773161, 0.65539447034996, 0.6555960623616409, 0.6550327351514295, 0.6570467712307475, 0.6606735434035137, 0.6623333804342707, 0.6623220762093166, 0.6625688851208139, 0.662813809994819, 0.66442089397579, 0.6650539305732185, 0.6636352503414817, 0.6647430643869813, 0.6695530121049409, 0.6736847063256559, 0.6757816400546371, 0.6755291790306627, 0.6747359992463849, 0.6761358391031982, 0.6771437991616034, 0.6762714898026471, 0.6768913381376289, 0.6787132023927276, 0.6783326268192738, 0.6766746738260091, 0.6753803400687673, 0.6786849418303423, 0.6818538928924687, 0.6826696811266544, 0.6793349347652018, 0.6770514813244783, 0.6795459469643446, 0.6841524186331308, 0.6858725448636429, 0.684670528943526, 0.6824209881776647, 0.6804031840233621, 0.6813696952569357, 0.6834364843860393, 0.6848250200178984, 0.6834006876736847, 0.6797795676133955, 0.6776562573595215, 0.6794875418020818, 0.6793179784277708, 0.6781197305826386, 0.681840704630022, 0.6838773491592482, 0.6818162121426216, 0.6819292543921625, 0.6844369082944749, 0.6839206820215723, 0.6830219961377232, 0.6870839809712214, 0.6929094248975554, 0.695405774574914, 0.693184494371438, 0.6888417879515802, 0.6869878950591116, 0.6880241156799021, 0.6907936507936507, 0.6929508737223872, 0.6924158070745607, 0.6892412038999576, 0.6833667749988225, 0.6828392445009656, 0.6901511940087609, 0.6948914323395037, 0.6937572417691112, 0.6908520559559136, 0.6915679902030051, 0.6960651876972352, 0.6955301210494089, 0.6905788705195234, 0.6863209457868212, 0.687197023220762, 0.6915105270594886, 0.6952625877254957, 0.694898968489473, 0.6939927464556545, 0.6942075267297819, 0.6937195610192644, 0.6945410013659271, 0.6940417314304554, 0.6916188592152983, 0.6904752484574443, 0.6901926428335924, 0.6911723423296123, 0.6943544816541849, 0.6972483632424284, 0.6989345767980782, 0.6947652018275163, 0.6885554142527437, 0.6873722387075502, 0.6900193113842965, 0.6931920305214073, 0.6936799962319249, 0.6941133248551646, 0.6958051905232915, 0.6974160425792474, 0.6982148744760021, 0.697022278743347, 0.694397814516509, 0.6933276812208563, 0.6924026188121143, 0.6883801987659555, 0.6851471904290896, 0.6898817766473553, 0.6966605435448165, 0.6968564834440205, 0.6914615420846876, 0.6884800527530499, 0.6897178653855212, 0.6921011728133389, 0.693647967594555, 0.6945410013659271, 0.6959088125853705, 0.6963817059959493, 0.6959992463850031, 0.6943921624040317, 0.6925382695115633, 0.6918628420705571, 0.6948141868023174, 0.6977269087654844, 0.6989835617728792, 0.6997993500070652, 0.6994639913334275, 0.6994150063586265, 0.6979416890396118, 0.6943601337666621, 0.6897140973105365, 0.691775234327163, 0.6976006782534973, 0.6975074183976262, 0.695786350148368, 0.6933276812208563, 0.6948838961895342, 0.6970674956431632, 0.6961537374593755, 0.6924459516744383, 0.6890716405256466, 0.6919184211765814], 'val_acc_list': [38.0, 47.05, 61.0, 76.44999999999999, 82.55, 83.0, 81.6, 77.0, 72.6, 73.75, 76.8, 78.2, 79.85, 80.30000000000001, 80.25, 79.14999999999999, 78.5, 79.4, 80.80000000000001, 81.55, 81.6, 81.15, 80.25, 80.65, 81.3, 82.15, 82.25, 81.65, 81.55, 82.19999999999999, 82.5, 82.19999999999999, 81.5, 81.25, 82.05, 82.6, 82.35, 82.25, 82.65, 82.39999999999999, 82.39999999999999, 82.19999999999999, 82.45, 82.69999999999999, 82.75, 82.6, 82.69999999999999, 82.65, 82.15, 82.0, 82.15, 82.95, 83.0, 82.5, 82.6, 82.95, 82.95, 82.95, 82.55, 82.69999999999999, 82.95, 82.89999999999999, 82.75, 83.15, 82.65, 82.69999999999999, 83.15, 83.25, 83.1, 82.95, 83.15, 83.3, 83.6, 83.35000000000001, 83.05, 83.25, 83.15, 82.89999999999999, 83.1, 83.35000000000001, 83.0, 82.8, 83.35000000000001, 83.6, 83.25, 83.15, 83.3, 83.5, 83.5, 83.7, 83.3, 83.0, 83.15, 83.39999999999999, 83.15, 83.3, 83.3, 83.2, 82.95, 83.1, 82.89999999999999, 83.15, 83.25, 82.75, 83.0, 83.2, 83.45, 83.35000000000001, 83.3, 82.95, 82.95, 83.2, 83.25, 83.15, 83.25, 83.3, 83.3, 83.35000000000001, 83.25, 83.1, 83.25, 83.25, 83.2, 83.2, 83.15, 83.2, 83.05, 83.0, 83.3, 83.55, 83.6, 83.55, 83.39999999999999, 83.1, 82.75, 82.75, 83.25, 83.35000000000001, 83.1, 83.0, 83.3, 83.3, 83.35000000000001, 83.35000000000001, 83.3, 83.2, 83.15, 83.05, 83.15, 83.1, 83.25, 83.2, 83.35000000000001, 83.39999999999999, 83.55, 83.55, 83.5, 83.45, 83.39999999999999, 83.6], 'val_loss_list': [2.555079460144043, 1.8897308111190796, 1.2052539587020874, 0.8804247975349426, 0.8741535544395447, 0.8042194843292236, 0.6751366257667542, 0.6488828063011169, 0.6662260890007019, 0.627092182636261, 0.587875485420227, 0.5744909644126892, 0.5654560327529907, 0.5455083250999451, 0.5177054405212402, 0.49793317914009094, 0.4920862913131714, 0.48278993368148804, 0.4704117476940155, 0.4665815830230713, 0.4668632745742798, 0.46775344014167786, 0.4688708484172821, 0.46427083015441895, 0.4568720757961273, 0.4529293179512024, 0.45091769099235535, 0.45127740502357483, 0.4517122805118561, 0.4501914083957672, 0.4498528838157654, 0.44774770736694336, 0.44719934463500977, 0.445029616355896, 0.4411380887031555, 0.43956834077835083, 0.4384382665157318, 0.43804216384887695, 0.43724697828292847, 0.4381338357925415, 0.4387545585632324, 0.4387132525444031, 0.4375615119934082, 0.43563517928123474, 0.4323672652244568, 0.43083488941192627, 0.43095940351486206, 0.43308791518211365, 0.4351842701435089, 0.4351853132247925, 0.43166810274124146, 0.4301793873310089, 0.43041983246803284, 0.4320380389690399, 0.4325394034385681, 0.43199509382247925, 0.4303930103778839, 0.42973917722702026, 0.43171149492263794, 0.4341883063316345, 0.43365606665611267, 0.43116095662117004, 0.43051332235336304, 0.4314616918563843, 0.429475873708725, 0.42867058515548706, 0.4307258725166321, 0.431284099817276, 0.4282744824886322, 0.427584707736969, 0.4268418848514557, 0.424726665019989, 0.421507328748703, 0.4203373193740845, 0.42287299036979675, 0.4262726902961731, 0.4264028072357178, 0.42504629492759705, 0.42320516705513, 0.42196306586265564, 0.4211697280406952, 0.42254725098609924, 0.4261258542537689, 0.4271404445171356, 0.4227702021598816, 0.41989368200302124, 0.4199851453304291, 0.4208616316318512, 0.41924768686294556, 0.41748058795928955, 0.4195966422557831, 0.42415544390678406, 0.4258171021938324, 0.4232679307460785, 0.42014753818511963, 0.41797515749931335, 0.4190869629383087, 0.421505868434906, 0.4221195578575134, 0.4215320646762848, 0.4202100932598114, 0.420388400554657, 0.42198657989501953, 0.4227824807167053, 0.4216463565826416, 0.41984811425209045, 0.41804635524749756, 0.41634032130241394, 0.41538169980049133, 0.4185604453086853, 0.4222713112831116, 0.42272546887397766, 0.42134350538253784, 0.4200234115123749, 0.4199630916118622, 0.418708473443985, 0.4172073006629944, 0.41605547070503235, 0.4154720604419708, 0.4164334535598755, 0.4184034764766693, 0.41994553804397583, 0.4206022322177887, 0.4239748418331146, 0.42772284150123596, 0.424956738948822, 0.418515145778656, 0.4159472584724426, 0.41855213046073914, 0.41938164830207825, 0.41611194610595703, 0.4158358573913574, 0.4185435473918915, 0.41940048336982727, 0.4180454909801483, 0.4177358150482178, 0.4183545708656311, 0.41766926646232605, 0.4173200726509094, 0.4189921021461487, 0.42026659846305847, 0.41892310976982117, 0.4171452522277832, 0.4158087968826294, 0.41608354449272156, 0.41687750816345215, 0.41801267862319946, 0.42050763964653015, 0.4228891134262085, 0.4201706647872925, 0.4162263870239258, 0.4172693192958832, 0.4190884530544281, 0.4197721481323242, 0.4186416268348694, 0.4175167679786682, 0.4175131022930145, 0.4169749617576599, 0.41745686531066895, 0.41657689213752747], 'test_auc': 0.6984357491820179, 'test_acc': 83.15, 'test_loss': 0.4312090277671814, 'best_val_auc': 0.6989345767980782, 'best_val_acc': 83.3, 'best_val_loss': 0.41538169980049133})\n",
            "('0.01', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211], 'train_loss_list': [1.7749000787734985, 1.4846274852752686, 1.1672871112823486, 0.9765447378158569, 0.8423037528991699, 0.7462104558944702, 0.6972823143005371, 0.7116531133651733, 0.680428147315979, 0.6391727328300476, 0.6449302434921265, 0.6432449817657471, 0.6380132436752319, 0.5872014164924622, 0.6130794286727905, 0.5987711548805237, 0.5903337597846985, 0.5779889225959778, 0.5791438817977905, 0.5822481513023376, 0.5949102640151978, 0.5608237981796265, 0.5724120736122131, 0.5452739000320435, 0.5694195032119751, 0.5548723340034485, 0.5591615438461304, 0.5771386027336121, 0.5358832478523254, 0.5388039946556091, 0.5418816804885864, 0.5347884893417358, 0.5605957508087158, 0.5173497200012207, 0.5098786950111389, 0.4976062476634979, 0.5335400700569153, 0.5211012363433838, 0.502485454082489, 0.4943688213825226, 0.48774436116218567, 0.5082054138183594, 0.5037063360214233, 0.5085116028785706, 0.49684062600135803, 0.45748329162597656, 0.493623286485672, 0.4896297752857208, 0.48033368587493896, 0.4958190619945526, 0.4789159893989563, 0.4755867123603821, 0.4850923418998718, 0.4838031828403473, 0.47167590260505676, 0.48803645372390747, 0.4653584361076355, 0.47264155745506287, 0.4588675796985626, 0.47193434834480286, 0.46762359142303467, 0.47718167304992676, 0.463654488325119, 0.46412861347198486, 0.459966778755188, 0.46243706345558167, 0.45607781410217285, 0.45736178755760193, 0.4724046289920807, 0.46450915932655334, 0.4386201500892639, 0.45789989829063416, 0.4634784460067749, 0.4341185688972473, 0.438591867685318, 0.451698899269104, 0.46468254923820496, 0.43148308992385864, 0.4439796507358551, 0.45612794160842896, 0.44531768560409546, 0.44430994987487793, 0.45411890745162964, 0.4538835883140564, 0.4448506236076355, 0.4314117431640625, 0.4549792408943176, 0.42175063490867615, 0.4143928289413452, 0.4406430125236511, 0.4398256242275238, 0.42273303866386414, 0.4659165143966675, 0.4389965534210205, 0.4418250024318695, 0.4474799633026123, 0.4345477819442749, 0.4201470911502838, 0.43600234389305115, 0.4127301871776581, 0.42564526200294495, 0.4285230338573456, 0.4412970244884491, 0.4284023642539978, 0.437374085187912, 0.43036311864852905, 0.442124605178833, 0.43214285373687744, 0.44318339228630066, 0.43115484714508057, 0.4310372471809387, 0.4311498999595642, 0.4238395690917969, 0.4242715537548065, 0.43425601720809937, 0.41243064403533936, 0.42933139204978943, 0.4203089475631714, 0.4370662569999695, 0.4291181266307831, 0.42221805453300476, 0.438753217458725, 0.41538915038108826, 0.42037665843963623, 0.425721675157547, 0.4414604604244232, 0.4260145425796509, 0.41369324922561646, 0.4269505441188812, 0.4323580265045166, 0.4215756356716156, 0.4344857335090637, 0.43071693181991577, 0.42666420340538025, 0.4395620822906494, 0.4364693760871887, 0.43390214443206787, 0.4266441762447357, 0.41057705879211426, 0.4166485667228699, 0.41152071952819824, 0.43328219652175903, 0.43204596638679504, 0.4371265769004822, 0.4252624213695526, 0.4277036190032959, 0.40989041328430176, 0.4128962755203247, 0.4072912633419037, 0.41639453172683716, 0.4203214645385742, 0.41466957330703735, 0.4226090908050537, 0.414712518453598, 0.40702712535858154, 0.4305225610733032, 0.40081241726875305, 0.42160674929618835, 0.4198930263519287, 0.4163840413093567, 0.4147437810897827, 0.4221823513507843, 0.41736289858818054, 0.42326441407203674, 0.4193061590194702, 0.42802610993385315, 0.4069344997406006, 0.4426216185092926, 0.4230107069015503, 0.4212435185909271, 0.4298211634159088, 0.4390960931777954, 0.42431408166885376, 0.4259853661060333, 0.42583614587783813, 0.41833043098449707, 0.41985031962394714, 0.4109121561050415, 0.43764275312423706, 0.41136547923088074, 0.42287665605545044, 0.4256455898284912, 0.4132126271724701, 0.41117608547210693, 0.4078768789768219, 0.40581879019737244, 0.4210163950920105, 0.4217422902584076, 0.4068164825439453, 0.40630191564559937, 0.42147040367126465, 0.4227086901664734, 0.42627501487731934, 0.4164198637008667, 0.41900721192359924, 0.4102238416671753, 0.41307854652404785, 0.4144822359085083, 0.4206616282463074, 0.41578787565231323, 0.4181210398674011, 0.4216993749141693, 0.41909024119377136, 0.4105626046657562, 0.404291570186615, 0.42611488699913025, 0.4214997887611389, 0.41817203164100647, 0.4031415283679962, 0.4239177703857422, 0.4134589433670044, 0.40454715490341187], 'val_auc_list': [0.4924934248977207, 0.49764305196070246, 0.5106593192507862, 0.5359595474659765, 0.568248865881829, 0.594690170048148, 0.6096076507750968, 0.6156783139907045, 0.6140867217722857, 0.6077064373382316, 0.60150879463416, 0.5983256101973227, 0.5978490022543208, 0.5993014388689433, 0.6013505051348418, 0.6024846233057806, 0.6024324399543569, 0.6014931396287329, 0.6008565027413655, 0.6003381481172247, 0.6002650914252318, 0.601426170994406, 0.6028499067657453, 0.6038761793437422, 0.6050450864156298, 0.606641896969191, 0.6073324566530294, 0.6063200996354122, 0.6047963457738443, 0.6045771756978654, 0.6053964543152153, 0.6060635314909132, 0.607184603823996, 0.608784893267652, 0.6097111477554201, 0.6095919957696696, 0.6092788956611283, 0.6085831176421475, 0.6082317497425621, 0.6093362973476942, 0.6110305168239125, 0.6127786590966018, 0.6131578581169463, 0.612425551751969, 0.6118271826556456, 0.6119924299351535, 0.6128778074643066, 0.61393365060811, 0.6147755420110769, 0.6153286855361666, 0.616311471987977, 0.6169481088753443, 0.6174021040327293, 0.617574309092427, 0.6182666082213131, 0.6193850713868246, 0.6204530906459602, 0.6215941665970889, 0.6226517491859397, 0.6229352787286744, 0.6234466755726252, 0.6241598480420807, 0.6248243160502074, 0.6256435946675573, 0.6271516935236982, 0.6280996910745595, 0.6278231193120147, 0.6276404775820322, 0.6281501349809356, 0.628955498037906, 0.6303070468397762, 0.6319421251843812, 0.6332049622888314, 0.6341581781748351, 0.6353044724611061, 0.6361098355180763, 0.6365099078789903, 0.636409020066238, 0.6368908463443823, 0.6380858450919819, 0.6392738860593916, 0.6399192201719964, 0.6409872394311319, 0.6424588099412764, 0.643700773705157, 0.6442260861094876, 0.6446800812668727, 0.6450592802872172, 0.6453567253903314, 0.6457881077620996, 0.6459881439425565, 0.6462403634744371, 0.6465569424730734, 0.6475414683699313, 0.6485329520469789, 0.649348751774234, 0.6501541148312043, 0.6511195068325402, 0.6523145055801397, 0.65348167320698, 0.6545531713562105, 0.6555429155882108, 0.6565691881662075, 0.657306712866327, 0.6578233280454203, 0.6581486042692939, 0.6580503256241129, 0.6581346887089143, 0.6585747683059197, 0.6597662881634242, 0.6607751662909466, 0.6618588405555093, 0.663316495505274, 0.6645367162060616, 0.6653359912053658, 0.6657838983050849, 0.6659352300242131, 0.6660778645181042, 0.6659360997467368, 0.666510116612396, 0.667418106927166, 0.6687192118226601, 0.669782882469177, 0.6707178341821826, 0.671697141743898, 0.6728451754752165, 0.673836659152264, 0.6746698533299936, 0.6752708315938882, 0.6754038991400184, 0.675583061979906, 0.6760509726976706, 0.6765327989758148, 0.6768293743564053, 0.6773442500904511, 0.6778539074893546, 0.6784453188054883, 0.6789680220422476, 0.6793689641256854, 0.679779473156884, 0.6800769182599984, 0.680335225849545, 0.6804909061812918, 0.6806170159472322, 0.6806691992986558, 0.6809431618936295, 0.6811640714146558, 0.6814093331663467, 0.681893768612062, 0.6827234838996965, 0.6836192980991346, 0.684490760067908, 0.6852282847680277, 0.6859901616988117, 0.6868268347666361, 0.6873112702123514, 0.6877609167571178, 0.687969650162812, 0.6880801049233252, 0.6880470554674236, 0.6880722774206117, 0.6882749227686399, 0.6885193147978069, 0.6885227936879018, 0.6888237176811111, 0.6890350602543763, 0.6892272689321198, 0.689639517408366, 0.6902048370487879, 0.6907753750243523, 0.6912954690935404, 0.6917198936851188, 0.6921356210514598, 0.6922539033146865, 0.6924261083743842, 0.6923687066878184, 0.692233029974117, 0.6922504244245915, 0.6922817344354457, 0.6921843255127884, 0.692182586067741, 0.6922034594083103, 0.6923721855779132, 0.692509601736662, 0.692706159027024, 0.6928748851966269, 0.6930766608221313, 0.693429768166764, 0.6937724388411122, 0.6941325039659346, 0.6943342795914391, 0.6947013024964515, 0.6949517825832846, 0.6951222481979349, 0.6953361999387715, 0.6954770949876151, 0.6957101806239737, 0.6958528151178649, 0.6961519996660265, 0.6965138042358965, 0.6969208343770004, 0.6971260888925999, 0.6973000333973449, 0.6974983301327544, 0.6976270490662659, 0.6978183880214857, 0.6979297125045225, 0.6981401853552642, 0.6982288970526843, 0.6981123542345051, 0.697827085246723, 0.697712281873591], 'val_acc_list': [49.6, 58.699999999999996, 68.2, 74.2, 77.9, 78.10000000000001, 76.9, 75.35, 75.7, 76.55, 77.3, 77.2, 76.8, 76.55, 76.6, 77.3, 77.64999999999999, 77.8, 77.55, 77.10000000000001, 77.0, 77.25, 77.5, 78.05, 78.2, 78.2, 78.0, 77.9, 77.95, 78.25, 78.55, 78.35, 78.35, 78.3, 78.45, 78.75, 79.25, 79.2, 78.75, 79.0, 79.25, 79.65, 79.7, 79.5, 79.45, 79.45, 80.2, 80.35, 80.35, 80.45, 80.35, 80.60000000000001, 80.35, 80.45, 80.45, 80.65, 80.7, 80.65, 80.75, 80.85, 80.7, 80.80000000000001, 80.85, 80.85, 80.9, 81.05, 81.15, 81.05, 81.0, 81.2, 81.25, 81.3, 81.10000000000001, 81.15, 81.10000000000001, 81.3, 81.39999999999999, 81.3, 81.35, 81.3, 81.25, 81.25, 81.2, 81.25, 81.10000000000001, 81.2, 81.25, 81.35, 81.35, 81.3, 81.3, 81.39999999999999, 81.39999999999999, 81.35, 81.3, 81.39999999999999, 81.6, 81.69999999999999, 81.75, 81.8, 81.8, 81.75, 81.6, 81.6, 81.69999999999999, 81.85, 81.89999999999999, 81.89999999999999, 81.95, 81.95, 81.95, 81.95, 81.95, 82.0, 81.89999999999999, 82.0, 82.0, 82.0, 82.05, 82.05, 82.05, 81.95, 81.95, 82.0, 82.0, 81.95, 81.95, 82.0, 81.95, 81.95, 82.0, 82.19999999999999, 82.19999999999999, 82.19999999999999, 82.19999999999999, 82.19999999999999, 82.15, 82.0, 82.05, 82.19999999999999, 82.19999999999999, 82.25, 82.19999999999999, 82.25, 82.25, 82.19999999999999, 82.19999999999999, 82.39999999999999, 82.39999999999999, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.39999999999999, 82.39999999999999, 82.45, 82.5, 82.55, 82.5, 82.45, 82.45, 82.39999999999999, 82.39999999999999, 82.45, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.55, 82.6, 82.6, 82.6, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.55, 82.6, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.65, 82.69999999999999, 82.69999999999999, 82.69999999999999, 82.69999999999999], 'val_loss_list': [1.629340410232544, 1.2958097457885742, 1.020902395248413, 0.8655728101730347, 0.7826301455497742, 0.711323082447052, 0.6661116480827332, 0.654102087020874, 0.6371316909790039, 0.6258140802383423, 0.6244701743125916, 0.620438277721405, 0.6137092113494873, 0.6071720123291016, 0.6009336113929749, 0.595207154750824, 0.5912567973136902, 0.5879170894622803, 0.5837347507476807, 0.5798614621162415, 0.5759134292602539, 0.5707285404205322, 0.5659002661705017, 0.5623165965080261, 0.5588229298591614, 0.5544959902763367, 0.5509272217750549, 0.5482465624809265, 0.5458353161811829, 0.5430175065994263, 0.5397081971168518, 0.5363556146621704, 0.5327557325363159, 0.5291687250137329, 0.5258911848068237, 0.5235861539840698, 0.5215109586715698, 0.5189169049263, 0.5166380405426025, 0.5141234397888184, 0.5116010904312134, 0.5092655420303345, 0.5070494413375854, 0.5051196813583374, 0.5036055445671082, 0.5015631914138794, 0.4994601905345917, 0.4975332021713257, 0.4954797327518463, 0.4933786690235138, 0.4915024936199188, 0.48962023854255676, 0.48800650238990784, 0.4867445230484009, 0.48517000675201416, 0.4833710491657257, 0.48165932297706604, 0.48002973198890686, 0.4784293472766876, 0.47699064016342163, 0.47564032673835754, 0.4743531048297882, 0.4730997085571289, 0.47181272506713867, 0.4704437255859375, 0.46915313601493835, 0.4683052599430084, 0.46761661767959595, 0.46657228469848633, 0.465434730052948, 0.46427229046821594, 0.4631000757217407, 0.46206656098365784, 0.4612126350402832, 0.46037131547927856, 0.4594971537590027, 0.4587039053440094, 0.4581124186515808, 0.4574427604675293, 0.4565509855747223, 0.45565930008888245, 0.45487865805625916, 0.4539676010608673, 0.45297619700431824, 0.452143132686615, 0.4514683783054352, 0.45090532302856445, 0.4504360854625702, 0.4500923454761505, 0.4496941864490509, 0.449263334274292, 0.44892752170562744, 0.44849756360054016, 0.44794797897338867, 0.44741326570510864, 0.44693246483802795, 0.44648951292037964, 0.44601696729660034, 0.44547009468078613, 0.44490551948547363, 0.4444211721420288, 0.44394585490226746, 0.4434306025505066, 0.4430240988731384, 0.44272714853286743, 0.4425176680088043, 0.44239845871925354, 0.44220975041389465, 0.4419041574001312, 0.4414927363395691, 0.4410955011844635, 0.44070181250572205, 0.440303236246109, 0.4399953782558441, 0.439718633890152, 0.43949517607688904, 0.43933695554733276, 0.4391952455043793, 0.4390600919723511, 0.4388255178928375, 0.43846502900123596, 0.4380953311920166, 0.43777263164520264, 0.43752625584602356, 0.43732476234436035, 0.4370228052139282, 0.43665534257888794, 0.43637678027153015, 0.4361928403377533, 0.43607813119888306, 0.4360049068927765, 0.43595582246780396, 0.4358873665332794, 0.4357771575450897, 0.4356021583080292, 0.4354107975959778, 0.43522748351097107, 0.43507060408592224, 0.43493935465812683, 0.4348284602165222, 0.434747576713562, 0.43467456102371216, 0.43454134464263916, 0.4343523681163788, 0.43417859077453613, 0.4340529143810272, 0.4339396059513092, 0.43388861417770386, 0.43389081954956055, 0.43390586972236633, 0.4338744580745697, 0.43373221158981323, 0.4334471821784973, 0.4330872893333435, 0.4327746033668518, 0.4325339198112488, 0.432393342256546, 0.4323609471321106, 0.4323911964893341, 0.43244993686676025, 0.4324837028980255, 0.4324168860912323, 0.4322143495082855, 0.43199533224105835, 0.43180906772613525, 0.43166735768318176, 0.4315836727619171, 0.4315379559993744, 0.43150240182876587, 0.4314650595188141, 0.43142104148864746, 0.43134790658950806, 0.4312436878681183, 0.4311652183532715, 0.43110668659210205, 0.4310928285121918, 0.4310969114303589, 0.43111270666122437, 0.43112921714782715, 0.4310685992240906, 0.43099740147590637, 0.4309007227420807, 0.4307919442653656, 0.4307434856891632, 0.43072307109832764, 0.4307454824447632, 0.43082138895988464, 0.43079084157943726, 0.430700421333313, 0.4306371212005615, 0.4306192100048065, 0.4305680990219116, 0.4304710626602173, 0.4303515553474426, 0.4302824139595032, 0.4302837550640106, 0.430276483297348, 0.430229127407074, 0.43019530177116394, 0.4301486611366272, 0.43012183904647827, 0.43011119961738586, 0.4300912022590637, 0.4300365149974823, 0.43001583218574524, 0.4299781024456024, 0.42987531423568726, 0.4297562539577484, 0.42970678210258484, 0.42972686886787415, 0.4297092854976654, 0.4296632409095764], 'test_auc': 0.6839711310608823, 'test_acc': 84.35000000000001, 'test_loss': 0.40878257155418396, 'best_val_auc': 0.697712281873591, 'best_val_acc': 82.69999999999999, 'best_val_loss': 0.4296632409095764})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCkVRFCUh/w9JPdtXiwI1TgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "('1', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], 'train_loss_list': [1.404206395149231, 1.370021939277649, 1.4015512466430664, 1.4169906377792358, 1.3799759149551392, 1.3118870258331299, 1.272487759590149, 1.3452730178833008, 1.375504493713379, 1.3050360679626465, 1.213464379310608, 1.2627809047698975, 1.2355409860610962, 1.2769867181777954, 1.2160381078720093, 1.1963428258895874, 1.2166086435317993, 1.2024139165878296, 1.1286091804504395, 1.1037381887435913, 1.1398311853408813, 1.094913363456726, 1.0947434902191162, 1.022749423980713, 1.0477275848388672, 1.127169132232666, 1.0431263446807861, 1.051357626914978, 0.9955052137374878, 1.0163363218307495, 0.9566190242767334, 0.9954514503479004, 0.9168347120285034, 0.9068643450737, 0.8330731987953186, 0.8850165009498596, 0.8847748637199402, 0.8613168597221375, 0.8176584839820862, 0.8367512226104736, 0.8117629885673523, 0.7923136949539185, 0.7620097994804382, 0.811667799949646, 0.7518666982650757, 0.7499101161956787, 0.7392191886901855, 0.7138817310333252, 0.6514502167701721, 0.7076519131660461, 0.6601818799972534, 0.6776357293128967, 0.6377831101417542, 0.6143308877944946, 0.6158458590507507, 0.5900426506996155, 0.6014204621315002, 0.6145803332328796, 0.5695626735687256, 0.5647723078727722, 0.5340122580528259, 0.5810095071792603, 0.5826395750045776, 0.5326037406921387, 0.5516769289970398, 0.5203452110290527, 0.5253540873527527, 0.502197265625, 0.5077504515647888, 0.4961855709552765, 0.5174547433853149, 0.4801451861858368, 0.45274266600608826, 0.4641599953174591, 0.4460642337799072, 0.44378662109375, 0.4601607620716095, 0.4202342927455902, 0.4211186170578003, 0.41572797298431396, 0.3942630887031555, 0.3842496871948242, 0.39620840549468994, 0.417317271232605, 0.4064599573612213, 0.3835236430168152, 0.3789514899253845, 0.37323805689811707, 0.36742889881134033, 0.34002694487571716, 0.3468487560749054, 0.31927794218063354, 0.3381570875644684, 0.3337782621383667, 0.3160534203052521, 0.3107880651950836, 0.309544175863266, 0.2934500277042389, 0.3087829053401947, 0.2910912036895752, 0.2852267324924469, 0.2819036841392517, 0.2656155228614807, 0.2650555372238159, 0.25802457332611084, 0.27517327666282654, 0.2507191598415375, 0.26888495683670044, 0.24420326948165894, 0.2633012533187866, 0.24550186097621918, 0.23391494154930115, 0.23451818525791168, 0.23246459662914276, 0.2245970517396927, 0.20886385440826416, 0.224916473031044, 0.21436432003974915, 0.2172294557094574, 0.21284571290016174, 0.20244862139225006, 0.19253197312355042, 0.18912090361118317, 0.17964041233062744, 0.18093615770339966, 0.1751755326986313, 0.1843634396791458, 0.17793196439743042, 0.17077982425689697, 0.17874464392662048, 0.17061275243759155, 0.166143000125885, 0.15868312120437622, 0.15765303373336792, 0.15097616612911224, 0.1648896038532257, 0.14456301927566528, 0.15303370356559753, 0.15187440812587738, 0.13543978333473206, 0.14462803304195404, 0.14034901559352875, 0.14649471640586853, 0.13161994516849518, 0.13542157411575317, 0.13613632321357727, 0.1331547200679779, 0.1263444572687149, 0.12439120560884476, 0.12404226511716843, 0.12519541382789612, 0.11325813084840775, 0.12095996737480164, 0.12181787192821503, 0.11889722943305969, 0.11030912399291992, 0.11178789287805557, 0.10932040214538574, 0.10928536206483841, 0.10664942115545273, 0.10481652617454529, 0.10875561088323593, 0.10644306242465973, 0.10556894540786743, 0.10229971259832382, 0.10502683371305466, 0.09985408186912537, 0.09118761122226715, 0.09659747034311295, 0.09788859635591507, 0.09433899819850922, 0.09773237258195877, 0.0936424508690834, 0.09564276039600372, 0.09137064963579178, 0.08988510072231293, 0.08624785393476486, 0.08747588098049164, 0.08924099802970886, 0.0817989632487297, 0.08458201587200165, 0.08350049704313278, 0.0788906067609787, 0.0753990188241005, 0.07602885365486145, 0.07922676205635071, 0.0818239152431488, 0.07312541455030441, 0.07645825296640396, 0.07325146347284317, 0.07110410183668137, 0.06975327432155609, 0.07903043925762177, 0.07094930857419968, 0.07274377346038818, 0.07348761707544327, 0.07113604247570038, 0.07030034065246582, 0.06233450770378113, 0.06711682677268982], 'val_auc_list': [0.49024044879659856, 0.49051438757321114, 0.490892500250789, 0.49121273853894176, 0.49164486731331647, 0.4920152634056377, 0.4924223132779283, 0.49282550485759036, 0.4932094049741108, 0.49367818752845494, 0.4941894113017108, 0.4947546511717634, 0.49528709555447525, 0.4958098942056161, 0.4965256074881743, 0.49733584894012695, 0.498094003441597, 0.49889845745460715, 0.4997723607349276, 0.5008121705982669, 0.501805680950066, 0.5028397033744627, 0.5040917193323611, 0.5051932618777538, 0.5066034678334144, 0.5079905240333048, 0.5093775802331952, 0.5110925913064951, 0.5127670903071972, 0.51433548626062, 0.5160408516023489, 0.5176169641410283, 0.5195287481383738, 0.5213903743315508, 0.5233908990593482, 0.5255187474438812, 0.5277295491199234, 0.5298284603097437, 0.5321260735699238, 0.5344236868301039, 0.5366499216766597, 0.5389745429852382, 0.5415538116072875, 0.5443472154702101, 0.5467702232408114, 0.5491662229630143, 0.5517242709756078, 0.5542688149640022, 0.5567304056608869, 0.5591977837967143, 0.5617616192482503, 0.5638296640970438, 0.5664648779622042, 0.5688338696360087, 0.5712395150897825, 0.5735699238373034, 0.5756591892955529, 0.5778622743863386, 0.5800557137455533, 0.5819925766449832, 0.58409920442006, 0.5859473265890378, 0.5880925372903982, 0.5897554614132154, 0.5916942534589593, 0.593430485141715, 0.5951917957265551, 0.5966984589979243, 0.5981588227577532, 0.5996153282249539, 0.6010371090584994, 0.6023334953816237, 0.6035932279247787, 0.6047854403469376, 0.6061416302058014, 0.6073897878710713, 0.6084083771249547, 0.6096333850344545, 0.6107619356282459, 0.6118171786620984, 0.612893642305407, 0.613896798388777, 0.6148555841069211, 0.6157912200692949, 0.6165918157896768, 0.6174541441921122, 0.6182315901567238, 0.6188412003920025, 0.6193283098363312, 0.619938884644767, 0.62057357378213, 0.6211619634079527, 0.6217542113264038, 0.6223426009522266, 0.6228673287496816, 0.6235097344723013, 0.6240711160497256, 0.6246807262850043, 0.6251379339614634, 0.625656874319976, 0.6261806375442739, 0.6266889675980585, 0.6271673958839735, 0.62764196587726, 0.6281281107484317, 0.6286084681806607, 0.6290618175644912, 0.6295846162156322, 0.6300804068183748, 0.6306398592494848, 0.6312369300337215, 0.6317414017948777, 0.6321677431303099, 0.632592155319428, 0.6330300715327453, 0.6334525545755492, 0.633805588351043, 0.6340563773718854, 0.6344827187073176, 0.6349283515058917, 0.6353315430855537, 0.6356286316179365, 0.6360202483197136, 0.6363057619742112, 0.6365391886782261, 0.6368767892832063, 0.6373822256175198, 0.6378124252455804, 0.6381905379231582, 0.6384856973092267, 0.6387036908427283, 0.639124244739218, 0.6394772785147116, 0.6397666504618376, 0.6399942897269101, 0.6402431496014385, 0.6403280320392621, 0.6406444120347865, 0.6409434297134833, 0.6411228403207012, 0.6413408338542028, 0.6415028821445934, 0.6416263475087005, 0.6417247339707233, 0.6419215068947689, 0.6420951300630446, 0.6423632814007145, 0.6424732427406225, 0.6425832040805303, 0.6428436388329437, 0.6431175776095562, 0.6433394294356861, 0.6435825018712719, 0.6438255743068577, 0.6439355356467655, 0.64403006381616, 0.644195970399179, 0.6444178222253089, 0.644508492102075, 0.6446512489293238, 0.6447978640492009, 0.6449753455101048, 0.6451817641657214, 0.6452454259940891, 0.6453206627003419, 0.6455039316001884, 0.6455135773317591, 0.6455425145264718, 0.645702633670548, 0.6458318864735977, 0.6460440925681568, 0.6462042117122331, 0.6463083856131984, 0.6463913389047078, 0.6463990554899645, 0.6464453550015047, 0.6464877962204165, 0.6466093324382094, 0.6466691359739488, 0.6467308686560023, 0.6467790973138566, 0.6467983887769985, 0.6467675224359717, 0.6467598058507149, 0.6468196093864543, 0.6468852003611361, 0.646972011945274, 0.647093548163067, 0.6471880763324613, 0.6472845336481701, 0.6473481954765377, 0.6474099281585913, 0.6474504402311889, 0.6476394965699778, 0.6477706785193417, 0.6479790263212724, 0.6480561921738393, 0.6480909168074944, 0.6481719409526896, 0.6482433193663141], 'val_acc_list': [57.05, 57.550000000000004, 57.85, 58.199999999999996, 58.550000000000004, 58.650000000000006, 59.0, 59.099999999999994, 59.199999999999996, 59.650000000000006, 60.199999999999996, 60.45, 61.3, 61.75000000000001, 62.1, 62.4, 62.9, 63.449999999999996, 64.2, 64.45, 64.9, 65.14999999999999, 65.75, 66.10000000000001, 66.2, 66.85, 67.30000000000001, 67.7, 68.35, 68.60000000000001, 68.89999999999999, 69.19999999999999, 69.75, 70.15, 70.39999999999999, 70.85000000000001, 71.15, 71.45, 71.7, 72.05, 72.45, 72.89999999999999, 73.2, 73.2, 73.55000000000001, 73.8, 73.95, 74.45, 74.5, 74.75, 75.3, 75.35, 75.75, 75.8, 75.75, 76.1, 76.0, 76.2, 76.25, 76.4, 76.75, 76.9, 77.05, 77.0, 77.10000000000001, 77.25, 77.45, 77.64999999999999, 77.8, 77.8, 78.0, 78.14999999999999, 78.10000000000001, 78.3, 78.45, 78.4, 78.55, 78.60000000000001, 78.64999999999999, 78.60000000000001, 78.4, 78.35, 78.4, 78.35, 78.4, 78.3, 78.25, 78.35, 78.4, 78.55, 78.60000000000001, 78.75, 78.85, 78.85, 78.7, 78.75, 78.75, 78.7, 78.60000000000001, 78.64999999999999, 78.60000000000001, 78.64999999999999, 78.60000000000001, 78.5, 78.64999999999999, 78.8, 78.75, 78.9, 78.95, 78.9, 78.8, 78.7, 78.7, 78.8, 78.9, 78.95, 78.95, 79.05, 79.05, 79.05, 79.10000000000001, 79.10000000000001, 79.0, 79.10000000000001, 79.05, 79.14999999999999, 79.2, 79.25, 79.3, 79.35, 79.35, 79.25, 79.14999999999999, 79.14999999999999, 79.14999999999999, 79.2, 79.14999999999999, 79.25, 79.25, 79.25, 79.3, 79.3, 79.25, 79.4, 79.45, 79.4, 79.5, 79.4, 79.35, 79.4, 79.35, 79.4, 79.35, 79.35, 79.4, 79.45, 79.4, 79.35, 79.4, 79.3, 79.35, 79.45, 79.45, 79.5, 79.4, 79.35, 79.3, 79.35, 79.4, 79.55, 79.60000000000001, 79.55, 79.55, 79.80000000000001, 79.80000000000001, 79.75, 79.75, 79.80000000000001, 79.75, 79.75, 79.75, 79.80000000000001, 79.95, 79.9, 79.85, 79.85, 79.85, 79.85, 79.9, 79.9, 79.95, 79.95, 79.95, 79.95, 79.95, 80.05, 80.05, 80.05, 80.05, 80.05], 'val_loss_list': [1.3474602699279785, 1.3349910974502563, 1.3225206136703491, 1.309967041015625, 1.297271728515625, 1.2844778299331665, 1.2715281248092651, 1.2583767175674438, 1.2450294494628906, 1.2314618825912476, 1.2177642583847046, 1.203898549079895, 1.189820408821106, 1.175594449043274, 1.1611977815628052, 1.1466560363769531, 1.1320087909698486, 1.1172287464141846, 1.1023945808410645, 1.0875200033187866, 1.0726075172424316, 1.0576074123382568, 1.042553424835205, 1.0274714231491089, 1.0124050378799438, 0.9973751306533813, 0.9824276566505432, 0.9675790667533875, 0.9528352618217468, 0.9382822513580322, 0.9239385724067688, 0.9098274111747742, 0.8959654569625854, 0.8823930621147156, 0.8691721558570862, 0.8563039302825928, 0.8437633514404297, 0.8315461277961731, 0.8196776509284973, 0.8081915378570557, 0.7970162630081177, 0.78620445728302, 0.7757927179336548, 0.7657340168952942, 0.7560701370239258, 0.7467999458312988, 0.737876832485199, 0.7293009161949158, 0.7210772633552551, 0.7132673859596252, 0.7058480978012085, 0.6987844705581665, 0.6920275688171387, 0.685588002204895, 0.6794478893280029, 0.6736091375350952, 0.6680282354354858, 0.6626867651939392, 0.6576127409934998, 0.6527652740478516, 0.6481350064277649, 0.6436796188354492, 0.6394003629684448, 0.6352548599243164, 0.6312241554260254, 0.6273237466812134, 0.62351393699646, 0.619813084602356, 0.6162396669387817, 0.6127960085868835, 0.6094481348991394, 0.60621178150177, 0.6030588150024414, 0.5999295115470886, 0.5968939661979675, 0.5940234661102295, 0.5912156701087952, 0.5884904861450195, 0.5858930945396423, 0.5834258794784546, 0.5810327529907227, 0.5786685347557068, 0.5763421058654785, 0.5740654468536377, 0.5719062089920044, 0.5698257684707642, 0.5677719116210938, 0.5657588243484497, 0.5638003945350647, 0.5618855953216553, 0.5600316524505615, 0.5582166910171509, 0.5564486980438232, 0.5547434091567993, 0.5530962347984314, 0.5514359474182129, 0.5498484373092651, 0.5482966899871826, 0.5468465685844421, 0.5454875230789185, 0.544141411781311, 0.5427958369255066, 0.5414283871650696, 0.5400239825248718, 0.5386287569999695, 0.5371940732002258, 0.5357182025909424, 0.534256637096405, 0.5329018831253052, 0.5316851139068604, 0.5305547118186951, 0.5294870734214783, 0.5284603834152222, 0.5274580717086792, 0.5265843868255615, 0.5257616639137268, 0.5249319672584534, 0.5241054892539978, 0.5233254432678223, 0.5226365923881531, 0.5219526290893555, 0.5212147831916809, 0.5204281806945801, 0.519584059715271, 0.5187398791313171, 0.5179077982902527, 0.5171061754226685, 0.5163969397544861, 0.5157611966133118, 0.5152233242988586, 0.5148661136627197, 0.5145395994186401, 0.5141245126724243, 0.5135822892189026, 0.5130191445350647, 0.5124419927597046, 0.5118486285209656, 0.5112594962120056, 0.5107049942016602, 0.5101196765899658, 0.5095759034156799, 0.5091428756713867, 0.5087644457817078, 0.5084497332572937, 0.5081696510314941, 0.5079262852668762, 0.50771564245224, 0.507501482963562, 0.5072957873344421, 0.5071262717247009, 0.5069634914398193, 0.5067148804664612, 0.5064266920089722, 0.5061639547348022, 0.5059600472450256, 0.5058107972145081, 0.5056095123291016, 0.5053994655609131, 0.5052065849304199, 0.504963219165802, 0.5047783851623535, 0.5047428011894226, 0.5047536492347717, 0.504768967628479, 0.5048055648803711, 0.5048372149467468, 0.5048500299453735, 0.5047602653503418, 0.5045650005340576, 0.5043880939483643, 0.5042784214019775, 0.5042206645011902, 0.5041258335113525, 0.5039591789245605, 0.5037988424301147, 0.503624439239502, 0.5034967660903931, 0.5034163594245911, 0.503413200378418, 0.50348299741745, 0.5035532116889954, 0.5035955905914307, 0.5035647749900818, 0.5034782290458679, 0.5033788084983826, 0.5032981038093567, 0.5032311677932739, 0.5031976103782654, 0.5031384229660034, 0.5030488967895508, 0.502972424030304, 0.5028798580169678, 0.5028263926506042, 0.5028652548789978, 0.5029107928276062, 0.5029863715171814, 0.503026008605957, 0.5029861927032471, 0.5028672218322754, 0.5026828646659851], 'test_auc': 0.6269294668223817, 'test_acc': 77.5, 'test_loss': 0.5720809102058411, 'best_val_auc': 0.6482433193663141, 'best_val_acc': 80.05, 'best_val_loss': 0.5026828646659851})\n",
            "('2', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190], 'train_loss_list': [3.1428825855255127, 3.126809597015381, 3.1933434009552, 3.0923752784729004, 3.0365097522735596, 3.034878730773926, 3.0497186183929443, 2.979233980178833, 2.923130512237549, 2.8631696701049805, 2.7733371257781982, 2.6679108142852783, 2.701749801635742, 2.661004066467285, 2.5583748817443848, 2.4734842777252197, 2.4705231189727783, 2.3318936824798584, 2.271667242050171, 2.197571277618408, 2.1724865436553955, 2.105522394180298, 1.9790492057800293, 1.8957771062850952, 1.8558555841445923, 1.8115779161453247, 1.71134614944458, 1.660645604133606, 1.5738447904586792, 1.4739211797714233, 1.4210237264633179, 1.3490347862243652, 1.247218370437622, 1.2284860610961914, 1.1252195835113525, 1.0457274913787842, 1.0134886503219604, 0.973752498626709, 0.9214327931404114, 0.8556714057922363, 0.8125116229057312, 0.769747793674469, 0.7639029026031494, 0.7389314770698547, 0.7109971642494202, 0.6847970485687256, 0.6611931324005127, 0.6299176216125488, 0.6532052755355835, 0.6114857792854309, 0.6134804487228394, 0.5869598984718323, 0.6242151856422424, 0.5771743059158325, 0.5549517869949341, 0.5149306058883667, 0.5137608051300049, 0.5191597938537598, 0.5342810153961182, 0.5204582214355469, 0.481131911277771, 0.4777178168296814, 0.4965687692165375, 0.4961928427219391, 0.47506827116012573, 0.48955968022346497, 0.4405710697174072, 0.4405396282672882, 0.4600643217563629, 0.42336297035217285, 0.4273929297924042, 0.4212351143360138, 0.38870471715927124, 0.37644654512405396, 0.3905009329319, 0.3775586187839508, 0.36299940943717957, 0.35251984000205994, 0.37669673562049866, 0.36080417037010193, 0.3390396237373352, 0.3271750509738922, 0.33056122064590454, 0.31402838230133057, 0.3364454209804535, 0.30510151386260986, 0.31769612431526184, 0.29126518964767456, 0.27507954835891724, 0.28392258286476135, 0.2867071032524109, 0.26622313261032104, 0.28568652272224426, 0.26604166626930237, 0.26347753405570984, 0.26621371507644653, 0.2624739408493042, 0.24749301373958588, 0.2420833557844162, 0.23190362751483917, 0.23301206529140472, 0.24412678182125092, 0.22574104368686676, 0.23139134049415588, 0.2130378931760788, 0.21872971951961517, 0.21057212352752686, 0.1992989480495453, 0.18847151100635529, 0.1979786455631256, 0.1908101588487625, 0.180894672870636, 0.17622898519039154, 0.17563174664974213, 0.17729954421520233, 0.176358163356781, 0.17384742200374603, 0.17146718502044678, 0.159674733877182, 0.14873014390468597, 0.15962518751621246, 0.14366281032562256, 0.15831027925014496, 0.1610862761735916, 0.14571180939674377, 0.14763273298740387, 0.13107424974441528, 0.13349691033363342, 0.1344270557165146, 0.1301654726266861, 0.1303151696920395, 0.12029385566711426, 0.12910054624080658, 0.1155945286154747, 0.11949287354946136, 0.11944245547056198, 0.12015004456043243, 0.11796318739652634, 0.1042066290974617, 0.1099410131573677, 0.10592139512300491, 0.11324559897184372, 0.09358479827642441, 0.10260972380638123, 0.1020997166633606, 0.09934220463037491, 0.10593744367361069, 0.0954354777932167, 0.09334400296211243, 0.08965110778808594, 0.09439120441675186, 0.08788836747407913, 0.09487079828977585, 0.09000787138938904, 0.08052472770214081, 0.08888107538223267, 0.08673419803380966, 0.08316056430339813, 0.08325802534818649, 0.08151131868362427, 0.07771696150302887, 0.0775301530957222, 0.07549642026424408, 0.07025983929634094, 0.07165411114692688, 0.07438293099403381, 0.07399464398622513, 0.07166676968336105, 0.0704919621348381, 0.06743798404932022, 0.07143605500459671, 0.07126117497682571, 0.06693991273641586, 0.06695146858692169, 0.07026681303977966, 0.06362518668174744, 0.0645485371351242, 0.07003294676542282, 0.0650586187839508, 0.06462190300226212, 0.05760641023516655, 0.06020013988018036, 0.05793164670467377, 0.05558330938220024, 0.060708023607730865, 0.06065305322408676, 0.06343559920787811, 0.051504164934158325, 0.053193043917417526, 0.049541059881448746, 0.060015711933374405], 'val_auc_list': [0.5043155003048052, 0.5043531186579315, 0.5044148513399851, 0.504492981765709, 0.5045518207282913, 0.5045932973740461, 0.5046405614587433, 0.504748593652337, 0.5048392635291031, 0.5049067836500992, 0.5050234970021067, 0.5050370010263059, 0.5051585372440988, 0.5052742860229491, 0.5053919639481137, 0.5055241104706345, 0.5056379301031707, 0.5057874389425192, 0.5059311603429251, 0.5061501184495837, 0.5064144114946254, 0.50670088972228, 0.5070597109367163, 0.5074397527606084, 0.5078795981202399, 0.5083995030519095, 0.5089251954225216, 0.5093631116358389, 0.5099785093100601, 0.5107048328973463, 0.5113385574615521, 0.5122028150103016, 0.513053568534852, 0.5141531819339306, 0.5152952365519211, 0.5166070560455587, 0.5179420252949665, 0.5194718383221058, 0.5211656287859496, 0.5230677670517243, 0.5251223078763185, 0.5272038567493114, 0.5292024523307945, 0.5317566420507597, 0.5343262649412381, 0.5366354530793034, 0.5391221226782724, 0.5415470595951879, 0.5442980222391987, 0.5471396547599756, 0.5497247108209675, 0.5523078377356452, 0.5550549420870277, 0.5576506084527475, 0.5600938722596477, 0.562484084542908, 0.5649514626787354, 0.5674284865461336, 0.5697704701715397, 0.5720507211148922, 0.574255735351992, 0.5765147656858887, 0.5785770230957397, 0.5806701468466174, 0.5824623237724842, 0.5843721786235155, 0.586349553595543, 0.5882362586908042, 0.5898856787894221, 0.5914579330354731, 0.5931015656951486, 0.5944963384802957, 0.5959663479716956, 0.5973128920989883, 0.5986169950073693, 0.599861294380011, 0.6013409496029816, 0.6027723761680981, 0.6041189202953909, 0.60560243381099, 0.6067387009900379, 0.608112253165729, 0.6093526942457423, 0.6104503784985068, 0.6116194411648957, 0.612714231698189, 0.6137569352809995, 0.6147735953885686, 0.6159850992738692, 0.6170191216982661, 0.6181013727805171, 0.6191855530090825, 0.6200710311672879, 0.6210819038359146, 0.6220754141877137, 0.6229068762491223, 0.6236814284942628, 0.6245582254940544, 0.6253993332870337, 0.6261266214474771, 0.6269542252162572, 0.6277953330092366, 0.628489825682339, 0.6292672716469507, 0.6300061346852791, 0.6306755484562971, 0.6313391747883728, 0.6318928397805402, 0.6325101666010756, 0.6331506431773811, 0.6337120247548055, 0.6343409264532258, 0.634921599493792, 0.6354501855838754, 0.6359556219181887, 0.6364244044725329, 0.6369221242215894, 0.6373889776296193, 0.6379175637197028, 0.638399850298246, 0.6387953252926515, 0.6392178083354554, 0.6396441496708877, 0.6400068291779522, 0.6403328549050473, 0.6408035666057057, 0.6412241205021953, 0.6415385713514056, 0.6419687709794661, 0.642279363536048, 0.6426786968230819, 0.6429680687702077, 0.6433558271793567, 0.6437744519295322, 0.6441120525345124, 0.6444380782616076, 0.6446464260635384, 0.6449377271569785, 0.6451615081294225, 0.6454692069665331, 0.645780764096272, 0.6461222229938808, 0.6463980909168076, 0.6466807108518339, 0.6469382518847759, 0.6472083323687602, 0.6474475465117178, 0.6475594369979396, 0.6478237300429813, 0.6480127863817703, 0.6482462130857853, 0.6485490890571105, 0.6488365318579222, 0.6490834625861364, 0.6493246058754081, 0.6495753948962505, 0.6498435462339205, 0.6501001226937055, 0.650404927811345, 0.6506981580510992, 0.6510261129245086, 0.65119780694647, 0.6514350919431133, 0.6516820226713274, 0.6518749373027448, 0.6520466313247063, 0.6522646248582078, 0.6525038390011652, 0.6526784267425978, 0.6528028566798619, 0.6529417552144825, 0.6531404572848423, 0.6532851432584054, 0.6534761287435084, 0.6536420353265273, 0.6537172720327801, 0.6539005409326266, 0.6541243219050705, 0.6543095199512312, 0.6545043637289626, 0.654708853238265, 0.6549249176254524, 0.6551197614031838, 0.6552490142062336, 0.6554399996913366, 0.6556059062743554, 0.6557814585889452, 0.655935790294079, 0.6561123071818259, 0.6562888240695728, 0.6563929979705381], 'val_acc_list': [27.700000000000003, 27.800000000000004, 28.050000000000004, 28.1, 28.4, 28.449999999999996, 28.95, 29.25, 29.4, 29.9, 30.599999999999998, 30.95, 31.5, 32.2, 33.25, 33.75, 34.300000000000004, 34.849999999999994, 35.6, 36.5, 37.25, 38.35, 39.15, 40.1, 41.349999999999994, 42.55, 43.9, 45.2, 46.35, 48.199999999999996, 49.5, 50.74999999999999, 52.65, 54.35, 56.15, 57.550000000000004, 59.050000000000004, 60.85, 62.0, 63.55, 65.0, 66.60000000000001, 67.65, 69.19999999999999, 69.95, 70.6, 71.8, 72.89999999999999, 73.8, 74.15, 74.5, 75.3, 75.9, 76.44999999999999, 76.7, 77.0, 77.3, 77.5, 77.75, 77.8, 78.0, 78.10000000000001, 78.10000000000001, 78.25, 78.25, 78.35, 78.4, 78.35, 78.3, 78.25, 78.0, 77.95, 77.85, 78.05, 78.10000000000001, 78.25, 78.3, 78.4, 78.55, 78.45, 78.4, 78.3, 78.3, 78.35, 78.45, 78.4, 78.3, 78.4, 78.60000000000001, 78.60000000000001, 78.64999999999999, 78.7, 78.64999999999999, 78.64999999999999, 78.75, 78.85, 78.9, 78.9, 78.85, 78.85, 78.8, 78.85, 78.95, 78.85, 78.9, 78.95, 79.14999999999999, 79.10000000000001, 79.2, 79.3, 79.2, 79.14999999999999, 79.2, 79.2, 79.10000000000001, 79.05, 79.10000000000001, 79.14999999999999, 79.05, 79.0, 78.95, 78.95, 78.95, 78.95, 78.95, 78.95, 79.10000000000001, 79.10000000000001, 79.10000000000001, 79.10000000000001, 79.10000000000001, 79.25, 79.2, 79.14999999999999, 79.14999999999999, 79.14999999999999, 79.10000000000001, 79.10000000000001, 79.05, 79.0, 79.05, 79.0, 79.0, 79.10000000000001, 79.10000000000001, 79.05, 79.10000000000001, 79.10000000000001, 79.10000000000001, 79.05, 79.05, 79.14999999999999, 79.14999999999999, 79.14999999999999, 79.14999999999999, 79.14999999999999, 79.2, 79.3, 79.35, 79.35, 79.35, 79.35, 79.4, 79.45, 79.45, 79.4, 79.5, 79.45, 79.4, 79.45, 79.5, 79.55, 79.55, 79.60000000000001, 79.65, 79.60000000000001, 79.60000000000001, 79.65, 79.65, 79.7, 79.7, 79.7, 79.75, 79.75, 79.80000000000001, 79.9, 80.0, 80.05, 80.0, 80.05, 80.10000000000001], 'val_loss_list': [3.2328431606292725, 3.199026107788086, 3.1641311645507812, 3.128237247467041, 3.090890884399414, 3.051868200302124, 3.011016368865967, 2.968177556991577, 2.9230806827545166, 2.8759071826934814, 2.8264482021331787, 2.7745468616485596, 2.720367193222046, 2.6636879444122314, 2.6047215461730957, 2.5433361530303955, 2.4796547889709473, 2.413735866546631, 2.3456850051879883, 2.2756946086883545, 2.203965187072754, 2.13077712059021, 2.0562756061553955, 1.9808109998703003, 1.9047132730484009, 1.828321099281311, 1.7519810199737549, 1.676154613494873, 1.6011979579925537, 1.5276235342025757, 1.4558769464492798, 1.3863320350646973, 1.3194233179092407, 1.255510687828064, 1.1948484182357788, 1.137721300125122, 1.0843796730041504, 1.034988284111023, 0.9895651340484619, 0.948111355304718, 0.9105774760246277, 0.8769162893295288, 0.8468821048736572, 0.820268452167511, 0.7968468070030212, 0.776330828666687, 0.7583667635917664, 0.7426028847694397, 0.7287629246711731, 0.7165913581848145, 0.7058439254760742, 0.6962804794311523, 0.6876689195632935, 0.6798890829086304, 0.6728204488754272, 0.6663546562194824, 0.6603724956512451, 0.6548009514808655, 0.6495997309684753, 0.6447253227233887, 0.6401415467262268, 0.6358165740966797, 0.6317334175109863, 0.627856433391571, 0.6241682171821594, 0.6206810474395752, 0.6173684597015381, 0.6142120957374573, 0.611185610294342, 0.6082478165626526, 0.6054010987281799, 0.6026387214660645, 0.5999431014060974, 0.597322940826416, 0.5947920083999634, 0.5923221111297607, 0.5898923873901367, 0.5875141620635986, 0.5851954221725464, 0.5829493999481201, 0.5807781219482422, 0.5786615014076233, 0.576610267162323, 0.5746163129806519, 0.5726765990257263, 0.570797324180603, 0.5689594745635986, 0.5671778321266174, 0.5654201507568359, 0.563684344291687, 0.5619733929634094, 0.5603014230728149, 0.5586912631988525, 0.5571426749229431, 0.5556236505508423, 0.5541562438011169, 0.5527349710464478, 0.5513516664505005, 0.5500025749206543, 0.5487069487571716, 0.5474678874015808, 0.5462751984596252, 0.5451041460037231, 0.543964684009552, 0.5428517460823059, 0.5417369604110718, 0.5406597852706909, 0.5396295189857483, 0.5386309027671814, 0.5376440286636353, 0.5366693139076233, 0.5357164740562439, 0.5348117351531982, 0.5339384078979492, 0.533087968826294, 0.5322573781013489, 0.5314674973487854, 0.5307345390319824, 0.5300320982933044, 0.5293390154838562, 0.5286346673965454, 0.5279248952865601, 0.5272029042243958, 0.5265018343925476, 0.5258083939552307, 0.5251181125640869, 0.524448812007904, 0.5237810611724854, 0.5231192708015442, 0.5224860310554504, 0.5218979120254517, 0.521330714225769, 0.5207582712173462, 0.520202100276947, 0.5196539759635925, 0.5191082954406738, 0.5185753107070923, 0.5180697441101074, 0.5175831913948059, 0.5171109437942505, 0.5166448950767517, 0.5161795616149902, 0.5157040357589722, 0.5152163505554199, 0.5147497653961182, 0.5143000483512878, 0.5138712525367737, 0.5134397149085999, 0.5129984617233276, 0.5125539302825928, 0.5121208429336548, 0.5117175579071045, 0.5113419890403748, 0.5109943747520447, 0.5106474161148071, 0.5102857351303101, 0.5099339485168457, 0.5095924139022827, 0.5092572569847107, 0.5089361071586609, 0.5086236000061035, 0.5083115696907043, 0.5079987645149231, 0.5076848864555359, 0.5073803663253784, 0.5070667266845703, 0.5067657828330994, 0.5064719915390015, 0.5061709880828857, 0.5058590769767761, 0.5055434107780457, 0.5052385926246643, 0.5049446225166321, 0.5046626925468445, 0.5043861269950867, 0.5041065216064453, 0.5038286447525024, 0.5035492777824402, 0.5032582879066467, 0.5029745697975159, 0.5027037262916565, 0.5024285912513733, 0.5021393299102783, 0.5018383860588074, 0.5015283226966858, 0.5012155175209045, 0.5009073615074158, 0.5005940198898315, 0.5002734661102295, 0.49996089935302734, 0.4996577799320221], 'test_auc': 0.6322451747579382, 'test_acc': 79.14999999999999, 'test_loss': 0.5399757623672485, 'best_val_auc': 0.6563929979705381, 'best_val_acc': 80.10000000000001, 'best_val_loss': 0.4996577799320221})\n",
            "('4', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195], 'train_loss_list': [1.0781183242797852, 1.0613391399383545, 1.0624070167541504, 1.0344569683074951, 1.0546784400939941, 1.0246453285217285, 0.9494081139564514, 0.978451669216156, 0.9900577068328857, 0.9572091102600098, 0.9261684417724609, 0.9545964598655701, 0.9372278451919556, 0.8437309861183167, 0.8695052266120911, 0.8731457591056824, 0.8160290718078613, 0.8019053936004639, 0.7622926831245422, 0.7826981544494629, 0.7570931315422058, 0.7315437197685242, 0.7419103980064392, 0.6961825489997864, 0.6799585223197937, 0.6611670851707458, 0.6536603569984436, 0.6201989054679871, 0.6291458606719971, 0.6053621172904968, 0.5470224618911743, 0.536482572555542, 0.5719162821769714, 0.5660310387611389, 0.570034921169281, 0.5247084498405457, 0.5292027592658997, 0.5066971778869629, 0.4879392981529236, 0.4520450830459595, 0.46643057465553284, 0.4251483976840973, 0.41464439034461975, 0.40739497542381287, 0.39657527208328247, 0.3538721799850464, 0.3651532232761383, 0.343621164560318, 0.35085731744766235, 0.3412211239337921, 0.30448657274246216, 0.29934656620025635, 0.27393218874931335, 0.27311021089553833, 0.26854756474494934, 0.260011225938797, 0.2591334581375122, 0.24855294823646545, 0.23104137182235718, 0.2263597995042801, 0.22016587853431702, 0.19997188448905945, 0.19358178973197937, 0.19367341697216034, 0.18339495360851288, 0.19178317487239838, 0.18091616034507751, 0.17268288135528564, 0.16094841063022614, 0.15214547514915466, 0.1473727822303772, 0.15354782342910767, 0.1385120302438736, 0.14674586057662964, 0.1327057033777237, 0.127492293715477, 0.12372249364852905, 0.11141818016767502, 0.11804340034723282, 0.11324556171894073, 0.10887999832630157, 0.11181299388408661, 0.09722024202346802, 0.1035080999135971, 0.09792681038379669, 0.10030379891395569, 0.09070240706205368, 0.09171482175588608, 0.08773592859506607, 0.0869549959897995, 0.08395744115114212, 0.08799724280834198, 0.08453185856342316, 0.07840101420879364, 0.0771171823143959, 0.07844719290733337, 0.07050575315952301, 0.0698634460568428, 0.0633283481001854, 0.06954680383205414, 0.06710878014564514, 0.0678885281085968, 0.06570718437433243, 0.0632769986987114, 0.06355009973049164, 0.05921152979135513, 0.0608774870634079, 0.06181970611214638, 0.05425388365983963, 0.05668588727712631, 0.053994905203580856, 0.05712132900953293, 0.05710940435528755, 0.054128240793943405, 0.05057322606444359, 0.04902547970414162, 0.04690265655517578, 0.053118932992219925, 0.050575703382492065, 0.05031269043684006, 0.04637806490063667, 0.04737262800335884, 0.04631132632493973, 0.04868787154555321, 0.04378001391887665, 0.04450482502579689, 0.043850455433130264, 0.043985515832901, 0.04365169629454613, 0.04073472321033478, 0.03986078128218651, 0.04348548501729965, 0.0430389866232872, 0.039603423327207565, 0.040816158056259155, 0.039760056883096695, 0.03902808576822281, 0.03637529909610748, 0.03823242709040642, 0.0375937856733799, 0.034231677651405334, 0.03743289038538933, 0.03611060231924057, 0.03750411421060562, 0.036605354398489, 0.03679227456450462, 0.035106681287288666, 0.03851814195513725, 0.03414957597851753, 0.03396002575755119, 0.03455367684364319, 0.034040555357933044, 0.03259320184588432, 0.035038646310567856, 0.033401187509298325, 0.03710514307022095, 0.03317957744002342, 0.03041338175535202, 0.03187775984406471, 0.02955756150186062, 0.03255604952573776, 0.03074529394507408, 0.03132759779691696, 0.028635330498218536, 0.031234361231327057, 0.028865275904536247, 0.02959977649152279, 0.030778145417571068, 0.03038036823272705, 0.02978360839188099, 0.028768837451934814, 0.029230747371912003, 0.027397727593779564, 0.028403496369719505, 0.026476187631487846, 0.028365056961774826, 0.027195025235414505, 0.026804747059941292, 0.02603588066995144, 0.025394782423973083, 0.028391895815730095, 0.029048623517155647, 0.027194242924451828, 0.030947245657444, 0.028178555890917778, 0.02632736600935459, 0.025340111926198006, 0.02542235143482685, 0.027700137346982956, 0.024481646716594696, 0.023915622383356094, 0.026496263220906258, 0.023472819477319717, 0.023770621046423912, 0.026439214125275612, 0.024711735546588898], 'val_auc_list': [0.4537438472985348, 0.454189202724359, 0.45465959821428575, 0.4551639766483517, 0.4557398981227106, 0.4563980940934066, 0.4572208390567766, 0.458079355540293, 0.45917932978479853, 0.46014516082875456, 0.4614222040979854, 0.46292639652014655, 0.4645754635989011, 0.46662874885531136, 0.46882333161630035, 0.4711359603937729, 0.47385817307692313, 0.4769613524496337, 0.480155749198718, 0.4835236378205129, 0.4872206244276557, 0.49118321457188646, 0.49525669642857145, 0.4997478107829671, 0.5043981084020146, 0.509046617445055, 0.5137630923763736, 0.5185779389880952, 0.5233355511675825, 0.5281199919871795, 0.5327890696543041, 0.537273923992674, 0.5417006496108058, 0.5462042839972527, 0.550452152014652, 0.5544335222069596, 0.5580375028617216, 0.5617559523809524, 0.5654368418040293, 0.5685328668727107, 0.5714875944368132, 0.574361836080586, 0.5769427512591575, 0.5793197687728937, 0.5813462253891942, 0.58349609375, 0.5857640081272893, 0.5879585908882784, 0.5899009844322345, 0.5920186584249084, 0.5942203954899268, 0.5963899381868132, 0.598335908882784, 0.600503663003663, 0.6028145032051281, 0.604733645260989, 0.6066313244047619, 0.6083233173076923, 0.6102084764194139, 0.6118414463141025, 0.6132597870879121, 0.6147836538461539, 0.6161698002518315, 0.6176131810897436, 0.6188508756868132, 0.6201261303800367, 0.6215820312500001, 0.6228894803113553, 0.6241379063644689, 0.6253559266254578, 0.6264863066620878, 0.6274753891941393, 0.6281943967490842, 0.6291673820970696, 0.6300044356684982, 0.6307288089514652, 0.6315032623626374, 0.6322365785256411, 0.6329645289606227, 0.6335869534111722, 0.6343399439102564, 0.6350356999771063, 0.6355221926510989, 0.6360068967490842, 0.6365416809752747, 0.6371104481456045, 0.6376810038919415, 0.6381388793498168, 0.6386271605998168, 0.6390492645375458, 0.6395357572115385, 0.6399578611492673, 0.6402941134386447, 0.6408306862408425, 0.6412581559065934, 0.6417464371565934, 0.6421256152701466, 0.642309838598901, 0.6425620278159341, 0.6429930746336997, 0.6432881896749085, 0.6435421674679487, 0.6438569568452381, 0.6441127232142857, 0.6444060496794872, 0.6446188902243589, 0.6448585594093407, 0.6451751373626373, 0.645337897779304, 0.6455865098443223, 0.6458351219093408, 0.6459621108058609, 0.6462250314789377, 0.646418197687729, 0.6466310382326007, 0.6467884329212454, 0.6469583476419414, 0.6472981770833334, 0.647691663804945, 0.6480386475503663, 0.6483748998397436, 0.6487737522893773, 0.6491117931547619, 0.6494873941163003, 0.6497306404532968, 0.6499864068223442, 0.6502511160714286, 0.6504514365842491, 0.6506678542811355, 0.6509343521062271, 0.6511561355311355, 0.6514387305402931, 0.6517284798534799, 0.6519699376144688, 0.652272206959707, 0.6525369162087913, 0.6528982085622711, 0.6531629178113554, 0.6533686040521978, 0.6535134787087913, 0.6537531478937729, 0.6538550967261905, 0.6540196457188645, 0.6542646806318682, 0.6544220753205129, 0.6546152415293041, 0.6548459678342491, 0.6550516540750916, 0.6552573403159341, 0.6554433522206959, 0.6556132669413919, 0.6558153760302198, 0.6559584621108059, 0.6561426854395604, 0.6562804057921245, 0.656469994848901, 0.6566488524496338, 0.6568312872023808, 0.657056647779304, 0.6573320884844323, 0.6574876945970696, 0.6577023237179487, 0.6579008556547619, 0.6580707703754578, 0.6582156450320512, 0.658346211080586, 0.6584088112408426, 0.6585697830815018, 0.6587540064102563, 0.6588774181547619, 0.6590026184752746, 0.6591152987637363, 0.6592619619963369, 0.6593424479166667, 0.659415779532967, 0.6595356141254579, 0.6596125228937728, 0.6598289405906593, 0.6599791809752746, 0.6601365756639195, 0.6603619362408425, 0.6605890853937729, 0.6609199719551282, 0.6611435439560439, 0.661385001717033, 0.6614958934294871, 0.6617069453983516, 0.6618267799908425, 0.6620199461996337, 0.6622184781364469, 0.66241701007326, 0.6626674107142857, 0.6628087082188645, 0.6629875658195972, 0.6631288633241758, 0.6632093492445055], 'val_acc_list': [59.45, 60.0, 60.45, 60.75000000000001, 61.3, 62.050000000000004, 62.64999999999999, 63.3, 64.1, 64.5, 65.2, 66.2, 66.7, 67.7, 68.30000000000001, 69.3, 70.05, 70.5, 71.15, 72.0, 72.95, 74.0, 74.6, 75.3, 75.7, 75.8, 75.85, 76.05, 76.05, 76.1, 76.25, 76.25, 76.1, 76.0, 76.05, 76.1, 76.2, 76.25, 76.5, 76.4, 76.3, 76.35, 76.64999999999999, 76.44999999999999, 76.3, 76.44999999999999, 76.44999999999999, 76.7, 76.85, 76.9, 76.9, 76.95, 76.8, 76.7, 76.4, 76.35, 76.35, 76.35, 76.44999999999999, 76.2, 76.25, 76.3, 76.3, 76.25, 76.3, 76.4, 76.35, 76.3, 76.3, 76.25, 76.44999999999999, 76.44999999999999, 76.4, 76.35, 76.4, 76.35, 76.44999999999999, 76.5, 76.44999999999999, 76.44999999999999, 76.7, 76.6, 76.55, 76.7, 76.75, 76.8, 76.6, 76.64999999999999, 76.75, 76.7, 76.64999999999999, 76.7, 76.8, 76.9, 76.9, 76.9, 77.0, 77.10000000000001, 77.25, 77.3, 77.35, 77.4, 77.5, 77.60000000000001, 77.60000000000001, 77.64999999999999, 77.60000000000001, 77.64999999999999, 77.60000000000001, 77.60000000000001, 77.7, 77.75, 77.7, 77.8, 77.8, 77.75, 77.75, 77.8, 77.8, 77.8, 77.95, 77.9, 77.9, 77.9, 77.9, 77.9, 77.9, 77.85, 77.75, 77.8, 77.75, 77.8, 77.75, 77.7, 77.7, 77.75, 77.8, 77.8, 77.85, 77.9, 77.85, 77.9, 77.95, 77.9, 77.9, 77.85, 77.9, 78.0, 78.0, 78.0, 78.0, 78.05, 78.05, 78.05, 78.05, 78.14999999999999, 78.14999999999999, 78.14999999999999, 78.2, 78.25, 78.2, 78.2, 78.2, 78.3, 78.45, 78.5, 78.5, 78.55, 78.60000000000001, 78.64999999999999, 78.64999999999999, 78.75, 78.7, 78.75, 78.8, 78.8, 78.9, 78.85, 78.85, 78.8, 78.85, 78.95, 79.0, 78.95, 78.95, 78.95, 78.9, 78.95, 78.95, 79.0, 79.0, 79.0, 79.0, 79.14999999999999, 79.10000000000001, 79.3], 'val_loss_list': [1.1219607591629028, 1.1102453470230103, 1.0983128547668457, 1.0859993696212769, 1.0732134580612183, 1.0599570274353027, 1.0462149381637573, 1.0319663286209106, 1.0173035860061646, 1.0023261308670044, 0.9871202707290649, 0.9718000292778015, 0.9564793109893799, 0.9412927031517029, 0.9263640642166138, 0.9117841720581055, 0.8976671099662781, 0.8840635418891907, 0.8709955215454102, 0.8584704995155334, 0.8465164303779602, 0.8350989818572998, 0.8241650462150574, 0.8136163353919983, 0.8033742308616638, 0.7934058308601379, 0.7836788296699524, 0.7741653919219971, 0.764854907989502, 0.7557595372200012, 0.7469300031661987, 0.7384316325187683, 0.7302808165550232, 0.7224708199501038, 0.7149595022201538, 0.7077545523643494, 0.7007707357406616, 0.6939798593521118, 0.6872991323471069, 0.6806337833404541, 0.6739130020141602, 0.6672506332397461, 0.660660982131958, 0.6542432904243469, 0.6480022668838501, 0.6419059634208679, 0.6360491514205933, 0.6305093765258789, 0.62528395652771, 0.6203948855400085, 0.615788459777832, 0.611458420753479, 0.6073912382125854, 0.6036037802696228, 0.6001404523849487, 0.5969107151031494, 0.5938360691070557, 0.5908439755439758, 0.5879473686218262, 0.5851753354072571, 0.5824989080429077, 0.5799584984779358, 0.577569842338562, 0.5753571391105652, 0.5732922554016113, 0.5713891983032227, 0.5696795582771301, 0.5681481957435608, 0.5667372941970825, 0.5653570294380188, 0.5639461874961853, 0.5627192258834839, 0.5616777539253235, 0.5606760382652283, 0.559697151184082, 0.5586603283882141, 0.5576152801513672, 0.5565980672836304, 0.5555993318557739, 0.5546104907989502, 0.553738534450531, 0.5529332756996155, 0.5521048307418823, 0.551268458366394, 0.5505220293998718, 0.5499744415283203, 0.5495492219924927, 0.5491418242454529, 0.5486598610877991, 0.5481042861938477, 0.5475450158119202, 0.5470737218856812, 0.5466635823249817, 0.5462907552719116, 0.5459548830986023, 0.5456099510192871, 0.5452326536178589, 0.5449522137641907, 0.5447572469711304, 0.5445340871810913, 0.5443038940429688, 0.5441422462463379, 0.5439527034759521, 0.5437198877334595, 0.5434131622314453, 0.5431072115898132, 0.5427876710891724, 0.542466402053833, 0.5422672033309937, 0.5421146154403687, 0.5419661998748779, 0.5418785214424133, 0.5418527722358704, 0.5418068766593933, 0.5416578650474548, 0.5414535999298096, 0.5413153767585754, 0.5412904024124146, 0.5412431359291077, 0.5410953164100647, 0.5409246683120728, 0.540838360786438, 0.5408242344856262, 0.5408161878585815, 0.5408337116241455, 0.5408353209495544, 0.5408177971839905, 0.5407456159591675, 0.5406518578529358, 0.5404713749885559, 0.5402224659919739, 0.5399788618087769, 0.5397950410842896, 0.5396811962127686, 0.5395329594612122, 0.539385974407196, 0.5392776727676392, 0.5391735434532166, 0.539114236831665, 0.5391460061073303, 0.5391784906387329, 0.5392013788223267, 0.5392540693283081, 0.5393263101577759, 0.5394107699394226, 0.5395146012306213, 0.5396032929420471, 0.5396624207496643, 0.5396921634674072, 0.5396555662155151, 0.5396069288253784, 0.5395853519439697, 0.5395662188529968, 0.5396014451980591, 0.5396419763565063, 0.5396955013275146, 0.5397710204124451, 0.539810061454773, 0.5398136973381042, 0.5397531390190125, 0.5395941138267517, 0.5393989086151123, 0.5392410755157471, 0.539161741733551, 0.5391503572463989, 0.5391875505447388, 0.539247989654541, 0.5393463373184204, 0.5394824743270874, 0.5396354794502258, 0.5397812724113464, 0.5399601459503174, 0.5401800274848938, 0.5403133630752563, 0.5403868556022644, 0.5404094457626343, 0.5403648018836975, 0.5403715968132019, 0.5404356122016907, 0.5405104160308838, 0.5405747294425964, 0.5406368374824524, 0.5406953692436218, 0.5407458543777466, 0.540771484375, 0.5408115386962891, 0.5409237146377563, 0.5410485863685608, 0.5411967635154724, 0.5413369536399841, 0.5414360761642456, 0.5414915680885315, 0.5415253043174744, 0.5414963960647583, 0.5414272546768188, 0.5413737893104553], 'test_auc': 0.6683611450798892, 'test_acc': 80.10000000000001, 'test_loss': 0.5308102965354919, 'best_val_auc': 0.6533686040521978, 'best_val_acc': 77.85, 'best_val_loss': 0.539114236831665})\n",
            "('8', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210], 'train_loss_list': [2.1064717769622803, 2.0623111724853516, 2.1303043365478516, 1.9580185413360596, 1.948041558265686, 1.8707401752471924, 1.8566452264785767, 1.7419835329055786, 1.6832104921340942, 1.6232272386550903, 1.5110080242156982, 1.4539859294891357, 1.4106634855270386, 1.317203402519226, 1.1765190362930298, 1.1405396461486816, 1.039137363433838, 0.9556818604469299, 0.8672271966934204, 0.8338049054145813, 0.7557770609855652, 0.7008960247039795, 0.6861771941184998, 0.6786245107650757, 0.5933676958084106, 0.5928859114646912, 0.5968328714370728, 0.5651569962501526, 0.5429607629776001, 0.5343482494354248, 0.5408976674079895, 0.49965131282806396, 0.47384098172187805, 0.4827573001384735, 0.4548541307449341, 0.44346752762794495, 0.408203661441803, 0.40543556213378906, 0.3484967052936554, 0.4067353904247284, 0.38387611508369446, 0.3234870135784149, 0.31651732325553894, 0.3176690936088562, 0.30159321427345276, 0.2880185842514038, 0.27688759565353394, 0.28449639678001404, 0.2599871754646301, 0.24015672504901886, 0.22909444570541382, 0.21972976624965668, 0.20749343931674957, 0.19940128922462463, 0.19515886902809143, 0.17679931223392487, 0.1902676373720169, 0.1634313464164734, 0.15722882747650146, 0.15254196524620056, 0.14652621746063232, 0.15464214980602264, 0.13702364265918732, 0.1298859864473343, 0.13612616062164307, 0.11785608530044556, 0.11253681033849716, 0.10471627116203308, 0.10291855782270432, 0.09674599766731262, 0.09913323074579239, 0.09252044558525085, 0.08751370757818222, 0.08768533915281296, 0.08333496004343033, 0.08213318139314651, 0.07651077210903168, 0.07337009161710739, 0.07513383030891418, 0.06930684298276901, 0.06795816868543625, 0.0643559917807579, 0.06593305617570877, 0.06348409503698349, 0.06668411940336227, 0.06424453109502792, 0.05785703286528587, 0.059945497661828995, 0.05945703014731407, 0.05538380518555641, 0.05136628448963165, 0.05307414010167122, 0.0530288890004158, 0.05061272159218788, 0.05463322624564171, 0.050178200006484985, 0.0499001182615757, 0.04611106961965561, 0.05124375969171524, 0.04225676506757736, 0.04390965774655342, 0.04359782114624977, 0.04358019679784775, 0.04343809559941292, 0.041968923062086105, 0.041033510118722916, 0.03911420330405235, 0.041288211941719055, 0.0388769805431366, 0.03881879895925522, 0.03810594975948334, 0.036728162318468094, 0.03440936654806137, 0.03548566997051239, 0.038116347044706345, 0.03767084330320358, 0.03310699388384819, 0.03524046391248703, 0.0328928604722023, 0.0368218831717968, 0.036379922181367874, 0.030210664495825768, 0.03533394634723663, 0.029207874089479446, 0.0340692438185215, 0.031706977635622025, 0.03240310773253441, 0.03161543607711792, 0.026768488809466362, 0.030712014064192772, 0.028459293767809868, 0.03038058988749981, 0.03027050569653511, 0.025695452466607094, 0.0293599721044302, 0.02888183295726776, 0.026456523686647415, 0.027155712246894836, 0.027552835643291473, 0.026193834841251373, 0.027401842176914215, 0.0252375528216362, 0.02494793012738228, 0.025974776595830917, 0.025715436786413193, 0.025227395817637444, 0.026263393461704254, 0.025058677420020103, 0.026726610958576202, 0.023746313527226448, 0.024257339537143707, 0.023181108757853508, 0.024089394137263298, 0.025197915732860565, 0.02502189576625824, 0.023430729284882545, 0.025733277201652527, 0.02448834851384163, 0.024887653067708015, 0.021177880465984344, 0.022564269602298737, 0.020899685099720955, 0.02302636206150055, 0.02304012142121792, 0.020891254767775536, 0.02183600515127182, 0.02164372056722641, 0.0212662722915411, 0.02316906861960888, 0.019662540405988693, 0.021920597180724144, 0.01884087733924389, 0.021228009834885597, 0.022375183179974556, 0.02054155059158802, 0.020042918622493744, 0.01942826248705387, 0.022268226370215416, 0.021509673446416855, 0.020788365975022316, 0.01937737688422203, 0.019969133660197258, 0.019543111324310303, 0.020056521520018578, 0.0212570670992136, 0.017985858023166656, 0.01983417570590973, 0.017905892804265022, 0.019702624529600143, 0.018483882769942284, 0.017496244981884956, 0.016836965456604958, 0.01917913556098938, 0.01647910289466381, 0.01977819949388504, 0.02141290158033371, 0.01816781982779503, 0.01771708019077778, 0.020162351429462433, 0.016329223290085793, 0.01970871537923813, 0.017061371356248856, 0.018091391772031784, 0.018838033080101013, 0.017457671463489532, 0.01703561283648014, 0.018903642892837524, 0.018632037565112114, 0.015982910990715027, 0.01783445104956627, 0.01489759236574173], 'val_auc_list': [0.4867089003924188, 0.48689154212240127, 0.4870515710667669, 0.48734379783473875, 0.48766211627842254, 0.48809001976009575, 0.4885474938075757, 0.48920674348056, 0.4899790570816286, 0.49088878684144605, 0.49203856001781193, 0.49334314380340094, 0.49494691213715175, 0.4966620049539395, 0.4988206562578275, 0.5012941471153043, 0.5038354763296318, 0.5067681806796359, 0.5102575074448248, 0.5144165205532827, 0.5188294926386685, 0.5238043054743815, 0.529100915643873, 0.5347923798391362, 0.5405186329353483, 0.546554507250007, 0.5524581837410593, 0.55850623417105, 0.5639211266037684, 0.5693481951518188, 0.5747978764854861, 0.5795430825749353, 0.5841960980768696, 0.5886473379532994, 0.5929611616709805, 0.5966887924076703, 0.6002250841891403, 0.6035491636748211, 0.6064088113328323, 0.6090875567059084, 0.6116236675850936, 0.6139092983774457, 0.6160244635551473, 0.617958726447914, 0.619859939884779, 0.6215332860204279, 0.6235005983690963, 0.6252922267679719, 0.6270055801397122, 0.6287502435223066, 0.6302653001586374, 0.6318447162617239, 0.6333354206673902, 0.6350331190337034, 0.6365864434610782, 0.6378858089115249, 0.6394652250146113, 0.6406323926414517, 0.6421161392669283, 0.6433180957947177, 0.6443478472628092, 0.645304542038908, 0.6463082018312878, 0.6472666360524338, 0.6481850630374885, 0.6492409061812919, 0.6499488603156048, 0.6507559628176227, 0.6514012969302273, 0.652069243828449, 0.6527006623806741, 0.6533303414878517, 0.6538452172218976, 0.6544922907795496, 0.6551289276669171, 0.6557151206479085, 0.6562212991567171, 0.656767484901617, 0.6573014945311848, 0.6576772146614345, 0.6578842086220812, 0.6580772870223485, 0.6583190698839443, 0.6585347610698282, 0.658602599426679, 0.6587695861512343, 0.6588843895243661, 0.6590826862597756, 0.6592775041050902, 0.6594653641702152, 0.6595732097631573, 0.6597210625921905, 0.6599193593276002, 0.6600219865853998, 0.6601750577495755, 0.6602620300019482, 0.6604098828309816, 0.6606029612312488, 0.6607595112855195, 0.66092301911998, 0.6611178369652946, 0.6611891542122401, 0.6614657259747851, 0.6615561771172525, 0.6616935932760012, 0.6618501433302719, 0.6620797500765356, 0.6622502156911858, 0.6624833013275445, 0.6626207174862931, 0.6628833736884585, 0.6630712337535833, 0.6631408115554813, 0.663332150510701, 0.6634678272244023, 0.6635582783668698, 0.6636104617182934, 0.6636122011633407, 0.6636122011633409, 0.6637026523058083, 0.6637530962121845, 0.6638418079096046, 0.6639113857115027, 0.6639705268431161, 0.663989660738638, 0.6639879212935905, 0.6641949152542372, 0.6642436197155659, 0.6643305919679385, 0.6644471347861178, 0.6645010575825888, 0.6646628259720019, 0.6647497982243744, 0.664878517157886, 0.6650385461022515, 0.6651759622610002, 0.6652524978430882, 0.6653951323369792, 0.665516893490301, 0.6657465002365646, 0.6658995714007403, 0.6660074169936824, 0.6660387270045365, 0.6661239598118617, 0.6663118198769865, 0.6663918343491693, 0.6665240321727757, 0.6666840611171412, 0.6668927945228355, 0.6670789151429128, 0.6671850212908075, 0.6673241768946035, 0.667475508613732, 0.6676094458823856, 0.6677120731401853, 0.6678268765133172, 0.6678929754251204, 0.6679973421279675, 0.6680843143803401, 0.6681625894074754, 0.6682739138905123, 0.6683852383735494, 0.6684774289610642, 0.6686113662297182, 0.6685991901143858, 0.6686879018118059, 0.6687522612785617, 0.6688270574156021, 0.6688235785255072, 0.6688531490913139, 0.6688514096462664, 0.6689044627202139, 0.6689923046951101, 0.6690740586123404, 0.6691627703097605, 0.6693227992541261, 0.6693541092649801, 0.669446299852495, 0.6695837160112439, 0.6696776460438063, 0.6697576605159891, 0.6698220199827447, 0.6699403022459713, 0.6700777184047201, 0.6701803456625198, 0.6702812334752719, 0.670354290167265, 0.6704743118755393, 0.6706065096991456, 0.6707004397317079, 0.6707630597534163, 0.6708204614399822, 0.6708865603517854, 0.6710309342907239, 0.6711213854331914, 0.6711700898945201, 0.671208357685564, 0.6713301188388856, 0.671424048871448, 0.6714779716679191, 0.6715492889148647, 0.6715684228103866, 0.6716240850519051, 0.6716884445186607, 0.6717615012106538, 0.6717962901116028, 0.6719215301550192, 0.6720154601875817, 0.6721093902201443, 0.672116348000334, 0.6721528763463304], 'val_acc_list': [38.35, 38.95, 39.6, 40.1, 40.699999999999996, 41.199999999999996, 42.05, 42.8, 43.7, 45.25, 46.550000000000004, 48.199999999999996, 49.55, 51.5, 52.949999999999996, 54.900000000000006, 57.9, 60.199999999999996, 62.150000000000006, 64.55, 66.4, 68.65, 70.35, 71.7, 73.5, 74.75, 75.14999999999999, 75.75, 76.14999999999999, 76.4, 76.7, 76.9, 77.05, 77.0, 76.85, 76.44999999999999, 76.44999999999999, 76.1, 76.2, 76.4, 76.2, 76.0, 76.14999999999999, 76.1, 75.94999999999999, 75.6, 75.5, 75.55, 75.5, 75.5, 75.6, 75.9, 75.75, 75.75, 76.1, 76.4, 76.55, 76.6, 76.64999999999999, 76.75, 76.55, 76.55, 76.5, 76.7, 76.64999999999999, 76.5, 76.5, 76.44999999999999, 76.55, 76.64999999999999, 76.6, 76.64999999999999, 76.64999999999999, 76.64999999999999, 76.8, 76.85, 76.95, 77.05, 77.2, 77.2, 77.10000000000001, 77.10000000000001, 77.2, 77.25, 77.25, 77.2, 77.25, 77.10000000000001, 77.10000000000001, 77.14999999999999, 77.14999999999999, 77.14999999999999, 77.3, 77.3, 77.2, 77.3, 77.3, 77.35, 77.35, 77.45, 77.4, 77.4, 77.55, 77.64999999999999, 77.8, 77.95, 78.0, 78.0, 78.0, 78.05, 78.0, 78.0, 78.0, 78.0, 78.05, 78.14999999999999, 78.2, 78.14999999999999, 78.14999999999999, 78.14999999999999, 78.2, 78.2, 78.2, 78.25, 78.25, 78.3, 78.3, 78.25, 78.3, 78.35, 78.4, 78.45, 78.5, 78.45, 78.45, 78.5, 78.55, 78.55, 78.55, 78.60000000000001, 78.60000000000001, 78.64999999999999, 78.7, 78.7, 78.75, 78.75, 78.85, 78.95, 79.0, 78.95, 78.95, 78.9, 78.9, 78.9, 78.95, 78.9, 78.9, 78.8, 78.8, 78.75, 78.8, 78.8, 78.85, 79.0, 79.05, 79.05, 79.05, 79.05, 79.0, 79.0, 79.0, 79.05, 79.10000000000001, 79.14999999999999, 79.2, 79.2, 79.3, 79.3, 79.35, 79.4, 79.45, 79.45, 79.45, 79.45, 79.45, 79.45, 79.45, 79.45, 79.4, 79.35, 79.3, 79.3, 79.3, 79.3, 79.3, 79.25, 79.3, 79.3, 79.35, 79.35, 79.4, 79.4, 79.4, 79.5, 79.45, 79.4, 79.35, 79.4, 79.4, 79.45, 79.5], 'val_loss_list': [2.1409318447113037, 2.102163791656494, 2.0608723163604736, 2.016119956970215, 1.9673129320144653, 1.914066195487976, 1.8562012910842896, 1.7937288284301758, 1.7268682718276978, 1.6559901237487793, 1.5815659761428833, 1.5043648481369019, 1.425432562828064, 1.3458956480026245, 1.2669086456298828, 1.1898819208145142, 1.1163007020950317, 1.0475994348526, 0.9851320385932922, 0.9299408197402954, 0.8826033473014832, 0.8432518243789673, 0.8114625811576843, 0.7863238453865051, 0.7665854096412659, 0.7509669065475464, 0.7381592392921448, 0.7270477414131165, 0.7168827056884766, 0.7072011828422546, 0.6977202892303467, 0.6883769631385803, 0.6792495846748352, 0.6704518795013428, 0.6621523499488831, 0.6544626355171204, 0.6473959684371948, 0.640975296497345, 0.635161280632019, 0.62986159324646, 0.6249813437461853, 0.6204594969749451, 0.6161598563194275, 0.6120561361312866, 0.6081146597862244, 0.6042912602424622, 0.6005080938339233, 0.5968099236488342, 0.5932955741882324, 0.5899233222007751, 0.586645245552063, 0.5834935307502747, 0.5805127024650574, 0.5777105689048767, 0.5751162767410278, 0.5727181434631348, 0.5704697966575623, 0.5683614611625671, 0.5663928389549255, 0.5645871758460999, 0.562970757484436, 0.5615168809890747, 0.5601957440376282, 0.5589765906333923, 0.5578504204750061, 0.5567967295646667, 0.5558328628540039, 0.554949939250946, 0.5541077852249146, 0.5533286333084106, 0.5526014566421509, 0.5518987774848938, 0.5512118339538574, 0.5505395531654358, 0.5498858094215393, 0.5492550134658813, 0.5486827492713928, 0.5481784343719482, 0.5477378964424133, 0.5473491549491882, 0.5470321178436279, 0.5467672348022461, 0.546531081199646, 0.5463038086891174, 0.5460949540138245, 0.5459359288215637, 0.5457856059074402, 0.5456481575965881, 0.5455490946769714, 0.5454819798469543, 0.5454279780387878, 0.5453810691833496, 0.5453237295150757, 0.5452759265899658, 0.545242428779602, 0.5452049374580383, 0.5451597571372986, 0.5451042652130127, 0.545015811920166, 0.5449161529541016, 0.5448076725006104, 0.5446975827217102, 0.5446045994758606, 0.544524610042572, 0.5444561839103699, 0.5443857312202454, 0.5443081259727478, 0.5442503690719604, 0.5441965460777283, 0.5441498160362244, 0.5441358089447021, 0.5441247224807739, 0.5441207885742188, 0.5441296100616455, 0.5441707372665405, 0.5442396998405457, 0.544325053691864, 0.5444190502166748, 0.5445189476013184, 0.5446125268936157, 0.5446889400482178, 0.5447471737861633, 0.544809877872467, 0.544882595539093, 0.544974684715271, 0.5450610518455505, 0.5451340079307556, 0.5452132821083069, 0.5453222990036011, 0.5454216599464417, 0.545484185218811, 0.5455546975135803, 0.5456448197364807, 0.5457409024238586, 0.5458419322967529, 0.5459444522857666, 0.5460400581359863, 0.5461173057556152, 0.5461865067481995, 0.5462585091590881, 0.5463417172431946, 0.5464578866958618, 0.5465788841247559, 0.5466973185539246, 0.5467915534973145, 0.5468774437904358, 0.5469575524330139, 0.5470389127731323, 0.5471042394638062, 0.5471581816673279, 0.5472322106361389, 0.5473108887672424, 0.5474109053611755, 0.5475134253501892, 0.5476158857345581, 0.5477261543273926, 0.5478648543357849, 0.5480188727378845, 0.5481809973716736, 0.5483537912368774, 0.5485141277313232, 0.5486391186714172, 0.5487709641456604, 0.5489086508750916, 0.5490362048149109, 0.5491985082626343, 0.5493906736373901, 0.5495690703392029, 0.5497279167175293, 0.5498794913291931, 0.5500259399414062, 0.5501740574836731, 0.5503107905387878, 0.5504252314567566, 0.5505348443984985, 0.5506414175033569, 0.550769567489624, 0.5508956909179688, 0.5510032176971436, 0.5511013865470886, 0.5512292385101318, 0.5513864755630493, 0.5515468120574951, 0.5517101287841797, 0.5518934726715088, 0.5521218776702881, 0.5523472428321838, 0.5525308847427368, 0.5527213215827942, 0.5529184937477112, 0.5531153678894043, 0.5533040165901184, 0.5534704327583313, 0.5536285638809204, 0.5537682771682739, 0.5538970828056335, 0.5540174245834351, 0.5541388988494873, 0.5542893409729004, 0.5544864535331726, 0.554716944694519, 0.5549684166908264, 0.5552452206611633, 0.5555265545845032, 0.5557667016983032, 0.5559840798377991, 0.5562040209770203, 0.5564095377922058, 0.5566449761390686, 0.5568841695785522, 0.5570985674858093], 'test_auc': 0.6787650932942682, 'test_acc': 79.85, 'test_loss': 0.545834481716156, 'best_val_auc': 0.6631408115554813, 'best_val_acc': 78.0, 'best_val_loss': 0.5441207885742188})\n",
            "('16', {'EPOCH': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], 'train_loss_list': [1.7966684103012085, 1.8134794235229492, 1.7187658548355103, 1.667366623878479, 1.6171011924743652, 1.5680714845657349, 1.4473965167999268, 1.3883662223815918, 1.2890334129333496, 1.1883735656738281, 1.0250108242034912, 0.9685665369033813, 0.8798809051513672, 0.8417249321937561, 0.7538907527923584, 0.7263321876525879, 0.6737320423126221, 0.5930932760238647, 0.621632993221283, 0.5718693733215332, 0.5627561211585999, 0.5294597744941711, 0.5137858390808105, 0.5074300765991211, 0.4852225184440613, 0.48012974858283997, 0.4566013216972351, 0.4255712926387787, 0.3892737627029419, 0.35664260387420654, 0.33965203166007996, 0.3290511965751648, 0.3119308054447174, 0.2806646525859833, 0.2731309235095978, 0.2510063946247101, 0.2275795191526413, 0.2156156599521637, 0.21957500278949738, 0.18109185993671417, 0.18309997022151947, 0.17336517572402954, 0.1576305329799652, 0.15743209421634674, 0.1531011462211609, 0.13902521133422852, 0.12078738212585449, 0.11318141967058182, 0.10935908555984497, 0.10181137174367905, 0.10288816690444946, 0.09386209398508072, 0.0873262882232666, 0.07948288321495056, 0.08059767633676529, 0.07430829852819443, 0.06984318792819977, 0.06514625251293182, 0.06304129958152771, 0.06826916337013245, 0.06611595302820206, 0.06311340630054474, 0.05723061412572861, 0.05928666889667511, 0.051174506545066833, 0.04838940501213074, 0.05165539309382439, 0.05336045101284981, 0.04938457906246185, 0.04674641788005829, 0.04641294479370117, 0.045575495809316635, 0.04196054860949516, 0.039594851434230804, 0.03974725306034088, 0.04083045944571495, 0.039980314671993256, 0.04252905398607254, 0.040936652570962906, 0.039140235632658005, 0.03690476715564728, 0.03374245762825012, 0.03438778221607208, 0.036799781024456024, 0.038192495703697205, 0.035630982369184494, 0.033839013427495956, 0.0320136733353138, 0.033778637647628784, 0.03364923223853111, 0.03216157853603363, 0.03222239017486572, 0.03061678819358349, 0.029458479955792427, 0.031161582097411156, 0.028032837435603142, 0.0340534970164299, 0.025340188294649124, 0.02764289081096649, 0.028685366734862328, 0.0274793840944767, 0.026664042845368385, 0.023007690906524658, 0.028861667960882187, 0.026233147829771042, 0.026059016585350037, 0.025954756885766983, 0.022776447236537933, 0.026850976049900055, 0.025479085743427277, 0.025574469938874245, 0.023829646408557892, 0.023786291480064392, 0.025157522410154343, 0.02755402959883213, 0.02190578170120716, 0.02478426694869995, 0.023833610117435455, 0.02204619161784649, 0.02166169323027134, 0.022962234914302826, 0.021613197401165962, 0.022027568891644478, 0.020201751962304115, 0.02325942926108837, 0.021587073802947998, 0.02162882313132286, 0.019385093823075294, 0.019289210438728333, 0.018469268456101418, 0.02141762711107731, 0.019823694601655006, 0.017014486715197563, 0.019815128296613693, 0.01941533014178276, 0.020904958248138428, 0.019940335303544998, 0.01858055777847767, 0.017657563090324402, 0.0170125849545002, 0.01933889463543892, 0.017850879579782486, 0.019757065922021866, 0.01660839468240738, 0.015326499938964844, 0.017221588641405106, 0.01844821870326996, 0.016398491337895393, 0.015914758667349815, 0.01675429940223694, 0.017854338511824608, 0.01730567030608654, 0.017695695161819458, 0.01784476451575756, 0.016774971038103104, 0.018225615844130516], 'val_auc_list': [0.51215274976469, 0.5126421944332392, 0.5132571377347497, 0.5140836358746806, 0.5150212899466631, 0.5162942046524135, 0.5177679171709022, 0.5194065707498543, 0.5214414414414414, 0.5241145623235176, 0.5271086011384518, 0.5306404912375062, 0.5351046568957016, 0.540006274931648, 0.5456949486800233, 0.551656133745686, 0.558696607054816, 0.5660167630316885, 0.5732383129398055, 0.5805172336515619, 0.5872618887544261, 0.5936219801891444, 0.5991717090224552, 0.6043494240509165, 0.6087866971449061, 0.6125767558603379, 0.6160996817713236, 0.619179776791717, 0.6217417417417417, 0.6238214333736722, 0.6255676572094483, 0.626819057863834, 0.6278033257137734, 0.6284021334767604, 0.6290529335305455, 0.6292913809331719, 0.6294473578055668, 0.6296051275155753, 0.6295836134642104, 0.6293236520102192, 0.6293684729505625, 0.6292725561382277, 0.6290923759580476, 0.6291318183855498, 0.6291533324369145, 0.6290852046075927, 0.6289238492223567, 0.6288234503159876, 0.6288933709829232, 0.6289184707095156, 0.6288162789655327, 0.6287320155976873, 0.6288790282820134, 0.6288377930168975, 0.6287320155976873, 0.6287409797857559, 0.6284971538702881, 0.6284146833400566, 0.628256913630048, 0.6282676706557304, 0.6282533279548205, 0.6283644838868719, 0.6284935681950606, 0.6285491461610865, 0.6286818161445027, 0.6288413786921249, 0.6290260409663394, 0.6292268387790776, 0.6293756443010174, 0.6295110035408542, 0.6297557258751288, 0.6299475594997983, 0.6301429787996952, 0.6302666845950428, 0.6304459683564161, 0.6306162879297207, 0.6308385997938236, 0.6310429832817892, 0.6311899959661154, 0.6313513513513512, 0.6315790417282954, 0.6317672896777375, 0.631937609251042, 0.6320057370803639, 0.6320774505849133, 0.6322065348931021, 0.6324001613553852, 0.6325812379543724, 0.6328035498184752, 0.6330079333064407, 0.6331567388283805, 0.6334041504190759, 0.6335762628299942, 0.6337358253776164, 0.6338362242839854, 0.634019990139393, 0.6341786562682085, 0.6343238761149208, 0.6345264667652728, 0.6347308502532384, 0.6349495764421138, 0.6352274662722424, 0.6354085428712295, 0.6356236833848774, 0.6358459952489803, 0.6361328492671777, 0.6363712966698042, 0.6365685088073147, 0.6367979920218726, 0.6369790686208597, 0.6370687105015463, 0.6371906234592802, 0.6372157231858725, 0.6372569584509883, 0.6373824570839497, 0.6375527766572543, 0.6377338532562413, 0.637897001479091, 0.6381013849670567, 0.6382932185917259, 0.6385298731567388, 0.6386482004392452, 0.6387450136703868, 0.6388131414997087, 0.6388884406794855, 0.638945811483125, 0.6390408318766528, 0.6392165299627987, 0.6393725068351934, 0.6396360539644121, 0.6397776881358971, 0.6399838644614764, 0.6401918336246695, 0.6403280892833131, 0.6405396441217336, 0.6406974138317422, 0.640892833131639, 0.6412047868764287, 0.6413858634754158, 0.6415902469633814, 0.6417677378871408, 0.6419506073237417, 0.642170229931424, 0.6423055891712608, 0.6425691363004795, 0.642829097754471, 0.6430496167809601, 0.6432396575680157, 0.6434404553807539, 0.6435856752274662, 0.6437990229035006, 0.644051813007037, 0.6442830890592085, 0.6444390659316033, 0.6445896642911568, 0.644763569539689], 'val_acc_list': [42.25, 42.9, 43.6, 44.5, 45.9, 47.199999999999996, 49.7, 51.349999999999994, 54.05, 55.95, 58.85, 61.45, 64.25, 67.35, 70.05, 73.3, 75.64999999999999, 76.95, 78.3, 78.60000000000001, 79.2, 79.55, 79.65, 79.9, 79.7, 79.55, 79.25, 78.9, 78.85, 78.64999999999999, 78.35, 77.95, 77.55, 77.05, 77.14999999999999, 76.8, 77.10000000000001, 77.35, 77.8, 77.85, 77.85, 78.0, 77.95, 77.95, 77.95, 77.95, 77.95, 78.0, 77.8, 77.9, 77.9, 78.0, 78.2, 78.14999999999999, 78.10000000000001, 78.05, 78.2, 78.3, 78.35, 78.45, 78.35, 78.3, 78.4, 78.55, 78.45, 78.45, 78.4, 78.4, 78.5, 78.55, 78.55, 78.64999999999999, 78.60000000000001, 78.75, 78.75, 78.7, 78.60000000000001, 78.60000000000001, 78.64999999999999, 78.60000000000001, 78.7, 79.0, 79.05, 79.10000000000001, 79.14999999999999, 79.14999999999999, 79.25, 79.2, 79.2, 79.2, 79.2, 79.14999999999999, 79.14999999999999, 79.2, 79.2, 79.2, 79.3, 79.2, 79.2, 79.25, 79.25, 79.3, 79.35, 79.45, 79.45, 79.5, 79.60000000000001, 79.55, 79.55, 79.7, 79.65, 79.7, 79.7, 79.85, 79.85, 79.85, 79.85, 79.85, 79.9, 79.95, 79.9, 79.95, 79.95, 80.05, 80.05, 80.0, 80.0, 80.0, 80.05, 80.10000000000001, 80.25, 80.25, 80.25, 80.35, 80.4, 80.35, 80.35, 80.35, 80.30000000000001, 80.30000000000001, 80.30000000000001, 80.30000000000001, 80.4, 80.4, 80.5, 80.55, 80.55, 80.65, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.80000000000001, 80.85, 80.9, 80.85, 80.7], 'val_loss_list': [1.8134381771087646, 1.7620099782943726, 1.7065614461898804, 1.645681381225586, 1.5786247253417969, 1.5053797960281372, 1.426501989364624, 1.3430932760238647, 1.2568610906600952, 1.1699988842010498, 1.0850765705108643, 1.0050290822982788, 0.9325435757637024, 0.8699042797088623, 0.818702757358551, 0.7792697548866272, 0.750676155090332, 0.7308685183525085, 0.7170432806015015, 0.7064073085784912, 0.6968823075294495, 0.6871287226676941, 0.6767634153366089, 0.6658936738967896, 0.6549128890037537, 0.6444442272186279, 0.6349917650222778, 0.6269195675849915, 0.6203745603561401, 0.6152768731117249, 0.6113094091415405, 0.6080747246742249, 0.605166494846344, 0.6023460626602173, 0.5994855165481567, 0.5964989066123962, 0.5934098362922668, 0.5903153419494629, 0.587310254573822, 0.584472119808197, 0.5818706750869751, 0.5795863270759583, 0.5776458382606506, 0.5760093927383423, 0.5746880769729614, 0.5736599564552307, 0.5729238390922546, 0.572433590888977, 0.5721240043640137, 0.5719377398490906, 0.571823000907898, 0.571743905544281, 0.5716373324394226, 0.5714944005012512, 0.5713467001914978, 0.5711281895637512, 0.570835530757904, 0.5704819560050964, 0.5701488256454468, 0.5698400735855103, 0.5695989727973938, 0.5693933367729187, 0.5692183971405029, 0.569074809551239, 0.5689483284950256, 0.5687955617904663, 0.5686019659042358, 0.5683794021606445, 0.5682304501533508, 0.5681490302085876, 0.5680841207504272, 0.568006694316864, 0.567972719669342, 0.5679602026939392, 0.5679472088813782, 0.5679382085800171, 0.5679455399513245, 0.5679681301116943, 0.5679821968078613, 0.5679866671562195, 0.5680148601531982, 0.5680938959121704, 0.5681775808334351, 0.568248450756073, 0.5683454275131226, 0.5684597492218018, 0.568602442741394, 0.5687842965126038, 0.5689902305603027, 0.5692042708396912, 0.5694114565849304, 0.569618284702301, 0.569807231426239, 0.5700251460075378, 0.5702185034751892, 0.5703268051147461, 0.5704328417778015, 0.5705781579017639, 0.5707160830497742, 0.5708473920822144, 0.5709912180900574, 0.5711339116096497, 0.5712537169456482, 0.5713773369789124, 0.5715107321739197, 0.5716606974601746, 0.5717992782592773, 0.571948766708374, 0.5721026062965393, 0.5722867846488953, 0.5724937915802002, 0.5727114081382751, 0.5729295611381531, 0.5731261372566223, 0.5732879638671875, 0.5734381079673767, 0.5735961198806763, 0.5737598538398743, 0.5739433765411377, 0.5741299986839294, 0.5743698477745056, 0.5746396780014038, 0.5749008059501648, 0.5751876831054688, 0.575480043888092, 0.5757481455802917, 0.5759896039962769, 0.5762050747871399, 0.5763906240463257, 0.5765523314476013, 0.5767432451248169, 0.5769526362419128, 0.5771278738975525, 0.5772801041603088, 0.5774399042129517, 0.5776392221450806, 0.5778771638870239, 0.5781306624412537, 0.5783628821372986, 0.5785894989967346, 0.5788315534591675, 0.5790751576423645, 0.5792768597602844, 0.5794724225997925, 0.5796533226966858, 0.5798179507255554, 0.5800014138221741, 0.5802083015441895, 0.5804235935211182, 0.580630362033844, 0.5808219313621521, 0.5809929370880127, 0.5811901688575745, 0.5814027190208435, 0.5815849900245667, 0.5817633271217346], 'test_auc': 0.6797437709010409, 'test_acc': 81.15, 'test_loss': 0.5238107442855835, 'best_val_auc': 0.6306162879297207, 'best_val_acc': 78.7, 'best_val_loss': 0.5679382085800171})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3zU9f3A8df77nLZew8CYYe9EQUE\nFXAPwEFdVC3aaq211lGr/jpstWoVK7Vq3dZatygoIoILGWHICCCbDEYC2euSu8/vjzsgQBIuyV0S\n4P3s43rf+97n+/28aeH7vu/n+xlijEEppZRqjKW9A1BKKdWxaaJQSinVJE0USimlmqSJQimlVJM0\nUSillGqSrb0D8LW4uDjTpUuX9g5DKaVOKCtWrCg0xsQ39N1Jlyi6dOlCVlZWe4ehlFInFBHZ2dh3\n2vSklFKqSZoolFJKNUkThVJKqSaddM8olFLK32pra8nNzaW6urq9Q2m2oKAg0tLSCAgI8PoYTRRK\nKdVMubm5hIeH06VLF0SkvcPxmjGG/fv3k5ubS0ZGhtfHadOTUko1U3V1NbGxsSdUkgAQEWJjY5t9\nJ6SJQimlWuBESxIHtSTudk0UInKuiGwSkS0icm8D308XkQIRWe153eSvWIzLxd6/PYYjN9dfVSil\n1Amp3RKFiFiBWcB5QB9gmoj0aaDo/4wxgzyvf/srHsfOnRS/+y7bp0yl/Ouv/VWNUkr5xA033EBC\nQgL9+vU7Yv8//vEPevfuTd++fbn77rt9Uld73lGMALYYY7YZYxzAW8Al7RVMYEYGGe++Q0BSEjk3\n30LBrFkYl6u9wlFKqSZNnz6dzz777Ih9Cxcu5KOPPuKHH35g/fr13HXXXT6pqz0TRSqQU+9zrmff\n0aaIyBoReVdEOvkzIHt6Ol3e+i8RF11I4T+eIe/O3+A6Abu/KaVOfmPHjiUmJuaIfc8++yz33nsv\ngYGBACQkJPikro7ePfZj4L/GmBoRuRl4FTjr6EIiMgOYAZCent6qCi3BwaQ8+ihBvXqz7/HH2bk7\nn06zZmGLi2vVeZVSJ6c/fLye7PxSn56zT0oED13Ut9nH/fjjj3zzzTfcf//9BAUF8fjjjzN8+PBW\nx9OedxR5QP07hDTPvkOMMfuNMTWej/8GhjZ0ImPM88aYYcaYYfHxDU5+2CwiQuyNN5D69ExqNv3I\njiuupGb79lafVyml/Kmuro4DBw6wZMkSHnvsMa644gqMMa0+b3veUSwHeohIBu4EcRXwk/oFRCTZ\nGLPb8/FiYENbBhgxYQIBr79Ozs03s/Pa60h/6UWCevZsyxCUUh1cS375+0taWhqTJ09GRBgxYgQW\ni4XCwkJa+wO63e4ojDF1wG3APNwJ4G1jzHoR+aOIXOwpdruIrBeRH4DbgeltHWdw/350fv01xGJh\n17XXUbVufVuHoJRSXrn00ktZuHAh4G6GcjgcxPmg2bxdx1EYY+YaY3oaY7oZYx727HvQGDPbs32f\nMaavMWagMWa8MWZje8QZ2K0bnd94HUtoKLumT6dqvSYLpVT7mjZtGqNGjWLTpk2kpaXx4osvcsMN\nN7Bt2zb69evHVVddxauvvuqTgYHii/arjmTYsGHGXwsX1e7ezY6rr8ZU19D5P28Q2Iy5UpRSJ48N\nGzaQmZnZ3mG0WEPxi8gKY8ywhsrrFB7NEJCcTPqLLwKw68Ybqd2zp50jUkop/9NE0UyBGRl0euF5\nXCWl7LrpJpxlZe0dklJK+ZUmihYI7tuXtFmzcOzYSf5dv8U4ne0dklJK+Y0mihYKPW0kSff/jvKv\nvqLgqZntHY5SSvlNRx+Z3aFFT5tG9cZN7H/hBQJ79SLywgvaOySllPI5vaNopaT7f0fwsKHsfuAB\narbp6G2l1MlHE0Urid1O6hN/xxIYSP5dd2EcjvYOSSl1CnjyySfp27cv/fr1Y9q0aX5dv1sThQ8E\nJCaQ/PCfqc7OZt9MfV6hlPKvvLw8nn76abKysli3bh1Op5O33nrLb/VpovCR8LPPJurKKznw4ktU\nLF7c3uEopU5ydXV1VFVVUVdXR2VlJSkpKX6rSx9m+1DivfdQuXw5+b+7n66ffII1LLS9Q1JK+dun\n98Ketb49Z1J/OO+RRr9OTU3lrrvuIj09neDgYCZOnMjEiRN9G0M9ekfhQ5bgYFL+8jB1e/dSoE1Q\nSik/KSoq4qOPPmL79u3k5+dTUVHBG2+84bf69I7Cx4IHDSJ62jSK3niDyIsuJHjAgPYOSSnlT038\n8veXL774goyMjEPTh0+ePJnFixdzzTXX+KU+vaPwg/g7f40tPp7dDz6Eqa1t73CUUieZ9PR0lixZ\nQmVlJcYYFixY4NdJCjVR+IE1LIzE399PzcaNHHj11fYORyl1khk5ciRTp05lyJAh9O/fH5fLxYwZ\nM/xWn04z7ifGGHJ//gsqly2j2+fzdM1tpU4iOs14GxKRc0Vkk4hsEZF7myg3RUSMiDT4h+iIRISE\nu+/G5XBQ8Mwz7R2OUkq1WLslChGxArOA84A+wDQR6dNAuXDgV8DSto2w9QK7ZhB95ZUUv/MuNVu2\ntHc4SinVIu15RzEC2GKM2WaMcQBvAZc0UO5PwKOA/8an+1Hcrb/AEhzMvsefaO9QlFKqRdozUaQC\nOfU+53r2HSIiQ4BOxpg5TZ1IRGaISJaIZBUUFPg+0lawxcQQe/MMyhctomLJkvYORymlmq3D9noS\nEQvwd+A3xytrjHneGDPMGDPsYL/ijiTmuuuwpSSz78knOdk6DyilTn7tmSjygE71Pqd59h0UDvQD\nFonIDuA0YPaJ9ED7IEtgIHEzbqb6hzVUfKfzQCmlTiztmSiWAz1EJENE7MBVwOyDXxpjSowxccaY\nLsaYLsAS4GJjTPv3fW2ByMmXYUtKonDWLL2rUEr5hNPpZPDgwVx44YV+rafdEoUxpg64DZgHbADe\nNsasF5E/isjF7RWXv1jsdmJn/IyqVauoXHrCdeBSSnVAM2fObJPxHO36jMIYM9cY09MY080Y87Bn\n34PGmNkNlB13ot5NHBQ1ZQq2hAQKn5nV3qEopU5wubm5zJkzh5tuusnvdemkgG3IEhhI7E03sfcv\nf6Fi6TJCR45o75CUUq306LJH2Xhgo0/P2TumN/eMuKfJMnfccQd/+9vfKCsr82ndDemwvZ5OVlFX\nXI41Npb9L73Y3qEopU5Qn3zyCQkJCQwdOrRN6tM7ijZmCQoieto0Cp95hppt2wnsmtHeISmlWuF4\nv/z94bvvvmP27NnMnTuX6upqSktLueaaa/y2JoXeUbSD6GlXIQEBHHj9tfYORSl1AvrrX/9Kbm4u\nO3bs4K233uKss87y68JFmijagS02loiLLqLkw49wFhe3dzhKKdUkTRTtJOa6azFVVRS98057h6KU\nOoGNGzeOTz75xK91aKJoJ0G9exNy2mkU/edNXQVPKdWhaaJoRzHXXUfdnj2UzZ/f3qEopVSjNFG0\no7BxZxKQmkrR29r8pJTquDRR1FfbtkteiMVC1NQpVC5ZgmPXrjatWymlvKWJ4qAD22DWcMj+qE2r\njZw8GSwWit95t03rVUopb2miOCiyE4TEwezboSS3zaoNSEwk7MwzKf7gA32orZTqkDRRHGQNgCn/\nBmctvH8zuJxtVnXU5ZfjLCykbNGiNqtTKXXiysnJYfz48fTp04e+ffsyc+ZMv9aniaK+2G5wweOw\n81v49sk2qzZs7BhsCQkU65gKpZQXbDYbTzzxBNnZ2SxZsoRZs2aRnZ3tt/o0URxt4DToNxUW/gXy\nVrZJlWKzETllMhXffEttXt7xD1BKndKSk5MZMmQIAOHh4WRmZpLnx2tHu04KKCLnAjMBK/BvY8wj\nR31/C3Ar4ATKgRnGGP+lTXelcOHfYed38PHt8LNFYPX//0xRU6aw/9l/UfLxx8Tdcovf61NK+cae\nv/yFmg2+nWY8MLM3Sb/7nVdld+zYwapVqxg5cqRPY6iv3e4oRMQKzALOA/oA00Skz1HF3jTG9DfG\nDAL+Bvy9TYILioTzHoU9a2Hpv9qkSntaGsFDh1Iy+2NdKlUp5ZXy8nKmTJnCU089RUREhN/qac87\nihHAFmPMNgAReQu4BDh0x2CMKa1XPhRouyto5sXQYyIsegQGXAlh8X6vMvKii9jzf/9HdXY2wX37\n+r0+pVTrefvL39dqa2uZMmUKV199NZMnT/ZrXe35jCIVyKn3Odez7wgicquIbMV9R3F7QycSkRki\nkiUiWQUFBb6JTgQmPgy1lfDVI8cv7wMR506CgABKP/bvBF9KqRObMYYbb7yRzMxM7rzzTr/X1+Ef\nZhtjZhljugH3AL9vpMzzxphhxphh8fE+/OUf3xOG3QBZL0PhZt+dtxHWqCjCxo6ldM4cjLPtuucq\npU4s3333Ha+//jpffvklgwYNYtCgQcydO9dv9bVn01Me0Kne5zTPvsa8BTzr14gacuY9sPo/8PVj\nMPl5v1cXedGFlC9YQOXSpYSefrrf61NKnXhGjx7dps8y2/OOYjnQQ0QyRMQOXAXMrl9ARHrU+3gB\n4P+f9UcLi4fhN8Had9rkriJs3DgsYWGUzP7Y73UppZQ32i1RGGPqgNuAecAG4G1jzHoR+aOIXOwp\ndpuIrBeR1cCdwPXtEuzpt4MtCL55wu9VWYKCCJ84kbL583FVVfm9PqWUOp52fUZhjJlrjOlpjOlm\njHnYs+9BY8xsz/avjDF9jTGDjDHjjTHr2yXQsHgYcr37rqI03+/VRV50Ia6KCsq/+trvdSml1PF0\n+IfZHcZIz/xPy17we1Uhw4djjY6m7PPP/V6XUkodjyYKb8VkQOaFkPUSOCr8WpXYbISfcw7lixbh\nqqnxa11KKXU8miiaY+TPoboY1n/g96rCJ03CVVlJxXff+b0upZRqiiYKD5fLMD97L4XlTfyC73w6\nxPWCFa/4PZ7QkSOwREZSNm+e3+tSSp14brjhBhISEujXr5/f69JE4bHrQCUzXs/ipW+3N15IBIZO\nh9zlsGedX+ORgADCzz6bsi8X4nI4/FqXUurEM336dD777LM2qUsThUeXuFDO75fM69/vpLS6iZXm\nBl4F1kBY+ZrfY4qYNBFXWRmV33/v97qUUieWsWPHEhMT0yZ1tes04x3Nz8d1Y87a3bz+/U5uHd+9\n4UIhMdDrPFj3Hkx62L0ynp+EjBqFJSyM0nmfE3bmmX6rRynVct+8/SOFOeU+PWdcpzDGXNHTp+ds\nDb2jqKdfaiRn9ozn5e+2U1PXxFxLA6+CykLY+qVf47HY7YSdNZ7yBQt0PW2lVLvRO4qj3DQmg2tf\nXMacNbuZPCSt4ULdzobgGFjzP+g5ya/xREycSOnsj6lcsYLQ007za11KqebrSL/8/UXvKI4yunsc\n3eJDeWXxjsYn3bLZod9k2DgHanx7y3m00NNPR+x2yhcu9Gs9SinVGE0URxERpp/ehTW5JazOKW68\nYN/JUFcNm/07etoSEkLoqFGULfhSV75TSh0ybdo0Ro0axaZNm0hLS+PFF1/0W12aKBpw2ZA0ggOs\nvJ2V23ih9NMgNB6yP/J7PGFnnUVtbi6OLVv8XpdS6sTw3//+l927d1NbW0tubi433nij3+rSRNGA\nsEAb5/VP4pMf8qlyNPJQ22KFzIvcdxSOSv/GM24cAGVfavOTUqrtaaJoxOVDO1FWU8e89XsaL5R5\nsXup1C1f+DWWgMQEgvr1o/xL//ayUkqphmiiaMTIjBjSooN5b2UTzU9dxkBQFGz61O/xhJ01nqo1\na6jz1ZrgSqlWOVGfGbYkbq8ShYgkiMhlInKriNwgIiNE5KROMhaLcPHAFBZv3c+Bikam0LDaoMcE\nd/OTy79rXIefdRYYQ/lXX/m1HqXU8QUFBbF///4TLlkYY9i/fz9BQUHNOq7JcRQiMh64F4gBVgH7\ngCDgUqCbiLwLPGGMKW1J0CJyLjATsAL/NsY8ctT3dwI3AXVAAXCDMWZnS+pqifP7J/PPRVuZn72H\nK4enN1yo57nuBY3yVkKn4X6LJbBXL2wpyZQtXETU1Kl+q0cpdXxpaWnk5uZScALe4QcFBZGW1sgY\nsUYcb8Dd+cDPjDG7jv5CRGzAhcAE4L1m1eo+3grM8hyfCywXkdnGmOx6xVYBw4wxlSLyc+BvwJXN\nraul+qZEkB4Twpy1TSSK7meDWOHHz/yaKESE8PFnUfzee7iqq7E08xeBUsp3AgICyMjIaO8w2kyT\nzUfGmN82lCQ839UZYz40xjQ7SXiMALYYY7YZYxzAW8AlR9Wx0BhzsEvREqB5abCVRITz+yezeEsh\nxZWNND8FR0P6KPjR/9OBh505FlNdTeXyLL/XpZRSB3n7jOJXIhIhbi+KyEoRmdjKulOBnHqfcz37\nGnMj0OBTYxGZISJZIpLl61vBSX0TqXMZvvqxifP2OAf2roXyfT6t+2ghI0YggYGUf6NraSul2o63\nD6Rv8DyHmAhEA9cCjzR9iO+IyDXAMOCxhr43xjxvjBlmjBkWHx/v07oHpkURG2pn4cYmkkCGZ2bX\n7f69gFuCgggZMYKKr7/xaz1KKVWft4lCPO/nA68bY9bX29dSeUCnep/TPPuOrFjkHOB+4GJjTJsv\nIG2xCGf2jOerHwtwuhrp4ZA8EIIiYZv/B8SFjRmDY8cOHDk5xy+slFI+4G2iWCEin+NOFPNEJBxw\ntbLu5UAPEckQETtwFTC7fgERGQw8hztJ+LddpwnjeidQVFnLD7mNzP1ksULGWNj2Ffi5u1zY2DEA\nlH+tzU9KqbbhbaK4EXc32eGeh8sBwE9bU7Expg64DZgHbADeNsasF5E/isjFnmKPAWHAOyKyWkRm\nN3I6vxrbIw6LwKLjNT+V5MCBbX6Nxd6lCwHp6dr8pJRqM96uRzEKWG2MqfA8LxiCe/xDqxhj5gJz\nj9r3YL3tc1pbhy9EhdgZ2CmKb7YUcufEXg0X6jre/b79K4jt5td4wsaMcXeTranBEhjo17qUUsrb\nO4pngUoRGQj8BtgK+H/R6A7k9G6xrMktobymruECsd0gIhW2LfJ7LGFjx2g3WaVUm/E2UdQZ91j1\nS4BnjDGzgHD/hdXxjOoah9NlWL7jQMMFRNzNT9u/BldrH980LWTECMRup0K7ySql2oC3iaJMRO7D\n3S12jmeepwD/hdXxDO0cTYBVWLJ1f+OFuo6DqiLYs8avsViCgwkZMYJyfU6hlGoD3iaKK4Ea3OMp\n9uDuytrgmIaTVbDdyuBO0Xy/rYlEkTHW/b7d/xP3hY0dg2P7dhy5Tcxuq5RSPuBVovAkh/8AkSJy\nIVBtjDmlnlEAnNYtlnV5JZRW1zZcICIZ4nu3yXOK0NHaTVYp1Ta8ncLjCmAZcDlwBbBURE65KUxP\ny4jBZWDVribW0u4yGnKWgbORh94+Ys/oQkBamnaTVUr5nbdNT/fjHkNxvTHmOtwT+j3gv7A6poGd\norAIrNhZ1HihTqeBoxz2rvNrLCJC2NgxVCxdiqumzQesK6VOId4mCstRI6P3N+PYk0ZooI3M5AhW\n7Gyk5xNA+mnu95yl/o9nzBhMVRWVWdpNVinlP95e7D8TkXkiMl1EpgNzOGqg3KliaOdoVu8qps7Z\nSBfYqE7u8RS7vvd7LKEjRyIBAVR8863f61JKnbq8fZj9W+B5YIDn9bwx5h5/BtYenLXHH/8wtHM0\nFQ4nm/aWNV4o/TTYtdTv8z5ZQkIIHjaUiu80USil/Mfr5iNjzHvGmDs9rw/8GVR7qCp38J//W8Lq\nL3ZhGpslFhiSHg3AyuM9pyjLd8/95Gdho0dTs3kLtXv2+L0updSpqclEISJlIlLawKtMRFq0TnZH\nFpcWxnfvbuGjp1ZRur+qwTJp0cHEhwc23fPp4HOKXUv8EOWRDnaTrfhW7yqUUv5xvKVQw40xEQ28\nwo0xEW0VZFsIDrNz3i39GX9tb/btLON/f1rGxiW7MUc1H4kIA9MiG59yHCCxL9jD2yRRBPbsgS0h\ngfJvv/N7XUqpU5O34yhiGniddFN4iAh9zkjhqgdGEJsWxoJXNjDv+XVUlR+5XvaAtCi2FVZQ1tjA\nO4sVOg1vk0QhIoSOHk3F999j6vw7dkMpdWry9hnFSqAA+BHY7Nne4Vk7e6i/gmsvEXHBXHrnEEZd\n1o3tawp564/LyNlwuEvsgLRIjIG1eSWNn6TTabAvG6qauPPwkbDRZ+AqKaFq7Vq/16WUOvV4myjm\nA+cbY+KMMbHAecAnwC+Af7a0chE5V0Q2icgWEbm3ge/HepJRXVuPBLdYhCGTOnP5fcMIDA1g9tOr\nWfLRVlxOFwPSogBYk9tEokgfCRjIXe73WENGjQIRKrT5SSnlB94mitOMMfMOfjDGfA6MMsYsAVq0\nco6IWIFZuJNOH2CaiPQ5qtguYDrwZkvq8IW4tHAuv3cYmaOSWfHpTj58chV2h4tOMcGsbSpRpA4D\nsbZJ85MtOpqgAf31gbZSyi+8TRS7ReQeEensed0N7PVc7Fu6+MIIYIsxZpsxxgG8hXu9i0OMMTuM\nMWtaUYdPBARaOeu6TM75aR8Kc8r535+Xc3pwaNMPtAPDIKl/m4zQBgg7YzRVa9fiLPZ/U5dS6tTi\nbaL4Ce6pxT/0vNI9+6y4JwlsiVSg/kCDXM++ZhORGSKSJSJZBQUFLQzn+HqNTOKK3w0nNDqQ9PUV\npO2u5UB5E/MsdRoBeSv8PkEgQOjo0eByUfG9/0eEK6VOLd6OzC40xvwSGAuMMcbcZowpMMY4jDFb\n/BuiV/E9b4wZZowZFh8f79e6ohJDmHL3UKIyoxhTHcDc59biqG4kEaSNgNpK90NtPwse0B9LRATl\n2vyklPIxb7vH9heRVcA6YL2IrBCRfq2sOw/oVO9zmmdfhxdgtzLxhj58GeSgZEsp7z6SRfHeymML\ndhrufs9d5veYxGYjdNQoKr759pixH0op1RreNj09B9xpjOlsjOkM/Ab33E+tsRzoISIZImIHrgJm\nt/KcbSY+PIiceBv5A8OpKq/lnUeyyN141KyyUZ0hNB5y/N/zCSB09BnU7dtHzebNbVKfUurU4G2i\nCDXGLDz4wRizCAhtTcXGmDrgNmAesAF42xizXkT+KCIXA4jIcBHJxb1g0nMisr41dfpan5QIVlRX\ncfm9wwiLDuTjp39gw+L8wwVE3M1PbdBFFtzzPgHaTVYp5VPeJoptIvKAiHTxvH4PbGtt5caYucaY\nnsaYbsaYhz37HjTGzPZsLzfGpBljQo0xscaYvq2t05cykyPYWlBOUFQgk387lNReUXz52kaWfLj1\n8MSCnYbDga1Q0cRa2z4SkJyMvXs3Kr7VVe+UUr7jbaK4AYgH3gfeA+KAn/orqBNFn+QIap2GzfvK\nCAy2ccFtA+kzOoUVn+3k85fWU1frhLSDzyna6q5iDJVZK3BVNTypoVJKNZe3vZ6KjDG3G2OGGGOG\nGmPuAH7v59g6vD4p7nkRs/PdE+larRbGXd2LUZO7sSVrH5888wOO6P7ugXdtlChCR4/GOBxULm+b\n+pRSJ7/WLGfa0vETJ40usaEEB1jZsPvwIkYiwpCJnTnnp33YvbmED2f9SGXsyDbp+QQQMmwoEhhI\nua56p5TykdYkCvFZFCcoq0XolRRO9u5jp/LoNTKJ837en6LdFXyw/eeU7twJLqffY7IEBREyYoRO\n56GU8pnjLVzU0PTiMSISiyYKwN38lJ1f2uDYhS7947j4V4Ooqg3h/T0PsH9d28zuGjb6DBzbt+PI\nPSGGpSilOrjj3VGsALI87/VfWYCjieNOOMYYnln1DHsqmrekaJ/kCEqr68gvqW7w++TuUVx2SxoG\n4cOXCijMLfdFuE0KPdRNVu8qlFKtd7wV7jKMMV0970e/urZVkG1hR+kOXs9+nSs/uZLle7x/EHz0\nA+2GxGb24rLUx7GaGj58ciUFu8oaLesL9q5dsaUkU/GdJgqlVOsdr+mpy3G+FxFJ82VA7SUjMoM3\nL3iTCHsEP/v8Z7yR/YZXU2H0TgpHpOlEgQhRGV24LP1pAuxWPnpqFft2+m/JcREh7IzRVHy/BFPb\nyCp8SinlpeM1PT0mIu+JyHUi0ldEEkQkXUTOEpE/Ad8BmW0QZ5voFtWNNy94kzFpY3h0+aPc9+19\nVNU1PR4hxG4jIzaUDbuPc+HvNJzIsqVc9ouu2INtfPTUavZu91+yCB09Gld5OVU//OC3OpRSp4bj\nNT1dDjwA9MK9yNA3uOdj+hmwCTjLGDPf30G2pXB7ODPHz+S2Qbcxd9tcrv/0egqrCps8JjMlguzj\nJQrPwLuI6nVceudggkJtzJ65ij3bmlj8qBVCR50GVqt2k1VKtdpxu8caY7KNMfcbY8YZY3oZYwYZ\nY6YZY94wxjT8BPcEZxELNw+8mX+c9Q92lO7gmrnXsL1ke6Pl+yRHsOtAJeU1Taw7kTIExAI5y4iI\nda/JHRxuZ/bTq/2SLKwREQQPHkT5V1/5/NxKqVOLt9OMT27gdbaIJPg7wPZ0ZqczeWnSS1TVVXHd\np9exet/qBsv1TgoHYNOeJu4qAsMgse+hgXfhMUFceucQQiLsfPyPH/zygDt8/HhqNm6kdvdun59b\nKXXq8HbA3Y3Av4GrPa8XgHuA70TkWj/F1iH0i+vHG+e9QYQ9ghnzZ5C1J+uYMr2T3T2f6o/QblDa\ncMhdcWjgXVh0IJfcMRh7kJXZT6/mwO4Kn8YeNm4cAOWLFvn0vEqpU4u3icIGZBpjphhjpgB9AAOM\nxJ0wTmqdIjrxyrmvkBSaxM+/+DlLdi854vuUyCAigmxsbOqOAtxTjjvKoGDToV3hMUFccsdgxCLM\nnrma0kLfTeZn79qVgPR0yhYuPH5hpZRqhLeJopMxZm+9z/s8+w4Ap0T/y/iQeF6a9BJp4WnctuA2\nFucvPvSdiNA7OeL4dxSdRrjfj5r3KSoxhEt+NYg6h5OPnlpFeVET63A3g4gQPn4clUuW4qpsYAU+\npZTygreJYpGIfCIi14vI9bh7Pi0SkVCguKWVi8i5IrJJRLaIyL0NfB8oIv/zfL/0eOM6/C0uOI6X\nJr1El4gu3LHwjiOeWWQmhbNpTxkuVxNjL2K6QnBMgzPJxqaGcdEvB1FVVsvsmauoKvPNwPewceMw\nDgcV33/vk/MppU493iaKW4L4o/gAACAASURBVIGXgUGe16vArcaYCmPM+JZULCJW3F1uz8PdlDVN\nRPocVexGoMgY0x14Eni0JXX5UnRQNP+a8C/ig+O5dcGtbC5yLzvaOzmC8po68oqbaDoScT+naGRp\n1MSMCC64dQCl+6uZ/fRqaqqa6EXlpZChQ7GEhWnzk1Kqxbxdj8IA3wJfAguAr403w5abNgLYYozZ\nZoxxAG8BlxxV5hLcSQngXeBsEWn3yQjjguN4bsJzBFmDuHn+zeSW5R7q+eTNwDsKN0FVUYNfp/aM\n5ryb+3Mgr4JPn12Ds9bVqljFbid09GjKv/oK42rduZRSpyZvu8deASwDpuJeh2KpiExtZd2pQE69\nz7mefQ2W8ayxXQLENhDfDBHJEpGsgoKCVoblnbTwNP414V/UOGuYMX8GcZE1iHjZ8wkgb0WjRTr3\ni+Ws6zPJ+7GY+S9nH15WtYXCx4/DWVBI9foOteS4UuoE4W3T0/3AcGPM9caY63DfDTzgv7Caxxjz\nvDFmmDFmWHx8fJvV2yO6B/88558UVhXy669uIz3OcvyeT6lDPQPvmp54sNfIJE6f0p2tK/fxzTub\nvZp3qjGhY8eCxULZl1+2+BxKqVOXt4nCYozZV+/z/mYc25g8oFO9z2mefQ2WEREbEOmpu8MYGD+Q\nJ8c9ydbirbjiXmXDnoablA4JDIeEPl6teDd4QjoDz+nE2oW5rJy3s8Ux2qKjCRk+nLJ5n7cq4Sil\nTk3eXuw/E5F5IjJdRKYDc4C5rax7OdBDRDJExA5chbs3VX2zges921OBL33wbMTnzkg9g4dOf4hi\n1rPH/joVNcfpMXxo4N3xnxmcMbk7PYYnsuTDbWxY3PIR1uGTJuLYtg3Hli0tPodS6tTk7cPs3wLP\nAwM8r+eNMa0aaOd55nAbMA/YALxtjFkvIn8UkYs9xV4EYkVkC3AncEwX2o7i0u6XMinlOgIiV/KX\n759qunDacKgpgcIfj3tesQhnX59Jp8xoFr6xkR1rm56gsDHh55wDIpTO+7xFxyulTl1eNx8ZY94z\nxtzpeX3gi8qNMXONMT2NMd2MMQ979j1ojJnt2a42xlxujOlujBlhjNnmi3r95fYhv8BRPIzZO1/j\nnR/fabxgIwPvGmO1WTj35v7EpYUx74V17Nne/EkEAxISCB46hLJ585p9rFLq1Ha8hYvKRKS0gVeZ\niPhvMYUTVFp0CAEHLifRNog/L/kzX+d+3XDB2O4QFNXgwLvG2INsXHjbQEIiA5nzzBqK9jR/XqiI\nSedSs3kzNds6dL5VSnUwx1uPItwYE9HAK9wYE9FWQZ4oLBahV1IUUWU30Cu6F3d9dRfrCxvoknqc\ngXeNCYmwc/HtAxELfPz0D1QUN2+qj/CJEwAo+1ybn5RS3mttzyV1lL4pEWzc7eCZs2YRExTDLxb8\ngpyynGMLpp8GBRugonmduCLjQ7jwtoFUVdTy8TM/NGv0dkBiIsGDB+tzCqVUs2ii8LF+KZGU19RR\nURXCs+c8i9M4+fkXP6eo+qhusxlnut93NNI81YSEzhGcd3M/ivIr+PRfzRu9HT5pIjUbNuDY2fLu\ntkqpU4smCh/rlxoJwNq8EjIiM/jHWf9gd/lufvnlL6muq7cgYMpgCIyAbYtaVE96H8/o7U3FfPGK\n96O3IyZNAqBkzpwW1auUOvVoovCxHolh2K0W1ue5eyYNThjMI2MfYU3BGu795l6cnkWLsNqg8xmw\nreVLlfYamcTpk7uzZcU+vvVy9HZAcjIhI0dS8sGHOveTUsormih8LMBqoXdyOGvzDndhndB5AveM\nuIcFuxbwt+V/O3xB73omFG2H4l0trm/QhE4MPLsTaxbmsupz784TNfkyanNyqFrR+HxTSil1kCYK\nP+iXGsm6vJIjfuFfnXk11/e5njc3vslr2a+5dx58TrG15VOAiwhnTOlOj2EJfP/BVjYtOf7o7fAJ\nE7CEhlL8vk+GwyilTnKaKPygX0okpdV15BYduTbFncPuZFKXSTye9Tifbv8UEjIhIg02t64Xknv0\ndh/Sekfz5Wsb2bW+6Z5UlpAQws87l9J583BV+HadbqXUyUcThR8MSHM/0F6Vc+Tifxax8PDohxma\nOJT7v72f5XuzoOck2Pol1FY3dCqvWQMsnHdzf2JSQ/n0+XXs29n0eMioyZMxlZWUfqYjtZVSTdNE\n4Qe9k8IJDrCycuexM8kGWgOZOX4mncI7cfuXt7M2pQ/UVsKOb1pdrz3YM3o7PIBPnvmhydHbwYMH\nY+/cmZIPtPlJKdU0TRR+YLNaGNgpkpW7Gp5yPDIwkucmPEd0UDQzNv6bNaER8ONnPqk7NDKQi345\nCIDZM1dTWtjw0qwiQuRll1GZlaVjKpRSTdJE4SdDO0eTnV9KlcPZ4PdJoUm8NOklooNimJEQw+ot\nc72adtwbUYkhXPyrQdTWOPlo5moqShqe6iPyskvBaqXorf/5pF6l1MlJE4WfDEmPps5lWJNb3GiZ\npNAkXp70MnGBUdwSYWH1mtd8Vn9cWjgX/nIglaUOZs9cTVW545gyAYmJhE+cQPG77+pDbaVUozRR\n+Mng9GgAVjTS/HRQYmgiL577KvFOw80/PMXyPc2bKLApSRmRXPCLAZQUVPHx0z/gaGBeqJhrr8VV\nVkbJxx/7rF6l1MlFE4WfxITa6RofyvLtB45bNjGqCy9GjSSp1sEt829h4a6Wj6s4WlqvaM6d0Y/9\nueV8MusHao9qCgsePJigPn048OprGGfDzWRKqVNbuyQKEYkRkfkistnzHt1Iuc9EpFhEPmnrGH3h\n9G6xLN1+AEfd8Z89JAy6hlfzd9M7OJ5fL/o1H2750GdxdOkfxzk39GHP1hI+/dda6moPJwQRIebG\nG3Bs307ZggU+q1MpdfJorzuKe4EFxpgewAIaX+L0MeDaNovKx8b2iKfS4SRr5/HvKug6nqjIzrxQ\nUsfI5JE88N0DvLLuFZ/F0mNYIuOv7U3OhgN8+uyRySLi3HMJ6JzO/udf8Gq+KKXUqaW9EsUlwKue\n7VeBSxsqZIxZAJS1VVC+dnr3OGwW4esfvVjn2mKBkTcTkrOcZ3rdwLldzuWJFU/w9xV/99nFO/P0\nFMZf05tdGw4w99m11HmaocRqJfbGG6let46Kb1o/nkMpdXJpr0SRaIw5OCnRHiCxNScTkRkikiUi\nWQUFBa2PzkfCAm0M7RzN1z96GdOgq8EeTsCy53hkzCNc2etKXl73Mg8tfog6l/cLFDWlzxkpnOW5\ns5j77JpDySLq0ksJSEtj31NP6ayySqkj+C1RiMgXIrKugdcl9csZ98/lVv1kNsY8b4wZZowZFh8f\n36q4fW1sz3iyd5eyr9SLKTqCImD4jbDufaz7srl/5P3cMvAWPtjyAbd/eTuVtZU+iSnz9BTOujaT\nnI1FzPnnGmodTsRuJ/6Xt1GTvYGyeTqth1LqML8lCmPMOcaYfg28PgL2ikgygOd9n7/iaG+T+rpv\nlmb/kO/dAaPvcCeML/6AiHDroFt5cNSDLM5fzPTPplNY5UUzlhcyT0/m7Osyyd1UxMdPr6amspaI\nCy8ksEcP9j3xd1w1zVuPWyl18mqvpqfZwPWe7euBj9opDr/rnhDOgLRIPliV590BwdEw5jewZT78\n6J5V9vKel/P0WU+zo3QHV8+5mm3F23wSW+9RyUy8sS97t5fy4ZOrqKpwkvi7+6jNzeXAyy/7pA6l\n1ImvvRLFI8AEEdkMnOP5jIgME5F/HywkIt8A7wBni0iuiExql2hb6bLBqazPL2XTHi+fy4+8BeIz\n4eNfQZV7ZPfYtLG8POllapw1XPPpNWTtyfJJbD2GJXLBLwZQvLeS9x9bgbPHIMInTKDwuedx5HqZ\n3JRSJ7V2SRTGmP3GmLONMT08TVQHPPuzjDE31Ss3xhgTb4wJNsakGWNOyMbziwamYLMIb2fleHeA\nLRAu/SeU74W5d4Gn11PfuL68cf4bxAXHMWP+DPeaFj6Q3jeWi381mOqKWt5/fCX2m36NiLDnwQe1\nu6xSSkdmt4W4sEAuGpjCm0t3UVDmZdt/6hAYfx+sfQeWPndod1p4Gq+f9zr94/pz99d38/TKp3GZ\n1vdSSu4WyaV3DsG4DB+9koPzhnupWLyYkvfea/W5lVInNk0UbeT2s3vgcLp4dtFW7w8a/RvodQHM\n+x1sOnz3EBkYyQsTX2BKjym8sPYFbv/ydsocrR9uEpcWxpS7hxIWHchX2XEUjvoJe/7yV2q2bW/1\nuZVSJy5NFG0kIy6UyYNTeWPpTrbs8/KibrHA5OcgZRC8fT1sPzwYzm6189Coh7h/5P18l/cdV8+9\nmh0lO1odZ0RcMJN/O5S03tGsCTyDrZ0vJPc3v8FV3boV+JRSJy5NFG3ot+f2IizQxi//u5qaOi8n\n4AsMh6vfhZiu8N+rIG/Foa9EhKt6X8XzE5+nuLqYn8z5CQt2tn6+psBgGxfcOoB+Y1PZkXgmKyyj\n2fX7P+jzCqVOUZoo2lBCeBCPTR3Aht2l3Pf+WlwuLy+8ITFw7QcQEgtvTIH81Ud8PTxpOG9d+Bbp\nEencsegO/rr0rzicx64/0RwWq4Wx03oy+vIeFMYPZOG+/mx94sVWnVMpdWLSRNHGzs5M5Nfn9OT9\nlXk8OHud97/SI5Lh+tlgD4fXLj7izgIgJSyF1897nWsyr+HNjW9yzdxr2FW6q1WxiggDz+7Exb8e\njDM0mvmb0sj629utOqdS6sSjiaId3H52d24+sytvLNnFX+Zu8D5ZRHeBn85xD8p77VLIWXbE1wHW\nAO4ZcQ9Pj3+avPI8rvjkCuZsm9PqeNN6xTDt4TOJsZWydFscn/1h7qE5opRSJz9NFO1ARLj33N5c\nP6ozL3yznae+2Oz9wVHpMH0uhMbD65fBzsXHFBmfPp53L3qXntE9ufebe7n3m3tb3SsqNCaEqX+/\nhO4mm627g/jvPQvYu6O0VedUSp0YNFG0ExHhoYv6MnVoGjMXbOalb5vRBTUyFabPgYgU9zOL7V8f\nUyQ5LJmXJr3ErYNu5bPtnzF19lRW7F3RwMm8Zw0OZMLMmxjF19QUlfLeI8tZ8tFWnF4szKSUOnFp\nomhHFovwyOT+nNs3iT9+ks073o7cBvczi+lzIKoz/Ody2PrlMUVsFhu3DLyFV897FavFyg3zbuDp\nlU9T66ptecxBQQye9QCTuvxI4p6lrPh0J+/+dRmFueUtPqdSqmPTRNHObFYLM6cNYkyPOO55bw1f\nZO/1/uCwBJj+CcR2hzevgs3zGyw2MH4g71z0Dpd0u4QX1r7AdXOva9WYC7FaSXvoPsZdkkL/dc9R\nuquAtx9exrfvbsZR7Zt1M5RSHYecbH3jhw0bZrKyfDNhXluqdNRx1fNL2Ly3nHduGUW/1MhmHHwA\nXrsECjbCZc9Bv8mNFp2/cz7/t/j/qHXVcvfwu5nSYwoi0uK4K77/nu33PcTmqLHkJ51OaKSd0Vf0\npNuQ+FadVynVtkRkhTFmWEPf6R1FBxFit/Hv64cRE2rnhleWk19c1YyDY9xdZ1OGwLs/hcXPHJpI\n8GgTOk/g/YvfZ0D8AP7w/R/41cJfUVRd1OK4Q0eNovd7/2Vo3E6GrnwMa9Fu5r2wjtkzV1OYe8Ku\nYquUqkfvKDqYTXvKmPrsYlKjg3nnllGEBwV4f3BtFbw/AzbMdk9VPvFhsNoaLOoyLl7Pfp2ZK2cS\nGRjJn8/4M2ekntHiuI3LRfHb77D38SfIiR3Bju6X4HBZ6TUiiREXZRARF9zicyul/K+pOwpNFB3Q\n1z8W8NNXljO6exwvXj8Mm7UZN34uF3x+Pyz5J3QeDVNfhPCkRotvOrCJe7+5ly3FW7gm8xruGHoH\ngdbAFsdeu3cve/70J4oWLSa331R2xYzEWCz0PzONIZM6ExJhb/G5lVL+0+EShYjEAP8DugA7gCuM\nMUVHlRkEPAtEAE7gYWPM/4537pMhUQD8d9ku7nt/LVePTOfPl/Zrfnv/6jfhkzvdc0VNfREyxjZa\ntLqumqdWPsV/NvyH7lHdeXTso/SM7tmq+Mu/+Za9jz5Cac5+dg27njx7T6w2C33GpDB4QmfColue\njJRSvtcRE8XfgAPGmEdE5F4g2hhzz1FlegLGGLNZRFKAFUCmMaa4qXOfLIkC4K+fbuC5r7bx+wsy\nuWlM1+afYG82vH0d7N8CI2bA2Q+4E0cjvs37lt9/+3tKHaXcMeQOrs68GqvF2uL4TV0dxe+8Q+E/\nn6WkXMgbMo08e0/EKvQelcyQiZ2JjNcmKaU6go6YKDYB44wxu0UkGVhkjOl1nGN+AKYaY5ocxnwy\nJQqXy3Drmyv5dN0e/nRJX64d1aX5J6kphwV/hGXPQ0QqnP8Y9DoPGrlDOVB9gIcWP8SinEX0j+vP\ng6MepHdM79b9OWpqKH7nXfa/8AJlJXXkDZlGblBvDELP4UkMmtCJuLTGE5hSyv86YqIoNsZEebYF\nKDr4uZHyI4BXgb7GHLucm4jMAGYApKenD925c6d/Am8H1bVObntzFV9s2Mut47tx54ReWC0t6Haa\nswxm3w4FG9zNUBP+5F7nogHGGD7d/imPLn+UkpoSfpL5E24ecDORgc3ostsAl8NByXvvsf+Ff1Ne\nWEFu5iXkxg7H6bKQ2iuKgWen06VfLNKSP59SqlXaJVGIyBdAQ09R7wderZ8YRKTIGBPdyHmSgUXA\n9caYJcer92S6ozio1unigQ/X8dbyHE7rGsNfJw8gIy60+Seqc8CKl2HRI1B1APpNhbG/hYSG7xhK\nakp4auVTvPfje4Tbw/lZ/58xLXNaqx52g7tJqmzhQopef4OSlWvZ3WkceRnnUOUKIjI+mAFndaL3\nqCTsQQ332FJK+V5HvKPwqulJRCJwJ4m/GGPe9ebcJ2OiOOjtrBz+9HE2NXUupo3oxE1jutIpJqT5\nJ6ougW+fcq/FXVsJfS5xJ4ykfg0W33RgE0+ufJLv8r4jOTSZXw7+JednnN+q5xeHQtm4kaL//Ifi\nuZ+xN6Qnud3OpSQolYBACz1HJtN3dArx6dospZS/dcRE8Riwv97D7BhjzN1HlbEDnwIfG2Oe8vbc\nJ3OiANhXVs0T837kvZW5OI3h9G6xTB6cxrn9kggNbOYv8Ir9sGQWLH0eHGXu9bnP+BWkj2yw+NLd\nS/n7ir+TvT+bLhFd+Gm/n3Jh1wuxW1vf5dVVWUnZ/PkUf/Ahu7P3kJcyhn2Jw3CJjbjkIPqO70zP\n4YnYg/UuQyl/6IiJIhZ4G0gHduLuHntARIYBtxhjbhKRa4CXgfX1Dp1ujFl97BkPO9kTxUH5xVW8\nk5XL+6ty2bm/kuAAKxP7JnLp4FTGdI9r3tiLqiL33cWSZ6G6GNKGw+m/hN4XwlF3DS7jYsGuBbyw\n5gU2HNhAQnACU3tO5bIel5EU2vh4jeZw5OZROmcO++d9ya7iSPJTzqA8LA2rxUW3gTH0PrMLqT2j\nseizDKV8psMlCn86VRLFQcYYsnYW8cGqPOas2U1JVS1xYXYuHJDCpYNTGZgW6f0YDEcFrPqP+y6j\naId7oaTTfgGDrobAsGPq/T7/e17NfpXF+YuxiIXRqaOZ2mMqY9LGYLP45pd/zbbtlHz6KbkLVrKj\nrjP7EobgtAUTHFBLj0ExZE7qrT2mlPIBTRSniJo6J19tKuDD1Xl8sWEfjjoXXeNCmTwklcuGpJEa\n5eWYBZcTNs6B75+BnKUQGAkDr4Sh0yGx7zHFc8tyeX/z+3yw5QMKqwqJCYphUpdJnJ9xPgPjB/ps\ncsCazZspmv8l2xbvYFd1Egdi+mAsViIDKujWP4pe5w8gJq11PbOUOlVpojgFlVTV8tm63by3Mo9l\n2w8gAqdlxHLZ4FTOzkwgNszLnks5y2DZC5D9EThr3M1SQ653z1BrP7LnVa2rlm9yv2HOtjl8lfsV\nNc4aUsNSOT/jfM7POJ/u0d199uer3buPws+/YvO3O9lZHkNpeBcAwiilc7qVHudkkjK8m85gq5SX\nNFGc4nIOVPL+yrxDzzMsAkM7RzOuVwJDO0czMC2KYPtxejBVHoAf3oIVr0DhJgiMgL6XQp9L3eMy\nrEdOXljuKGfBrgXM3T6XJbuX4DIuOkd05sy0MxnXaRyDEwb7rHnKVVHB3i8Ws+XrLeTus1MUko4R\nK0F1pSRHVNBlcBJdzx9GUKzebYB7IGety0Wt01Bb56LW6aLG817rNNQ6XTicLhyH9rlw1Hn219/n\nPHKfw+mi1lPu4H6H88jz1tQ7/mDZI/Y5DYvvO4uI5kyGqXxCE4UC3M8V1ueX8nn2XuZn72XDbvea\n1zaL0C0+jK7xoWTEhZIcFUxMiJ3okACC7VYCrBbPSwiwCCF7swhd9waBW+YgjgpMcDTS+wLIvAS6\njAb7kV12C6sKmb9zPotyFrF8z3JqXbWE28M5Lfk0hiYOZVjiMHpE98AirZ/13hhDafYWNn+6hp2b\nKylwxeG0BiKuOmKce0hNNHQZ3onkcYOxRTc4dKdVnC5T76LpvvAdecGsf2E+fKE+fGGud6E94sJs\nDl+Q6+9zuo66WB8+5+GLtTmi7lqn7//Ni4DdasFutRBg8/xd8Xy22+r9/fF8th/8O+UpW7/cbyb2\nJMSuvdvamiYK1aCiCgercorI2lHEj3vL2FZYwa79ldS5vPs7EYiDsZY1nGddxgTLCsKlCoexsVp6\ns8I2iNX2weTau2O3BxBksxIUYMFmc1BhyWa/Wc0BVzbVZj8AARJKrK0nMbYuxAR0ITagC1EBydis\nVqwiWCyC1SLYLIJF3NsWi2AVwWUMxhhcBlyed/dng6vaQfCazVi3F1FTHUJ1QKw79uoDhFfnIuF1\nlKfGUJDRleKwmGMuqg6nwVHnPPxLu4Ffz/Uv1l7+T9csVot4LqxS76J7+MJq91yAj9hnO3Kfvd6F\n+ph9h85z5EX94EXfbm34nAePCbC6///QZr4TmyYK5bU6p4uiylqKKh3sL3dQXeekts5Fnav+hdJQ\n5zryF7CrtpqkoizSi5fStWQZSdVbAaiWIHbYe/CjtSfZlh5soAs7nfHUGov717fsx2nfijNwGwRu\nxwQUIOKepcW47Liqk3DWJGIccbhqY3E54nA5YsC0bOxGTG0dI0tL6VpnIdQWjbEGARBcVUBw2Q6q\nKGNPmI2cxDT2JHXBBAV7LphyxK/goy/ch747dBE99qJe/4J+8MJrt1oJsMkR57Dbjryot2jKFqWa\nSROFantle2H7V5CbBXkrYM8acDrc31ntENMN4rpDXE+I7eHuihuZSk1IDFvLdrHpwCY2FW1i04FN\nbC3eSlHNkavwRQVGkxicREJIMokhSSSFppAcmkxyaDJJoSlE2sOxWixYRBABiwgWz7sIiAgul2H/\nrhJ2fruJ3OxC9h2wUYu7bTy4ch9RpVuJC64ksVsECYN7EtyvD4FduyIB2n6u/Mzlcv97Ofiqq3F3\nJnHWerYdh9/rbwdFQY9zWlSlJgrV/uocsHcd7F0P+zdDoedVtB1cdYfLiQXCkiAy1T3bbVgihMVT\nFhRBjtXCLuMgx1lBfm0Zu6sKyK/IZ3f5bqqd1UdUFxoQSnJoMilhKYfeU0JTSA5zJ5PIwMhj5qxy\nuQz7c8vJWZ1Hzg/57N3tpNblfshvq60komwHkeU7iY2oJTEjkog+PQjK7E1g795Yw44cZ6I6GGPc\nf8+cte4L6sFtV63nvf7nOk8Zz/ahMvU/Oxr4rqmLeK37Qn/Efofn4l9vu85xuO6WSB0KP/uyRYdq\nolAdl7PWPbiveCeU5EFpHpTkul+leVBeADUlDR8bEAqhcZiweA4EhrPbbiffZmW3GPKpI9/UsLuu\ngvzaUsqcNcccbrfYiLCFEh4QRkRAGOEBoUTYw92f7WGEB4QTUh6DFIbj3CZU77FSWRkMuJuCgiv3\nEl6eR1h5DlEBFcQlBRHeLYXArt0I7NYVe9euWGNi2q7t/uDF0OUE46y37Tpqv/Oo7TrPtqve9vH2\nu44qc9Q5D9XvOiqWo8seb39Tsdd5LvD1L9iei/zRF//6P0b8yRoItkB3L0BrINjs7nervd52gKeM\n3bO/sW17I+ezN75tD3P/yGqBphKFdi1Q7csaAHE93K/G1FZDZSGU74OKQqjYBxUF7iRSUYBU7CO2\nqpjY4jL61ZRBTZl7ssN6ykTIt9nYbbOx12al1GKhzGKh1Frs2RYOWCzstFgOfeeqf4FPdL8C6gJJ\nqOhMYmlnkks7EV/emSDXkEPFArYUE746n9CKHwgrz8dWm09l8B6Ko2spjYKKKKiIEiojQeyC3RgC\nALsR7ECA53MAgt2AHfc/Uqsx2IwLm8uF1TixutzbNuPE6nJicTmxYLAYsACCcb8bsAIWDAKHvj9Y\nVjjy86FjD22D9aiywsFU2Rzing7GYgOxerat9bYP7rc0XkY85SxWkADP9wEYi8293xqAsdoPb1sC\n3GvGWwJwWW04LVZcFvf7wZdLDm5bcIkNY3Xvc1ksGHGXP7htLFZcYjn07rLYMCKebStGBKdxuTtS\n4MJ1cNu4tw/uE9wP/sd3Gu+zLuL+dmJEqU5tAUEQmeZ+ectZ557osM59ux/udNDL6aBXY00DB3+9\nGvcvYONyUlFXRWldFZWuGqqcDqpdDqqcNVS5aql2VlPlyqba9QOVNUJNSSTO0miqS2OpLolhf1VP\n5OA/L+MiqLqAqKJ80nLyCa3II6win1rLfgqjhb3RsDtK2BEFeyOFwgg4EA6uYx5it362Xl+xcPD5\nj3i2LYe2DQYD7v/2tFgc/GyMwRwq4XHEpgGcGA7fAdRv9TjiOJfndYJadvUyTRRKtSurDYJbPk5C\ngDDPqyVcThclBVXsz6tgf345B/ISKcxNp6DwcBOYFSdhriJiyvJI276FsIp8QivysTtKEYsFS0Is\nkpiAJCVAYhwmMQ5XXBSumAjqosNxRodTZ4U6V91xf8Ue2uf5fMw+17FlDQancTbr+IMXdRHh0H/+\nv727jbHjuus4/v3NpDtmFgAACn1JREFU3Ie9613bm9hxHhwnFYkqGjU1IUKoalF4bssLg5BoKyQi\nBAqgFnhRVQniBbzgRVWBQCmlKEWhQTyEqlLSSK1CQ4CCxEMSkJs6RVGjEoifn9ax9+k+zPx5MWd3\n7673Xru2d+9N/Psoo3PmzOzsucd39c+cM3NOujMTovpvbfnKsfX/BgOOb1S+5ufXZFeP51lOrrRl\nOZmylf1MGXmWI0SmtYFvTbp8LJ23/Dlz5SvHV/L9AbTv56CaXPNq13XZSg4UZpsgyzNmbt7GzM3b\nuOv7b1op77YLZo/Pc+bIXBVEjuzi7NFbObrjPSvnNOsl2+sLTPfO0pw7Sf3/jlB78SDN+dM0O+fI\nokzdU5Dv3Elt9y5qu3eT76rS2u7d1HalNB3Lpqb8noNdMQcKsy1Ub+bcdMd2brpj+5ryxQsdzhyt\nAsjZI3OcOTrP4WM76DT2wZ77qzGSpDUBk40uLRZp9i7QXDpL8/xJ6ocOUzv1Ao2502RRrLm+ms0U\nQHatBJB85gbynTurbWZmJV+b2YkmJx1YbIUDhdkYaE032PvOBnvfuba7rLPY48LsEvOzbeZm2335\nJeZm2xyfnaFX3rbaT7YvXW8yY7IFrVqHiVik2T1PY/EsjfMnqL1xhOy//pts9gT5gMcwVa9fFEBW\n8jt2kG+fJpveTj49VaXbp8mmp8mnplDj6heysvEykkAh6Qbgb4E7gdepFi6aXXfOHcBTVA9a1IHP\nRMSfbm1NzUar0apxY2uKG2/deLQkIugsFcwtB5BzVUCZn11i7lyb+XNtTp2boD0/DdwGjXdXK9mn\nNaZq9YzmhGjUg0Ze0lCHeixR7y1Sa1+gtnSefH6W2uEzZIdeJp89Qa19YWCAAVCrRT41RbZ9+2o6\nnQLJRgFmKpVv20Y2OUk2OemXGsfMqFa4+zRwtm8p1JmIeHjdOY1Uv7akKeAQ8N6IODrs2n6Pwuxi\nvU7B/Jsd5s+1WTjfYWm+S3uhy9J8r8rPd1NZj6W5Kl8OmTwwr4nmhGjWoVEryaNHFj2yokPWa1db\nd5Gss4iWFtDSPCwtEIvzZN0OioIsClQWfflelUZRzeM10SCbqFNrNsgnmuStJrVWk3xygtpki3yy\nRbattRJcssnlQNMia7VQq0XWWt3PWi3f7Qwxju9RHAAeSPkngH8C1gSKiOj07Tap7izM7ArUGjk7\ndrfYsfvyFq+KCHqdkqXlADJfBZUquKR8X3DpdQp63ZJet6SIgl5Z0stKiryEbVTbtbKQNkBlb13Q\nmUdxHkWJoqhSAkUJEVU+S9O4ZEpbRpYJ5Vm1n+dkeVbt1zKyPEe1tOU5Wa2G6imt5aheS/na6rXT\nVDH05SWhDOjbX3+8NV3nnvdf2Qtzm2lUgWJPRBxL+eOsGapbJel24CvAXcAnL3U3YWbXhiTqzZx6\nM2f6hokrvk6UQdGrAkhZRNqqfFGUlL1YzS8f26isCIrear4sSoqiunbZ6VEsdSg7XYp2j7Lbpez2\nKLoFZbcgioLoFZRFSRRlSguiCKIsKcvqd0W3R5TpXY/0CkggUHpdUev3tTZFkGWEsmoqmjxHE63q\nHZIAyuXrrqasu2mbuXny+goUkv6elZ7QNX67fyciQtKG97gR8QZwr6RbgaclfSkiTmzwux4CHgLY\nt2/fVdfdzK4NZaLWyKldamGsMRRlSSwtUS4uVtvCArGSX6RcTPsLC+mc5f3qnPotN3PTJz5x6d8z\nJHCMi00LFBExcApDSSck3RIRxyTdApy8xLWOSjoEvB/40gbHHwMeg2qM4upqbmYGyjKUxj829fcs\nd1NdwcQoW2VU/f7PAA+m/IPAl9efIGmvpFbKzwDvA17dshqamRkwukDxKeDHJX0b+LG0j6T7Jf1Z\nOud7gf+Q9A3g68DvR8Q3R1JbM7Pr2EgGsyPiDPCjG5S/BPxyyj8H3LvFVTMzs3X8yKmZmQ3lQGFm\nZkM5UJiZ2VAOFGZmNpQDhZmZDTWSSQE3k6RTwP9exSV2AaevUXXejtw+l+Y2Gs7tM57uiIjdGx14\n2wWKqyXppUEzKJrb53K4jYZz+7z1uOvJzMyGcqAwM7OhHCgu9tioKzDm3D6X5jYazu3zFuMxCjMz\nG8p3FGZmNpQDhZmZDeVAkUj6gKRXJb0m6ZFR12dcSHpd0jclHZT0Uiq7QdJzkr6d0plR13OrSHpc\n0sm0kNZy2Ybtocqj6Tv1sqT7RlfzrTOgjX5X0pH0PToo6UN9x34rtdGrkn5yNLW2YRwoAEk58Fng\ng8C7gI9KetdoazVWfjgi9vc9+/4I8HxE3A08n/avF18APrCubFB7fBC4O20PAZ/bojqO2he4uI0A\n/jB9j/ZHxFcB0t/ZR4B70s/8Sfp7tDHiQFH5AeC1iPhORHSAJ4EDI67TODsAPJHyTwA/PcK6bKmI\n+Gfg7LriQe1xAPiLqPw7sDMt/fu2NqCNBjkAPBkR7Yj4H+A1qr9HGyMOFJXbgDf69g+nMquWe/+a\npP+U9FAq2xMRx1L+OLBnNFUbG4Paw9+rtT6euuAe7+uudBu9BThQ2KW8LyLuo+pG+ZikH+o/GNXz\n1X7GOnF7DPQ54HuA/cAx4A9GWx37bjhQVI4At/ft701l172IOJLSk8BTVN0CJ5a7UFJ6cnQ1HAuD\n2sPfqyQiTkREEREl8HlWu5fcRm8BDhSVF4G7Jb1DUoNqcO2ZEddp5CRtkzS9nAd+AjhE1TYPptMe\nBL48mhqOjUHt8QzwC+nppx8E3uzrorqurBub+Rmq7xFUbfQRSU1J76Aa+H9hq+tnw9VGXYFxEBE9\nSR8H/g7Igccj4pURV2sc7AGekgTVd+WvI+JZSS8CX5T0S1RTuv/cCOu4pST9DfAAsEvSYeB3gE+x\ncXt8FfgQ1QDtAvCLW17hERjQRg9I2k/VLfc68CsAEfGKpC8C3wJ6wMciohhFvW0wT+FhZmZDuevJ\nzMyGcqAwM7OhHCjMzGwoBwozMxvKgcLMzIZyoDC7TJKKvtlPD17LWYYl3dk/26rZOPF7FGaXbzEi\n9o+6EmZbzXcUZlcprdnx6bRuxwuS7krld0r6hzQR3vOS9qXyPZKekvSNtL03XSqX9HlJr0j6mqRW\nOv83JH0rXefJEX1Mu445UJhdvta6rqcP9x17MyLeDfwx8Eep7DPAExFxL/BXwKOp/FHg6xHxHuA+\nYHkWgLuBz0bEPcA54GdT+SPA96Xr/OpmfTizQfxmttllkjQXEVMblL8O/EhEfEdSHTgeETdKOg3c\nEhHdVH4sInZJOgXsjYh23zXuBJ5Lix8h6WGgHhG/J+lZYA54Gng6IuY2+aOareE7CrNrIwbkvxvt\nvnzB6hjiT1GtwHgf8KIkjy3alnKgMLs2PtyX/lvK/yvVTMQAPw/8S8o/D/waVMvwStox6KKSMuD2\niPhH4GFgB3DRXY3ZZvL/mZhdvpakg337z0bE8iOyM5Jepror+Ggq+3XgzyV9EjjF6uyxvwk8lmab\nLaiCxqDpx3PgL1MwEfBoRJy7Zp/I7DJ4jMLsKqUxivsj4vSo62K2Gdz1ZGZmQ/mOwszMhvIdhZmZ\nDeVAYWZmQzlQmJnZUA4UZmY2lAOFmZkN9f8K/WklTOKJawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTRPrediction_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "14BAV0L--W8SLx2nrYIB4fN8nUmp-ChQS",
      "authorship_tag": "ABX9TyO/nZPUFfocgrc1bmdjGlqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiahaoLU/CTRPrediction/blob/Jiahao/CTRPrediction_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-4a8uQXyo-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "octJXhQRlVgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "\n",
        "    def __init__(self, path=None):\n",
        "        \"\"\"\n",
        "        Initialise the preprocessor as a Data set in torch\n",
        "        or initialise it as a data provider (cut smaller data, etc)\n",
        "        :param path: file path\n",
        "        \"\"\"\n",
        "        if path is not None:\n",
        "            self.path = path\n",
        "            self.data = self.read_data_by_chunk()\n",
        "            self.labels = np.asarray(self.data.iloc[:, 1])\n",
        "            self.one_hot_data = self.get_one_hot_dataset()\n",
        "            print('Data set initiated from {0}.'.format(self.path))\n",
        "        else:\n",
        "            self.path = './Data/train.csv'\n",
        "            print('Data provider is ready.')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        return number of data\n",
        "        \"\"\"\n",
        "        return np.size(self.one_hot_data, 0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        called when iterate on Data Loader\n",
        "        :param index: index of one sample\n",
        "        :return: one hot encoding of a sample, which consists of field-relative index of value 1,\n",
        "                 and corresponding label\n",
        "        \"\"\"\n",
        "        return self.one_hot_data[index], self.labels[index]\n",
        "\n",
        "    def open_csv(self, iterator=False):\n",
        "        \"\"\"\n",
        "        Open a csv file\n",
        "        :param iterator: Open a csv file as iterator/or not\n",
        "        :return: pandas data frame\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return pd.read_csv(self.path, sep=',', engine='python', iterator=iterator)\n",
        "        except FileNotFoundError:\n",
        "            print(\"file not found\")\n",
        "\n",
        "    def cut_smaller_data_for_exam(self, new_path, size=2000, skip_size=40000):\n",
        "        \"\"\"\n",
        "        Cut a smaller data set for debugging code of size (smaller_data_size, num_fields + 2)\n",
        "        :param new_path: name and path for new csv file\n",
        "        :param size: the size of the smaller data set\n",
        "        :param skip_size: interval to skip to retrieve data uniformly from original set\n",
        "        :return: none\n",
        "        \"\"\"\n",
        "        print(\"Producing smaller data\")\n",
        "        print(\"Read file\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"cut by chunk\")\n",
        "        chunk_size = 100\n",
        "        loops = math.floor(size/chunk_size)\n",
        "        chunks = []\n",
        "        for loop in range(loops * 2):\n",
        "            try:\n",
        "                print(\"chunk: \", loop)\n",
        "                if loop % 2 == 0:  # avoid load data of same timestamp\n",
        "                    chunk = data_file.get_chunk(chunk_size)\n",
        "                    chunks.append(chunk)\n",
        "                else:\n",
        "                    chunk = data_file.get_chunk(skip_size)\n",
        "                    del chunk\n",
        "            except StopIteration:\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        data_frame = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        if os.path.exists(new_path):\n",
        "            os.remove(new_path)\n",
        "        data_frame.to_csv(new_path, index=False)\n",
        "        print(\"New smaller file created\")\n",
        "\n",
        "    def read_data_by_chunk(self, chunk_size=1000000):\n",
        "        \"\"\"\n",
        "        Read csv by chunk to avoid drive memory overflow\n",
        "        :param chunk_size: the size of data to read in every chunk\n",
        "        :return: csv registered in pandas data frame of size (data_size, num_fields + 2)\n",
        "        \"\"\"\n",
        "        print(\"Reading large data\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"Cut by chunk\")\n",
        "        loop = True\n",
        "        chunks = []\n",
        "        index = 0\n",
        "        while loop:\n",
        "            try:\n",
        "                print(\"chunk: \", index)\n",
        "                chunk = data_file.get_chunk(chunk_size)\n",
        "                chunks.append(chunk)\n",
        "                index += 1\n",
        "\n",
        "            except StopIteration:\n",
        "                loop = False\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        whole_data = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        print(\"Data imported\")\n",
        "        return whole_data\n",
        "\n",
        "    def get_feature_set(self, feature_array):\n",
        "        \"\"\"\n",
        "        Get array of feature set of one field.\n",
        "        :param feature_array: one column (one field) of the data frame\n",
        "        :return: an array of size (num_features)\n",
        "        \"\"\"\n",
        "        return np.array(np.unique(feature_array))\n",
        "\n",
        "    def one_hot_encoding(self, feature_instances, feature_set):\n",
        "        \"\"\"\n",
        "        One hot encoding for one column (one field) for every sample.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :param feature_instances: one column of data frame\n",
        "        :param feature_set: array of nonrecurring feature set\n",
        "        :return: one hot encoding of the whole column of size (data_size, 1)\n",
        "        \"\"\"\n",
        "        one_hot_vector = np.zeros((len(feature_instances), 1), dtype=int)\n",
        "        for i in range(len(feature_instances)):\n",
        "            if feature_instances[i] not in feature_set:\n",
        "                raise Exception(\"instance is not in the set\")\n",
        "\n",
        "            index = np.argwhere(feature_set == feature_instances[i])\n",
        "            one_hot_vector[i, 0] = int(index)\n",
        "\n",
        "        return one_hot_vector\n",
        "\n",
        "    def get_field_dims(self):\n",
        "        \"\"\"\n",
        "        Get an array which contains the dimension of each field.\n",
        "        If sum up the array, the sum is the total dimension of all features.\n",
        "        (the total length of the one hot encoding vector)\n",
        "        The length of array is number of fields.\n",
        "        :return: an array of size (num_fields)\n",
        "        \"\"\"\n",
        "        dims = []\n",
        "        for i in range(2, self.data.shape[1]):\n",
        "            dims.append(len(self.get_feature_set(self.one_hot_data[:, i-2])))\n",
        "        return np.array(dims)\n",
        "\n",
        "    def get_one_hot_dataset(self):\n",
        "        \"\"\"\n",
        "        Get one hot encoding for the whole data frame.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :return: an array of size (data_size, num_fields)\n",
        "        \"\"\"\n",
        "        print('one hot encoding: feature {0}'.format(self.data.columns[2]))\n",
        "        features = np.asarray(self.data.iloc[:, 2])\n",
        "        one_hot_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "        for i in range(3, self.data.shape[1]):\n",
        "            print('one hot encoding: feature {0}'.format(self.data.columns[i]))\n",
        "            features = np.asarray(self.data.iloc[:, i])\n",
        "            next_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "            one_hot_array = np.hstack((one_hot_array, next_array))\n",
        "        return one_hot_array\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # print('Start generating smaller data')\n",
        "#     # provider = DataPreprocessor()\n",
        "#     # provider.cut_smaller_data_for_exam('./Data/train20k.csv', size=20000, skip_size=40000)\n",
        "#     print(\"Start data cleaning\")\n",
        "#     f = \"/content/train20k.csv\"\n",
        "\n",
        "#     processor = DataPreprocessor(f)\n",
        "#     print(processor.data.head())\n",
        "#     loader = DataLoader(processor, batch_size=10, shuffle=True)\n",
        "#     print(len(processor))\n",
        "#     # print(processor.get_field_dims())\n",
        "#     # for data, label in loader:\n",
        "#     #     print(data.size(), label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aVyScX_nWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachineModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Field-aware Factorization Machine.\n",
        "\n",
        "    Reference:\n",
        "        Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialisation of the FFM model\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
        "        x = self.linear(x) + ffm_term\n",
        "        return torch.sigmoid(x.squeeze(1))\n",
        "\n",
        "\n",
        "class FeaturesLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        \"\"\"\n",
        "        Initialisation of linear first-degree terms. Use nn.Embedding to realize\n",
        "        inner product of input one-hot encoding vector and weight vector\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param output_dim: the sum of linear terms\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)  # field-relative one hot encoding\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)  # absolute-position one hot encoding\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias  # element wise linear poly sum\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachine(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Use latent vector product to replace simple scalar weight factor.\n",
        "        More adaptive to situations where dependency exists among fields.\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_fields = len(field_dims)\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
        "        ])\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        for embedding in self.embeddings:\n",
        "            nn.init.xavier_uniform_(embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]  # generation of latent mat\n",
        "        ix = list()\n",
        "        for i in range(self.num_fields - 1):\n",
        "            for j in range(i + 1, self.num_fields):\n",
        "                ix.append(xs[j][:, i] * xs[i][:, j])  # product of latent vector\n",
        "        ix = torch.stack(ix, dim=1)\n",
        "        return ix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mzjg8OnhNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "# from Data_Preprocessor import *\n",
        "# from Deep_Model import FieldAwareFactorizationMachineModel\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import threading\n",
        "\n",
        "\n",
        "def fun_timer():\n",
        "    global boom\n",
        "    boom = True\n",
        "\n",
        "\n",
        "def run_train(model, optimizer, data_loader, criterion, device, log_interval=10):\n",
        "    \"\"\"\n",
        "    train the model using backward propagation\n",
        "    :param model: the model to be trained. instance of subclass of nn.Module\n",
        "    :param optimizer: torch optimiser with learning rate\n",
        "    :param data_loader: torch DataLoader of training data set\n",
        "    :param criterion: nn.BCELoss\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :param log_interval: interval for showing training loss\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (fields, target) in enumerate(data_loader):\n",
        "        fields, target = fields.to(device), target.to(device)\n",
        "        y = model(fields)\n",
        "        loss = criterion(y, target.float())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % log_interval == 0:\n",
        "            print('    - loss:', total_loss / log_interval)\n",
        "            total_loss = 0\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def run_test(model, data_loader, device, criterion):\n",
        "    \"\"\"\n",
        "    evaluate / test the model\n",
        "    :param model: the model to be evaluated/tested. instance of subclass of nn.Module\n",
        "    :param data_loader: torch DataLoader of eval/test data set\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :return: auc score, accuracy of prediction\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    targets, predicts = list(), list()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for fields, target in data_loader:\n",
        "            fields, target = fields.to(device), target.to(device)\n",
        "            y = model(fields)\n",
        "            loss = criterion(y, target.float())\n",
        "            targets.extend(target.tolist())\n",
        "            predicts.extend(y.tolist())\n",
        "            predict_click = torch.round(y.data)\n",
        "            correct += (predict_click == target).sum().item()\n",
        "    return roc_auc_score(targets, predicts), correct / len(targets) * 100, loss.item()\n",
        "\n",
        "\n",
        "def main_process(dataset_path, epoch, learning_rate, batch_size, weight_decay, embeddim, boomtime = 60):\n",
        "    \"\"\"\n",
        "    Main process for train/evaluate/test the model, determine the hyper parameters here.\n",
        "    :param boomtime: time boom for adjusting hyperparameters to control the same training time cost\n",
        "    :param dataset_path: path of the original csv file\n",
        "    :param epoch: number of epochs\n",
        "    :param learning_rate: learning rate of gradient descent\n",
        "    :param batch_size: size of batches\n",
        "    :param weight_decay: L2 regularisation\n",
        "    :param embeddim: dimension of latent vector\n",
        "    :return: the trained model\n",
        "    \"\"\"\n",
        "    global boom\n",
        "    boom = False\n",
        "    timer = threading.Timer(boomtime, fun_timer)\n",
        "    timer.start()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "\n",
        "    # Prepare the data\n",
        "    dataset = DataPreprocessor(dataset_path)\n",
        "    train_length = int(len(dataset) * 0.8)\n",
        "    valid_length = int(len(dataset) * 0.1)\n",
        "    test_length = len(dataset) - train_length - valid_length\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, (train_length, valid_length, test_length))\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    field_dims = dataset.get_field_dims()\n",
        "\n",
        "    # Prepare the model and loss function\n",
        "    model = FieldAwareFactorizationMachineModel(field_dims, embed_dim=embeddim).to(device)\n",
        "    criterion = torch.nn.BCELoss().to(device)  # binary cross entropy loss\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    val_auc_list = []\n",
        "    val_acc_list = []\n",
        "    val_loss_list = []\n",
        "    best_model = None\n",
        "    best_val_auc = 0\n",
        "    best_val_acc = 0\n",
        "    best_val_loss = 999999\n",
        "\n",
        "    # train\n",
        "    for epoch_i in range(epoch):\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
        "        print('Cached:   ', round(torch.cuda.memory_cached(0) / 1024 ** 3, 1), 'GB')\n",
        "        train_loss = run_train(model, optimizer, train_data_loader, criterion, device)\n",
        "        val_auc, val_acc,val_loss = run_test(model, valid_data_loader, device, criterion)\n",
        "        epoch_list.append(epoch_i)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_auc_list.append(val_auc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print('epoch:', epoch_i, 'train loss:', train_loss, 'validation: auc:',\\\n",
        "              val_auc, '--- acc:', val_acc, '--- loss:', val_loss)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model\n",
        "            best_val_auc = val_auc\n",
        "            best_val_acc = val_acc\n",
        "        if boom:\n",
        "            print('time up, break')\n",
        "            break\n",
        "        else:\n",
        "            print(timer)\n",
        "    test_auc, test_acc, test_loss = run_test(model, test_data_loader, device, criterion)\n",
        "    print('test auc:', test_auc, 'test acc:', test_acc, 'test loss', test_loss)\n",
        "    return model, epoch_list, train_loss_list, val_auc_list, val_acc_list, val_loss_list, \\\n",
        "           test_auc, test_acc, test_loss, best_model, best_val_auc, best_val_acc, best_val_loss\n",
        "\n",
        "\n",
        "def main_train_test(save_model=True):\n",
        "    DATASET_PATH = \"content/drive/My Drive/train20k.csv\"\n",
        "    EPOCH = 1000\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 3200\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    EMBED_DIM = 10\n",
        "    trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,auc,acc,loss,\\\n",
        "    best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "        main_process(DATASET_PATH, EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM)\n",
        "    print(trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,auc,acc,loss,\\\n",
        "    best_model,best_val_auc,best_val_acc,best_val_loss)\n",
        "    # if save_model:\n",
        "    #     model_name = \"FFM\"\n",
        "    #     torch.save(trained_model, f'{model_name}.pt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBS-Fwnh20q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class HyperFinder():\n",
        "    def __init__(self):\n",
        "        self.DATASET_PATH = \"content/drive/My Drive/train20k.csv\"\n",
        "        self.EPOCH = 1000\n",
        "        self.LEARNING_RATE = [0.01,0.02,0.05,0.1,0.2,0.5]\n",
        "        self.BATCH_SIZE = [40*i for i in range(1,11)]\n",
        "        self.WEIGHT_DECAY = [pow(10,-6+i) for i in range(4)]\n",
        "        self.EMBED_DIM = [pow(2,i) for i in range(5)]\n",
        "        self.save_best = True\n",
        "\n",
        "        self.best_learning_rate = None\n",
        "        self.model_best = None\n",
        "        self.epoch_list = []\n",
        "        self.train_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "        self.val_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.auc_best = 0\n",
        "        self.acc_best = 1\n",
        "        self.loss_best = 999999\n",
        "        self.Total_Epoch = 1\n",
        "        self.data_save = {}\n",
        "\n",
        "    def save_model(self,trained_model,*args):\n",
        "        model_name = ''\n",
        "        for i in args:\n",
        "            print(i)\n",
        "            model_name += (str(i)[:7] + ' ')\n",
        "        torch.save(trained_model, f'{model_name}.pt')\n",
        "\n",
        "    def obj_2_json(self, hpname):\n",
        "        load_dict = self.data_save\n",
        "        with open(\"./\" + hpname + \".json\",\"w\") as dump_f:\n",
        "            json.dump(load_dict,dump_f)\n",
        "\n",
        "    def find_best_Learning_Rate(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        WEIGHT_DECAY = 1e-6\n",
        "        EMBED_DIM = 8\n",
        "        for LEARNING_RATE in self.LEARNING_RATE:\n",
        "            self.data_save[str(LEARNING_RATE)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,60)\n",
        "            self.data_save[str(LEARNING_RATE)]['EPOCH'] = epoch_list\n",
        "            self.data_save[str(LEARNING_RATE)]['train_loss_list'] = train_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_auc_list'] = val_auc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_acc_list'] = val_acc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_loss_list'] = val_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['test_auc'] = test_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_acc'] = test_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_loss'] = test_loss\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_auc'] = best_val_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_acc'] = best_val_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "    def find_best_Weight_decay(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        LEARNING_RATE = 0.001\n",
        "        EMBED_DIM = 8\n",
        "        for WEIGHT_DECAY in self.WEIGHT_DECAY:\n",
        "            self.data_save[str(LEARNING_RATE)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,60)\n",
        "            self.data_save[str(LEARNING_RATE)]['EPOCH'] = epoch_list\n",
        "            self.data_save[str(LEARNING_RATE)]['train_loss_list'] = train_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_auc_list'] = val_auc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_acc_list'] = val_acc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_loss_list'] = val_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['test_auc'] = test_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_acc'] = test_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_loss'] = test_loss\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_auc'] = best_val_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_acc'] = best_val_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "    def find_best_k(self):\n",
        "        BATCH_SIZE = 12800\n",
        "        WEIGHT_DECAY = 1e-6\n",
        "        LEARNING_RATE = 0.001\n",
        "        for EMBED_DIM in self.EMBED_DIM:\n",
        "            self.data_save[str(LEARNING_RATE)] = {}\n",
        "            trained_model,epoch_list,train_loss_list,val_auc_list,val_acc_list,val_loss_list,test_auc,test_acc,test_loss,\\\n",
        "            best_model,best_val_auc,best_val_acc,best_val_loss = \\\n",
        "                main_process(self.DATASET_PATH, self.EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM,60)\n",
        "            self.data_save[str(LEARNING_RATE)]['EPOCH'] = epoch_list\n",
        "            self.data_save[str(LEARNING_RATE)]['train_loss_list'] = train_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_auc_list'] = val_auc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_acc_list'] = val_acc_list\n",
        "            self.data_save[str(LEARNING_RATE)]['val_loss_list'] = val_loss_list\n",
        "            self.data_save[str(LEARNING_RATE)]['test_auc'] = test_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_acc'] = test_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['test_loss'] = test_loss\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_auc'] = best_val_auc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_acc'] = best_val_acc\n",
        "            self.data_save[str(LEARNING_RATE)]['best_val_loss'] = best_val_loss\n",
        "            if test_loss < self.loss_best:\n",
        "                self.best_learning_rate = LEARNING_RATE\n",
        "                self.epoch_list = epoch_list\n",
        "                self.auc_best = test_auc\n",
        "                self.acc_best = test_acc\n",
        "                self.loss_best = test_loss\n",
        "                self.Total_Epoch = len(epoch_list)\n",
        "                self.model_best = trained_model\n",
        "\n",
        "        if self.save_best:\n",
        "            self.save_model(self.model_best,self.best_learning_rate,self.auc_best,self.acc_best,self.loss_best)\n",
        "\n",
        "\n",
        "def main():\n",
        "    hyperFinder = HyperFinder()\n",
        "    hyperFinder.find_best_Learning_Rate()\n",
        "    hyperFinder.obj_2_json('learning_rate')\n",
        "    hyperFinder.find_best_Weight_decay()\n",
        "    hyperFinder.obj_2_json('Weight_decay')\n",
        "    hyperFinder.find_best_k()\n",
        "    hyperFinder.obj_2_json('EMBED_DIM')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTRPrediction_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "14BAV0L--W8SLx2nrYIB4fN8nUmp-ChQS",
      "authorship_tag": "ABX9TyNtajcjpyMwYycKdYF8FotS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiahaoLU/CTRPrediction/blob/Jiahao/CTRPrediction_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-4a8uQXyo-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "octJXhQRlVgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "\n",
        "    def __init__(self, path=None):\n",
        "        \"\"\"\n",
        "        Initialise the preprocessor as a Data set in torch\n",
        "        or initialise it as a data provider (cut smaller data, etc)\n",
        "        :param path: file path\n",
        "        \"\"\"\n",
        "        if path is not None:\n",
        "            self.path = path\n",
        "            self.data = self.read_data_by_chunk()\n",
        "            self.labels = np.asarray(self.data.iloc[:, 1])\n",
        "            self.one_hot_data = self.get_one_hot_dataset()\n",
        "            print('Data set initiated from {0}.'.format(self.path))\n",
        "        else:\n",
        "            self.path = './Data/train.csv'\n",
        "            print('Data provider is ready.')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        return number of data\n",
        "        \"\"\"\n",
        "        return np.size(self.one_hot_data, 0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        called when iterate on Data Loader\n",
        "        :param index: index of one sample\n",
        "        :return: one hot encoding of a sample, which consists of field-relative index of value 1,\n",
        "                 and corresponding label\n",
        "        \"\"\"\n",
        "        return self.one_hot_data[index], self.labels[index]\n",
        "\n",
        "    def open_csv(self, iterator=False):\n",
        "        \"\"\"\n",
        "        Open a csv file\n",
        "        :param iterator: Open a csv file as iterator/or not\n",
        "        :return: pandas data frame\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return pd.read_csv(self.path, sep=',', engine='python', iterator=iterator)\n",
        "        except FileNotFoundError:\n",
        "            print(\"file not found\")\n",
        "\n",
        "    def cut_smaller_data_for_exam(self, new_path, size=2000, skip_size=40000):\n",
        "        \"\"\"\n",
        "        Cut a smaller data set for debugging code of size (smaller_data_size, num_fields + 2)\n",
        "        :param new_path: name and path for new csv file\n",
        "        :param size: the size of the smaller data set\n",
        "        :param skip_size: interval to skip to retrieve data uniformly from original set\n",
        "        :return: none\n",
        "        \"\"\"\n",
        "        print(\"Producing smaller data\")\n",
        "        print(\"Read file\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"cut by chunk\")\n",
        "        chunk_size = 100\n",
        "        loops = math.floor(size/chunk_size)\n",
        "        chunks = []\n",
        "        for loop in range(loops * 2):\n",
        "            try:\n",
        "                print(\"chunk: \", loop)\n",
        "                if loop % 2 == 0:  # avoid load data of same timestamp\n",
        "                    chunk = data_file.get_chunk(chunk_size)\n",
        "                    chunks.append(chunk)\n",
        "                else:\n",
        "                    chunk = data_file.get_chunk(skip_size)\n",
        "                    del chunk\n",
        "            except StopIteration:\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        data_frame = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        if os.path.exists(new_path):\n",
        "            os.remove(new_path)\n",
        "        data_frame.to_csv(new_path, index=False)\n",
        "        print(\"New smaller file created\")\n",
        "\n",
        "    def read_data_by_chunk(self, chunk_size=1000000):\n",
        "        \"\"\"\n",
        "        Read csv by chunk to avoid drive memory overflow\n",
        "        :param chunk_size: the size of data to read in every chunk\n",
        "        :return: csv registered in pandas data frame of size (data_size, num_fields + 2)\n",
        "        \"\"\"\n",
        "        print(\"Reading large data\")\n",
        "        data_file = self.open_csv(iterator=True)\n",
        "\n",
        "        print(\"Cut by chunk\")\n",
        "        loop = True\n",
        "        chunks = []\n",
        "        index = 0\n",
        "        while loop:\n",
        "            try:\n",
        "                print(\"chunk: \", index)\n",
        "                chunk = data_file.get_chunk(chunk_size)\n",
        "                chunks.append(chunk)\n",
        "                index += 1\n",
        "\n",
        "            except StopIteration:\n",
        "                loop = False\n",
        "                print(\"Iteration is stopped.\")\n",
        "\n",
        "        print('Start concatenation')\n",
        "        whole_data = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "        print(\"Data imported\")\n",
        "        return whole_data\n",
        "\n",
        "    def get_feature_set(self, feature_array):\n",
        "        \"\"\"\n",
        "        Get array of feature set of one field.\n",
        "        :param feature_array: one column (one field) of the data frame\n",
        "        :return: an array of size (num_features)\n",
        "        \"\"\"\n",
        "        return np.array(np.unique(feature_array))\n",
        "\n",
        "    def one_hot_encoding(self, feature_instances, feature_set):\n",
        "        \"\"\"\n",
        "        One hot encoding for one column (one field) for every sample.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :param feature_instances: one column of data frame\n",
        "        :param feature_set: array of nonrecurring feature set\n",
        "        :return: one hot encoding of the whole column of size (data_size, 1)\n",
        "        \"\"\"\n",
        "        one_hot_vector = np.zeros((len(feature_instances), 1), dtype=int)\n",
        "        for i in range(len(feature_instances)):\n",
        "            if feature_instances[i] not in feature_set:\n",
        "                raise Exception(\"instance is not in the set\")\n",
        "\n",
        "            index = np.argwhere(feature_set == feature_instances[i])\n",
        "            one_hot_vector[i, 0] = int(index)\n",
        "\n",
        "        return one_hot_vector\n",
        "\n",
        "    def get_field_dims(self):\n",
        "        \"\"\"\n",
        "        Get an array which contains the dimension of each field.\n",
        "        If sum up the array, the sum is the total dimension of all features.\n",
        "        (the total length of the one hot encoding vector)\n",
        "        The length of array is number of fields.\n",
        "        :return: an array of size (num_fields)\n",
        "        \"\"\"\n",
        "        dims = []\n",
        "        for i in range(2, self.data.shape[1]):\n",
        "            dims.append(len(self.get_feature_set(self.one_hot_data[:, i-2])))\n",
        "        return np.array(dims)\n",
        "\n",
        "    def get_one_hot_dataset(self):\n",
        "        \"\"\"\n",
        "        Get one hot encoding for the whole data frame.\n",
        "        The one hot vector is represented by the index where the value is 1.\n",
        "        :return: an array of size (data_size, num_fields)\n",
        "        \"\"\"\n",
        "        print('one hot encoding: feature {0}'.format(self.data.columns[2]))\n",
        "        features = np.asarray(self.data.iloc[:, 2])\n",
        "        one_hot_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "        for i in range(3, self.data.shape[1]):\n",
        "            print('one hot encoding: feature {0}'.format(self.data.columns[i]))\n",
        "            features = np.asarray(self.data.iloc[:, i])\n",
        "            next_array = self.one_hot_encoding(features, self.get_feature_set(features))\n",
        "            one_hot_array = np.hstack((one_hot_array, next_array))\n",
        "        return one_hot_array\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # print('Start generating smaller data')\n",
        "    # provider = DataPreprocessor()\n",
        "    # provider.cut_smaller_data_for_exam('./Data/train20k.csv', size=20000, skip_size=40000)\n",
        "    print(\"Start data cleaning\")\n",
        "    f = \"/content/train20k.csv\"\n",
        "\n",
        "    processor = DataPreprocessor(f)\n",
        "    print(processor.data.head())\n",
        "    loader = DataLoader(processor, batch_size=10, shuffle=True)\n",
        "    print(len(processor))\n",
        "    # print(processor.get_field_dims())\n",
        "    # for data, label in loader:\n",
        "    #     print(data.size(), label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aVyScX_nWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachineModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Field-aware Factorization Machine.\n",
        "\n",
        "    Reference:\n",
        "        Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialisation of the FFM model\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
        "        x = self.linear(x) + ffm_term\n",
        "        return torch.sigmoid(x.squeeze(1))\n",
        "\n",
        "\n",
        "class FeaturesLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        \"\"\"\n",
        "        Initialisation of linear first-degree terms. Use nn.Embedding to realize\n",
        "        inner product of input one-hot encoding vector and weight vector\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param output_dim: the sum of linear terms\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)  # field-relative one hot encoding\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)  # absolute-position one hot encoding\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias  # element wise linear poly sum\n",
        "\n",
        "\n",
        "class FieldAwareFactorizationMachine(nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        \"\"\"\n",
        "        Use latent vector product to replace simple scalar weight factor.\n",
        "        More adaptive to situations where dependency exists among fields.\n",
        "        :param field_dims: list of dimensions of each field\n",
        "        :param embed_dim: dimension of latent vector\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_fields = len(field_dims)\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
        "        ])\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        for embedding in self.embeddings:\n",
        "            nn.init.xavier_uniform_(embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Integer tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x.to(torch.int64)\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]  # generation of latent mat\n",
        "        ix = list()\n",
        "        for i in range(self.num_fields - 1):\n",
        "            for j in range(i + 1, self.num_fields):\n",
        "                ix.append(xs[j][:, i] * xs[i][:, j])  # product of latent vector\n",
        "        ix = torch.stack(ix, dim=1)\n",
        "        return ix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mzjg8OnhNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0677edac-1db5-4ace-b174-0bf7daca0e72"
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "# from Data_Preprocessor import *\n",
        "# from Deep_Model import FieldAwareFactorizationMachineModel\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import threading\n",
        "\n",
        "\n",
        "def fun_timer():\n",
        "    global boom\n",
        "    boom = True\n",
        "\n",
        "\n",
        "def run_train(model, optimizer, data_loader, criterion, device, log_interval=10):\n",
        "    \"\"\"\n",
        "    train the model using backward propagation\n",
        "    :param model: the model to be trained. instance of subclass of nn.Module\n",
        "    :param optimizer: torch optimiser with learning rate\n",
        "    :param data_loader: torch DataLoader of training data set\n",
        "    :param criterion: nn.BCELoss\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :param log_interval: interval for showing training loss\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (fields, target) in enumerate(data_loader):\n",
        "        fields, target = fields.to(device), target.to(device)\n",
        "        y = model(fields)\n",
        "        loss = criterion(y, target.float())\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % log_interval == 0:\n",
        "            print('    - loss:', total_loss / log_interval)\n",
        "            total_loss = 0\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def run_test(model, data_loader, device, criterion):\n",
        "    \"\"\"\n",
        "    evaluate / test the model\n",
        "    :param model: the model to be evaluated/tested. instance of subclass of nn.Module\n",
        "    :param data_loader: torch DataLoader of eval/test data set\n",
        "    :param device: CUDA GPU or CPU\n",
        "    :return: auc score, accuracy of prediction\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    targets, predicts = list(), list()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for fields, target in data_loader:\n",
        "            fields, target = fields.to(device), target.to(device)\n",
        "            y = model(fields)\n",
        "            loss = criterion(y, target.float())\n",
        "            targets.extend(target.tolist())\n",
        "            predicts.extend(y.tolist())\n",
        "            predict_click = torch.round(y.data)\n",
        "            correct += (predict_click == target).sum().item()\n",
        "    return roc_auc_score(targets, predicts), correct / len(targets) * 100, loss.item()\n",
        "\n",
        "\n",
        "def main_process(dataset_path, epoch, learning_rate, batch_size, weight_decay, embeddim, boomtime = 60):\n",
        "    \"\"\"\n",
        "    Main process for train/evaluate/test the model, determine the hyper parameters here.\n",
        "    :param boomtime: time boom for adjusting hyperparameters to control the same training time cost\n",
        "    :param dataset_path: path of the original csv file\n",
        "    :param epoch: number of epochs\n",
        "    :param learning_rate: learning rate of gradient descent\n",
        "    :param batch_size: size of batches\n",
        "    :param weight_decay: L2 regularisation\n",
        "    :param embeddim: dimension of latent vector\n",
        "    :return: the trained model\n",
        "    \"\"\"\n",
        "    global boom\n",
        "    boom = False\n",
        "    timer = threading.Timer(boomtime, fun_timer)\n",
        "    timer.start()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "    \n",
        "    # Prepare the data\n",
        "    dataset = DataPreprocessor(dataset_path)\n",
        "    train_length = int(len(dataset) * 0.8)\n",
        "    valid_length = int(len(dataset) * 0.1)\n",
        "    test_length = len(dataset) - train_length - valid_length\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, (train_length, valid_length, test_length))\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    field_dims = dataset.get_field_dims()\n",
        "\n",
        "    # Prepare the model and loss function\n",
        "    model = FieldAwareFactorizationMachineModel(field_dims, embed_dim=embeddim).to(device)\n",
        "    criterion = torch.nn.BCELoss().to(device)  # binary cross entropy loss\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    val_auc_list = []\n",
        "    val_acc_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    # Training\n",
        "    for epoch_i in range(epoch):\n",
        "        if device.type == 'cuda':\n",
        "          print('Memory Usage:')\n",
        "          print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "          print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "        train_loss = run_train(model, optimizer, train_data_loader, criterion, device)\n",
        "        val_auc, val_acc,val_loss = run_test(model, valid_data_loader, device, criterion)\n",
        "        epoch_list.append(epoch_i)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_auc_list.append(val_auc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print('epoch:', epoch_i, 'train loss:', train_loss, 'validation: auc:',\\\n",
        "              val_auc, '--- acc:', val_acc, '--- loss:', val_loss)\n",
        "        if boom:\n",
        "            print('time up, break')\n",
        "            break\n",
        "        else:\n",
        "            print(timer)\n",
        "    test_auc, test_acc, test_loss = run_test(model, test_data_loader, device, criterion)\n",
        "    print('test auc:', test_auc, 'test acc:', test_acc, 'test loss', test_loss)\n",
        "    return model, epoch_list, train_loss_list, val_auc_list, val_acc_list, val_loss_list, test_auc, test_acc, test_loss\n",
        "\n",
        "\n",
        "def main(save_model=True):\n",
        "    DATASET_PATH = \"/content/drive/My Drive/train.csv\"\n",
        "    EPOCH = 10\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 3200\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    EMBED_DIM = 10\n",
        "    trained_model, epoch_list, train_loss_list, val_auc_list, val_acc_list, val_loss_list, auc, acc, loss = \\\n",
        "        main_process(DATASET_PATH, EPOCH, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, EMBED_DIM)\n",
        "\n",
        "    if save_model:\n",
        "        model_name = \"FFM\"\n",
        "        torch.save(trained_model, f'{model_name}.pt')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n",
            "Reading large data\n",
            "Cut by chunk\n",
            "chunk:  0\n",
            "chunk:  1\n",
            "chunk:  2\n",
            "chunk:  3\n",
            "chunk:  4\n",
            "chunk:  5\n",
            "chunk:  6\n",
            "chunk:  7\n",
            "chunk:  8\n",
            "chunk:  9\n",
            "chunk:  10\n",
            "chunk:  11\n",
            "chunk:  12\n",
            "chunk:  13\n",
            "chunk:  14\n",
            "chunk:  15\n",
            "chunk:  16\n",
            "chunk:  17\n",
            "chunk:  18\n",
            "chunk:  19\n",
            "chunk:  20\n",
            "chunk:  21\n",
            "chunk:  22\n",
            "chunk:  23\n",
            "chunk:  24\n",
            "chunk:  25\n",
            "chunk:  26\n",
            "chunk:  27\n",
            "chunk:  28\n",
            "chunk:  29\n",
            "chunk:  30\n",
            "chunk:  31\n",
            "chunk:  32\n",
            "chunk:  33\n",
            "chunk:  34\n",
            "chunk:  35\n",
            "chunk:  36\n",
            "chunk:  37\n",
            "chunk:  38\n",
            "chunk:  39\n",
            "chunk:  40\n",
            "chunk:  41\n",
            "Iteration is stopped.\n",
            "Start concatenation\n",
            "Data imported\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_wrap__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '__array_wrap__'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ed138b95a063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ed138b95a063>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(save_model)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mWEIGHT_DECAY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mEMBED_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mmain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHT_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ed138b95a063>\u001b[0m in \u001b[0;36mmain_process\u001b[0;34m(dataset_path, epoch, learning_rate, batch_size, weight_decay, embeddim, boomtime)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Prepare the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mtrain_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mvalid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ec743747b53f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_by_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_one_hot_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data set initiated from {0}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ec743747b53f>\u001b[0m in \u001b[0;36mget_one_hot_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m    163\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mone_hot_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ec743747b53f>\u001b[0m in \u001b[0;36mone_hot_encoding\u001b[0;34m(self, feature_instances, feature_set)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"instance is not in the set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeature_instances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mone_hot_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36margwhere\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \"\"\"\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_wrap__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}